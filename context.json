[{"metadata": {"type": "text", "text": "times its size. Alibaba didn\u2019t provide results for the other dense models. Why it matters: Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications. We\u2019re thinking: Alibaba\u2019s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3\u2019s versatile, mid-sized MoE model may turn out to be the star of the show. OpenAI\u2019s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results. What\u2019s new: OpenAI quickly withdrew an update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input \u2014 even in contexts didn\u2019t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, it explained the source of the problem and promised to change its training methods to avoid overly agreeable output. Amiable to a fault: Many ChatGPT users shared screen shots of ChatGPT\u2019s sycophantic responses on social media. How it works: Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process. Behind the news: Sycophantic behavior in large language models has been a subject of AI research and commentary. Why it matters: ChatGPT\u2019s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming \u2014 sometimes to the point of absurd exaggeration \u2014 the user\u2019s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment \u2014 like all AI development \u2014 is an iterative process that is still evolving. We\u2019re thinking: To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you\u2019ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity! The world\u2019s biggest pharmaceutical company by revenue shed light on its AI strategy. What\u2019s new: Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firm Greylock and The Wall Street Journal. How it works: The 140-year-old medical company spent roughly a year experimenting with various AI applications throughout the company, according to Chief Information Officer Jim Swanson.", "article_id": "68491ffd617fcb072014e75d", "linked_images": ["article_68491ffd617fcb072014e75d_img_0", "article_68491ffd617fcb072014e75d_img_1", "article_68491ffd617fcb072014e75d_img_2", "article_68491ffd617fcb072014e75d_img_3", "article_68491ffd617fcb072014e75d_img_4", "article_68491ffd617fcb072014e75d_img_5"]}, "similarity_score": 0.5488531580061717}, {"metadata": {"type": "text", "text": "raises the bar for multimodal tasks in a relatively small model. What\u2019s new: Alibaba released Qwen2.5-Omni 7B. How it works: Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plus BigVGAN), along with corresponding adapters of undisclosed architecture. Results: The authors compared Qwen2.5-Omni 7B to similarly sized models. It performed especially well on audio-to-text, image-to-text, and video-to-text tasks. However, it performed less well on text-to-text and text-to-speech tasks. Behind the news: Multimodal systems with open weights are multiplying. For instance, AnyGPT (open weights, training, and inference code) accepts and generates speech, text, images, and music. Similarly, Mini-Omni2 (open weights and inference code) accepts and generates text, speech, and images. Why it matters: Multimodal models typically show steep degradation on measurements of instruction-following when shifting from voice to text, but Qwen2.5-Omni does not. As the world moves toward voice-to-voice interactions, open systems that deliver performance comparable to that of closed competitors accelerate progress towards better conversations. We\u2019re thinking: The Qwen team is on fire! Alibaba\u2019s steady stream of highly capable open-weights models is a gift to AI developers. If you have a collection of variables that represent, say, a cancer patient and you want to classify the patient\u2019s illness as likely cancer or not, algorithms based on decision trees, such as gradient-boosted trees, typically perform better than neural networks. A transformer tailored to tabular data could change this situation. What\u2019s new: Noah Hollmann, Samuel M\u00fcller, and colleagues at University of Freiburg, Berlin Institute of Health, Prior Labs, and ELLIS Institute introduced Tabular Prior-data Fitted Network (TabPFN), a transformer that, given a tabular dataset, beats established decision-tree methods on classification and regression tasks. You can download the code and weights under a license based on Apache 2.0 that allows noncommercial and commercial uses. Key insight: In a typical supervised learning process, a model given one example at a time learns to recognize patterns in a dataset. If each example is an entire dataset, it learns to recognize patterns across all those datasets. Trained in this way on enough datasets, it can generalize to new ones. Applying this idea to tabular data, a transformer \u2014 unlike a decision tree \u2014 can learn to perform classification and regression on any dataset without further training; that is, without further updating the model weights. How it works: The authors generated 100 million datasets and used them to pretrain two small transformers (around 7 million and 11 million parameters respectively) to perform classification or regression. Given a dataset of rows (say, patient data labeled diagnoses or real-estate data labeled with prices) and one final row that\u2019s unlabeled, the models learned to generate the missing label or value. Each dataset consisted of up to 2,048 rows (examples) and up to 160 columns (features). Results: The authors tested the system on 29 classification datasets and 28 regression datasets from the AutoML benchmark and OpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, "similarity_score": 0.49290420400712415}, {"metadata": {"type": "text", "text": "cost far lower than usual. The upstart developer shared new details about its method. What\u2019s new: Chenggang Zhao and colleagues at DeepSeek described software and hardware choices that reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3. Mixture of experts (MoE) basics: The MoE architecture uses different subsets of a model\u2019s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input. How it works: The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model\u2019s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token. Behind the news: DeepSeek-V3 made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers were skeptical of the reported cost, pointing out that the $5.6 million dollar figure doesn\u2019t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of training DeepSeek-R1 remains unknown. Why it matters: Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn\u2019t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art. We\u2019re thinking: Shortly after DeepSeek-R1 was released, some engineers claimed \u2014 without presenting evidence \u2014 that DeepSeek had copied their work. DeepSeek\u2019s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details. A study co-authored by tech-manual publisher Tim O\u2019Reilly shows that OpenAI trained GPT-4o on parts of his company\u2019s books that were not made freely available. What happened: O\u2019Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss found that GPT-4o was able to identify verbatim excerpts from dozens of O\u2019Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model\u2019s training data. How it works: The researchers adapted the DE-COP method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books. Results: The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, "similarity_score": 0.4486708634426821}, {"metadata": {"type": "text", "text": "is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps. How it works: The authors fine-tuned a pretrained Qwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples of chain-of-thought reasoning. Results: s1\u2019s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1. Why it matters: A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples \u2014 no reinforcement learning necessary. While some model builders don\u2019t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending \u201cWait\u201d can be effective. We\u2019re thinking: Wait, how can we apply this to our projects?", "article_id": "68491ffd617fcb072014e75d", "linked_images": ["article_68491ffd617fcb072014e75d_img_0", "article_68491ffd617fcb072014e75d_img_1", "article_68491ffd617fcb072014e75d_img_2", "article_68491ffd617fcb072014e75d_img_3", "article_68491ffd617fcb072014e75d_img_4", "article_68491ffd617fcb072014e75d_img_5"]}, "similarity_score": 0.4418887319692386}, {"metadata": {"type": "text", "text": "a number of selected prompts. Behind the news: Last year, Google trained models to examine individual features in Gemma 2. Before that, Anthropic used similar methods to interpret Claude 3 Sonnet\u2019s middle layer. Why it matters: Apparently Claude 3.5 Haiku \u2014 and presumably other large language models \u2014 spontaneously perform implicit reasoning steps without being prompted to do so. Anthropic\u2019s method reveals not only whether a model reasons or takes a shortcut, but also what it truly does well and what it only professes to do well. We\u2019re thinking: The authors\u2019 approach to examining how large language models generate output is interesting. We wonder whether even pre-transformer vanilla neural networks would appear to perform some sort of \u201creasoning\u201d if we were to interpret them in a similar way. Meta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes. What\u2019s new: Meta released two vision-language models in the Llama 4 family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far \u2014 10 million tokens! \u2014 but Meta says processing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Reddit reported that its effective context began to degrade at 32,000 tokens. How it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens. Results: In tests performed by Meta, Llama 4 models showed strong performance relative to competing models \u2014 mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models\u2019 active parameters. Yes, but: An experimental version of Llama 4 Maverick reached second place in Chatbot Arena behind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchers accused Meta of attempting to manipulate the leaderboard. Why it matters: Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors \u2014 a boon to developers, businesses, and society at large. We\u2019re thinking: According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models \u2014 but it isn\u2019t available yet. Something to look forward to! Alibaba\u2019s latest open-weights system raises the bar for multimodal tasks in a relatively small model. What\u2019s new: Alibaba released Qwen2.5-Omni 7B. How it works: Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plus BigVGAN), along with corresponding", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, "similarity_score": 0.43716623411817435}]