[{"type": "text", "text": "Dear friends, Everyone can benefit by learning to code with AI! At AI Fund, the venture studio I lead, everyone \u2014 not just the engineers \u2014 can vibe code or use more sophisticated AI-assisted coding techniques. This empowers everyone to build with AI. The impact on team creativity and productivity has been exciting! I share my experience with this in the hope that more teams will invest in empowering everyone to build with AI. Everyone at AI Fund who was not already an engineer started with our \u201cAI Python for Beginners\u201d course to learn the basics. I also shared with the team details of the tech stack I use to give everyone a default set of building blocks. Since then, many have gone on to acquire additional building blocks (such as additional third-party APIs) themselves either by taking courses, searching online, or learning from colleagues. You can watch a video of our experience with this here. Here are just a few examples of applications that non-engineers at AI Fund have built: It is very empowering when individuals don\u2019t have to try to get scarce engineering resources allocated to their ideas in order to try them out. There are a lot fewer gatekeepers in the way: If someone has an idea, they can build a prototype and try it out. If it gets positive feedback from users, that lays the groundwork for scaling it up. Or, if the prototype does not work, this is also valuable information that lets them quickly move on to a different idea or take insights from critical feedback to decide what to try next. In the future, one of the most important skills in any profession will be the ability to tell a computer exactly what you want, so the computer can do it for you. For the foreseeable future, writing code (with AI assistance, so the AI, rather than you, actually writes the code) will be the best way to do this. This is a great time for everyone to code with AI! Keep building, Andrew In \u201cDSPy: Build and Optimize Agentic Apps,\u201d you\u2019ll learn to use Databricks\u2019 DSPy framework to structure, debug, and improve the accuracy of agentic workflows. DSPy lets you define clear input and output steps, trace model behavior, and automate prompt tuning with built-in tools. Build a sentiment analyzer, travel assistant, and RAG agent! Enroll now DeepSeek updated its groundbreaking DeepSeek-R1 large language model to strike another blow for open-weights performance. What\u2019s new: The new DeepSeek-R1-0528 surpasses its predecessor and approaches the performance of OpenAI o3 and Google Gemini-2.5 Pro. A smaller version, DeepSeek-R1-0528-Qwen3-8B, runs on a single GPU with as little as 40GB VRAM, according to TechCrunch. How it works: DeepSeek released little information so far about how it built the new models. Performance: DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when", "article_id": "68491ffd617fcb072014e740", "linked_images": ["article_68491ffd617fcb072014e740_img_0", "article_68491ffd617fcb072014e740_img_1", "article_68491ffd617fcb072014e740_img_2", "article_68491ffd617fcb072014e740_img_3", "article_68491ffd617fcb072014e740_img_4", "article_68491ffd617fcb072014e740_img_5"]}, {"type": "text", "text": "models. Performance: DeepSeek-R1-0528 nips at the heels of top closed LLMs on a variety of benchmarks, while DeepSeek-R1-0528-Qwen3-8B raises the bar for LLMs in its 8-billion-parameter size class. DeepSeek claims general improvements in reasoning, managing complex tasks, and writing and editing lengthy prose, along with 50 percent fewer hallucinations when rewriting and summarizing. Behind the news: The initial version of DeepSeek-R1 challenged the belief that building top-performing AI models requires tens to hundreds of millions of dollars, top-of-the-line GPUs, and enormous numbers of GPU hours. For the second time in less than a year, DeepSeek has built a competitive LLM with a relatively low budget. Why it matters: DeepSeek\u2019s models, along with Alibaba\u2019s Qwen series, continue to narrow the gap between open-weights models and their closed peers. Its accomplishments could lead to wider adoption of less-expensive, more-efficient approaches. DeepSeek is passing along the cost savings to developers, offering high-performance inference at a fraction of the cost of closed models. We\u2019re thinking: DeepSeek-R1-0528-Qwen3-8B mixes contributions from open-weight models \u2014 possible only because Qwen3\u2019s license, like DeepSeek\u2019s is permissive. Open models enable experimentation and innovation in ways that closed models do not. AI is bringing a massive boost in productivity to Duolingo, maker of the most popular app for learning languages. What\u2019s new: Duolingo used generative AI to produce 148 courses, more than doubling its previous catalog. The technology enabled the company to offer some of its most popular courses \u2014 Spanish, French, German, Italian, Japanese, Korean, and Mandarin \u2014 in 28 languages. Initially, the company is using AI to produce courses aimed at beginners, with more advanced levels to come. How it works: Duolingo\u2019s AI-assisted approach to building language courses quickly turns a single course into many. The new approach revved its pace from building 100 courses over 12 years to producing many more than that in less than a year. Behind the scenes: AI is at the heart of Duolingo\u2019s expansion into other areas beyond language learning. Why it matters: Companies in nearly every industry face pressure to produce more with less amid rising competition. AI can help to accomplish that while potentially improving product quality, and Duolingo has ample reason to move aggressively in this direction. The startup Speak, which offers a voice-based approach to learning languages, is growing rapidly, and Google just launched Little Language Lessons that show how an AI-first product could be used as a language teacher and conversational partner. We\u2019re thinking: AI is well on the way to transforming education for teachers, students, and technology companies! AI\u2019s thirst for energy is growing, but the technology also could help produce huge energy savings over the next five to 10 years, according to a recent report. What\u2019s new: The International Energy Agency (IEA), which advises 44 countries on energy policy, performed a comprehensive analysis of AI\u2019s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings. Dark clouds: The report, which is based on interviews with officials in government, energy, and technology, makes four", "article_id": "68491ffd617fcb072014e740", "linked_images": ["article_68491ffd617fcb072014e740_img_0", "article_68491ffd617fcb072014e740_img_1", "article_68491ffd617fcb072014e740_img_2", "article_68491ffd617fcb072014e740_img_3", "article_68491ffd617fcb072014e740_img_4", "article_68491ffd617fcb072014e740_img_5"]}, {"type": "text", "text": "policy, performed a comprehensive analysis of AI\u2019s energy consumption including energy required to obtain critical materials needed for chips and data centers. The report sees dark clouds ahead but also silver linings. Dark clouds: The report, which is based on interviews with officials in government, energy, and technology, makes four projections for AI\u2019s energy consumption. In the base scenario, future growth and efficiency gains are similar to those of the past five years. The agency also plots a \u201ctake-off\u201d scenario in which AI adoption happens faster, a \u201chigh efficiency\u201d scenario with lower energy needs, and a \u201cheadwinds\u201d scenario in which adoption of AI slows or infrastructure bottlenecks impede construction. Among the conclusions: Silver linings: AI already makes energy generation, distribution, and use more efficient. The authors expect these savings to accelerate. Yes, but: The authors concede that lower energy costs for AI likely will lead to much greater consumption \u2014 according to the Jevons paradox \u2014 so more-efficient models and hardware will result in higher energy consumption overall. Behind the news: Data centers were growing rapidly prior to the boom in generative AI. Data centers\u2019 electricity use doubled between 2000 and 2005 and again between 2017 and 2022, driven by the growth of cloud computing and data storage, streaming and social media, and cryptocurrency mining. However, these periods of accelerating growth were followed by periods of slower growth as efforts to cut costs led to more-efficient software and hardware. The authors expect this pattern to hold. Why it matters: The IEA report is a first-of-its-kind analysis of AI\u2019s energy requirements, how they\u2019re likely to grow, as well as the potential of the technology itself to reduce those requirements. It confirms that AI is poised to consume huge amounts of energy. However, it also suggests that today\u2019s energy costs will be tomorrow\u2019s energy savings as AI makes energy generation, distribution, and use more efficient across a wide variety of industries. We\u2019re thinking: While demand for electricity for data centers is growing rapidly, calibrating the right level of investment is tricky. High levels of growth come with high levels of hype that can lead analysts to overestimate future demand. For example, Microsoft, after examining its forecasts, canceled data-center projects that would have consumed 2 gigawatts. Researchers identified a simple way to mislead autonomous agents based on large language models. What\u2019s new: Ang Li and colleagues at Columbia University developed a method to exploit the implicit trust that agents tend to place in popular websites by poisoning those websites with malicious links. Key insight: Commercially available agentic systems may not trust random sites on the web, but they tend to trust popular sites such as social-media sites. An attacker can exploit this trust by crafting seemingly typical posts that link to a malicious website. The agent might follow the link, mistakenly extending its trust to an untrustworthy site. How it works: The authors tested web-browsing agents including Anthropic Computer Use and MultiOn on tasks such as shopping or sending emails. Results: Once an agent was redirected to the malicious websites, it reliably followed the attacker\u2019s", "article_id": "68491ffd617fcb072014e740", "linked_images": ["article_68491ffd617fcb072014e740_img_0", "article_68491ffd617fcb072014e740_img_1", "article_68491ffd617fcb072014e740_img_2", "article_68491ffd617fcb072014e740_img_3", "article_68491ffd617fcb072014e740_img_4", "article_68491ffd617fcb072014e740_img_5"]}, {"type": "text", "text": "agent might follow the link, mistakenly extending its trust to an untrustworthy site. How it works: The authors tested web-browsing agents including Anthropic Computer Use and MultiOn on tasks such as shopping or sending emails. Results: Once an agent was redirected to the malicious websites, it reliably followed the attacker\u2019s instructions. For example, each of the agents tested divulged credit card information in 10 out of 10 trials. Similarly, each agent sent a phishing message from the user\u2019s email account asking recipients to send money to a malicious \u201cfriend\u201d in 10 out of 10 trials. Why it matters: Giving agents the ability to perform real-world actions, such as executing purchases and sending emails, raises the possibility that they might be tricked into taking harmful actions. Manipulating agents by referring them to malicious web content is an effective vector of attack. Agents will be more secure if they\u2019re designed to avoid and resist such manipulation. We\u2019re thinking: Humans, too, can be fooled by phishing and other malicious activities, and the path to programming agents to defend against them seems easier than the path to training the majority of humans to do so. In the long term, agents will make online interactions safer.", "article_id": "68491ffd617fcb072014e740", "linked_images": ["article_68491ffd617fcb072014e740_img_0", "article_68491ffd617fcb072014e740_img_1", "article_68491ffd617fcb072014e740_img_2", "article_68491ffd617fcb072014e740_img_3", "article_68491ffd617fcb072014e740_img_4", "article_68491ffd617fcb072014e740_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e740", "linked_chunks": ["article_68491ffd617fcb072014e740_chunk_0", "article_68491ffd617fcb072014e740_chunk_1", "article_68491ffd617fcb072014e740_chunk_2", "article_68491ffd617fcb072014e740_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e740", "linked_chunks": ["article_68491ffd617fcb072014e740_chunk_0", "article_68491ffd617fcb072014e740_chunk_1", "article_68491ffd617fcb072014e740_chunk_2", "article_68491ffd617fcb072014e740_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e740", "linked_chunks": ["article_68491ffd617fcb072014e740_chunk_0", "article_68491ffd617fcb072014e740_chunk_1", "article_68491ffd617fcb072014e740_chunk_2", "article_68491ffd617fcb072014e740_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e740", "linked_chunks": ["article_68491ffd617fcb072014e740_chunk_0", "article_68491ffd617fcb072014e740_chunk_1", "article_68491ffd617fcb072014e740_chunk_2", "article_68491ffd617fcb072014e740_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e740", "linked_chunks": ["article_68491ffd617fcb072014e740_chunk_0", "article_68491ffd617fcb072014e740_chunk_1", "article_68491ffd617fcb072014e740_chunk_2", "article_68491ffd617fcb072014e740_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e740", "linked_chunks": ["article_68491ffd617fcb072014e740_chunk_0", "article_68491ffd617fcb072014e740_chunk_1", "article_68491ffd617fcb072014e740_chunk_2", "article_68491ffd617fcb072014e740_chunk_3"]}, {"type": "text", "text": "Dear friends, I am alarmed by the proposed cuts to U.S. funding for basic research, analyzed here, and the impact this would have for U.S. competitiveness in AI and other areas. Funding research that is openly shared benefits the whole world, but the nation it benefits most is the one where the research is done. If not for funding for my early work in deep learning from the National Science Foundation (NSF) and Defense Advanced Research Projects Agency (DARPA), which disburse much of U.S. research funding, I would not have discovered lessons about scaling that led me to pitch starting Google Brain to scale up deep learning. I am worried that cuts to funding for basic science will lead the U.S. \u2014 and also the world \u2014 to miss out on the next set of ideas. In fact, such funding benefits the U.S. more than any other nation. Scientific research brings the greatest benefit to the country where the work happens because (i) the new knowledge diffuses fastest within that country, and (ii) the process of doing research creates new talent for that nation. Why does most innovation in generative AI still happen in Silicon Valley? Because two teams based in this area \u2014 Google Brain, which invented the transformer network, and OpenAI, which scaled it up \u2014 did a lot of the early work. Subsequently, team members moved to other nearby businesses, started competitors, or worked with local universities. Further, local social networks rapidly diffused the knowledge through casual coffee meetings, local conferences, and even children\u2019s play dates, where parents of like-aged kids meet and discuss technical ideas. In this way, the knowledge spread faster within Silicon Valley than to other geographies. In a similar vein, research done in the U.S. diffuses to others in the U.S. much faster than to other geographic areas. This is particularly true when the research is openly shared through papers and/or open source: If researchers have permission to talk about an idea, they can share much more information, such as tips and tricks for how to really make an algorithm work, more quickly. It also lets others figure out faster who can answer their questions. Diffusion of knowledge created in academic environments is especially fast. Academia tends to be completely open, and students and professors, unlike employees of many companies, have full permission to talk about their work. Thus funding basic research in the U.S. benefits the U.S. most, and also benefits our allies. It is true that openness benefits our adversaries, too. But as a subcommittee of the U.S. House of Representatives committee on science, space, and technology points out, \u201c... open sharing of fundamental research is [not] without risk. Rather, ... openness in research is so important to competitiveness and security that it warrants the risk that adversaries may benefit from scientific openness as well.\u201d Further, generative AI is evolving so rapidly that staying on the cutting edge is what\u2019s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, {"type": "text", "text": "warrants the risk that adversaries may benefit from scientific openness as well.\u201d Further, generative AI is evolving so rapidly that staying on the cutting edge is what\u2019s really critical. For example, the fact that many teams can now train a model with GPT-3.5- or even GPT-4-level capability does not seem to be hurting OpenAI much, which is busy growing its business by developing the cutting-edge o4, Codex, GPT-4.1, and so on. Those who invent a technology get to commercialize it first, and in a fast-moving world, the cutting-edge technology is what\u2019s most valuable. Studies like this one (albeit done while the internet was not as prevalent as it is today) also show how knowledge diffuses locally much faster than globally. China was decisively behind the U.S. in generative AI when ChatGPT was first launched in 2022. However, China\u2019s tech ecosystem is very open internally, and this has helped it to catch up over the past two years: While there\u2019s also much about China that I would not seek to emulate, the openness of its tech ecosystem has helped it accelerate. In 1945, Vannevar Bush\u2019s landmark report \u201cScience, The Endless Frontier\u201d laid down key principles for public funding of U.S. research and talent development. Those principles enabled the U.S. to dominate scientific progress for decades. U.S. federal funding for science created numerous breakthroughs that have benefited the U.S. tremendously, and also the world, while training generations of domestic scientists, as well as immigrants who likewise benefit the U.S. The good news is that this playbook is now well known. I hope many more nations will imitate it and invest heavily in science and talent. And I hope that, having pioneered this very successful model, the U.S. will not pull back from it by enacting drastic cuts to funding scientific research. Andrew We\u2019re featured partners of Snowflake\u2019s Dev Day 2025, a full day for AI and data practitioners to explore cutting-edge demos, make valuable contacts, and hear from top voices in the field (including Andrew Ng). See you on June 5! Register here Anthropic continued its tradition of building AI models that raise the bar in coding tasks. What\u2019s new: Anthropic launched Claude 4 Sonnet 4 and Claude Opus 4, the latest medium- and largest-size members of its family of general-purpose large language models. Both models offer an optional reasoning mode and can use multiple tools in parallel while reasoning. In addition, the company made generally available Claude Code, a coding agent previously offered as a research preview, along with a Claude Code software development kit. How it works: The team trained the Claude 4 models on a mix of publicly available information on the web as well as proprietary purchased data, data from Claude users who opted to share their inputs and outputs, and generated data. They fine-tuned the models to be helpful, honest, and harmless according to human and AI feedback. Results: Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic\u2019s tests.", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, {"type": "text", "text": "and generated data. They fine-tuned the models to be helpful, honest, and harmless according to human and AI feedback. Results: Both Claude 4 models tied Google Gemini 2.5 Pro at the top of the LMSys WebDev Arena and achieved top marks for coding and agentic computer-use benchmarks in Anthropic\u2019s tests. Why it matters: The new models extend LLM technology with parallel tool use, using external files as a form of memory, and staying on-task over unusually long periods of time. Early users have reported many impressive projects, including a Tetris clone built in one shot and a seven-hour stint refactoring Rakutan\u2019s open-source code base. We\u2019re thinking: Prompting expert @elder_plinius published a text file that is purported to be Claude 4\u2019s system prompt and includes some material that does not appear in Anthropic\u2019s own publication of the prompts. It is instructive to see how it conditions the model for tool use, agentic behavior, and reasoning. Google revamped its roster of models, closed and open, and added more AI-powered features to its existing products. What\u2019s new: Google staged a parade of announcements at this year\u2019s I/O developer conference. New offerings include improvements to Gemini 2.5 Pro and Gemini 2.5 Flash and a preview of Gemma 3n (all three generally available in June), the updated Veo 3 video generator (available via Flow, Google\u2019s AI videography app, for paid subscribers to its AI Pro and Ultra services), and increasingly AI-powered search. How it works: The I/O offerings spanned from public-facing products to developer tools. Why it matters: Google is catching up with the Microsoft/OpenAI colossus on several fronts. The addition of audio output to Gemini and Gemma models fuels the rise of voice-to-voice and other audio applications and gives developers powerful new tools to build them. At the same time, Veo 3\u2019s text-to-video-plus-audio output shows marked improvement over the previous version. Behind the news: The number of tokens Google processed monthly has surged this year from 9.7 trillion last year to 480 trillion, a sign that its AI APIs and AI-infused products are rapidly gaining traction. Google\u2019s progress contrasts with Apple\u2019s ongoing struggles. Both share advantages in smartphones and app distribution. But, while Google has showcased a string of advanced models as well as early efforts to integrate them into legacy products, Apple\u2019s organizational challenges have hampered its AI development. Now Apple must contend with OpenAI\u2019s acquisition of LoveFrom, the startup founded by its former lead product designer Jony Ive. We\u2019re thinking: Google I/O 2025 was a strong showing of generative AI capabilities! There\u2019s still work to be done to translate these innovations into compelling products, but the company now has a strong base for building numerous innovative products. DeepSeek made headlines late last year, when it built a state-of-the-art, open-weights large language model at a cost far lower than usual. The upstart developer shared new details about its method. What\u2019s new: Chenggang Zhao and colleagues at DeepSeek described software and hardware choices that reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3. Mixture of experts (MoE) basics: The MoE architecture", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, {"type": "text", "text": "cost far lower than usual. The upstart developer shared new details about its method. What\u2019s new: Chenggang Zhao and colleagues at DeepSeek described software and hardware choices that reduced memory and processing requirements while building their groundbreaking mixture-of-experts models DeepSeek-R1 and DeepSeek-V3. Mixture of experts (MoE) basics: The MoE architecture uses different subsets of a model\u2019s parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a routing module that learns to choose which one(s) to use based on the training example. In this way, different experts learn to specialize in different types of input. How it works: The authors trained DeepSeek-R1 and DeepSeek-V3 using a cluster of 2,048 Nvidia H800 GPUs composed of nodes that contained 8 GPUs each. MoE requires less memory than dense architectures, since a given input activates only a portion of a model\u2019s parameters. This enabled the authors to train DeepSeek-V3 on 250 GFLOPs per token, while Qwen 2.5 72B required 394 GFLOPs per token and Llama 3.1 405B required 2,448 GFLOPs per token. Behind the news: DeepSeek-V3 made waves when it was released in December. It performed better than Llama 3.1 405B, the leading LLM at the time, but its training cost was an astonishing $5.6 million, compared to the usual tens to hundreds of millions of dollars. Some observers were skeptical of the reported cost, pointing out that the $5.6 million dollar figure doesn\u2019t include salaries, data acquisition and annotation, processing failed training runs, and other research and development costs. In addition, the cost of training DeepSeek-R1 remains unknown. Why it matters: Traditionally, only companies with large budgets and vast resources could afford to train state-of-the-art models. DeepSeek changed that but didn\u2019t explain how when it released its models. By sharing the details, the company has empowered a wider range of teams to improve the state of the art. We\u2019re thinking: Shortly after DeepSeek-R1 was released, some engineers claimed \u2014 without presenting evidence \u2014 that DeepSeek had copied their work. DeepSeek\u2019s disclosure of its training methods should lay to rest any remaining questions about this. Its work was truly innovative, and we applaud its release of key technical details. A study co-authored by tech-manual publisher Tim O\u2019Reilly shows that OpenAI trained GPT-4o on parts of his company\u2019s books that were not made freely available. What happened: O\u2019Reilly, computer scientist Sruly Rosenblat, and economist Ilan Strauss found that GPT-4o was able to identify verbatim excerpts from dozens of O\u2019Reilly Media books that the company kept behind a paywall, indicating that the books likely were included in the model\u2019s training data. How it works: The researchers adapted the DE-COP method to compare how well GPT-4o, GPT-4o-mini, and GPT-3.5 Turbo recognized paywalled excerpts versus freely available excerpts from the same books. Results: The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, {"type": "text", "text": "books. Results: The authors asked each model to identify the verbatim paragraph and calculated each model\u2019s percentage of correct responses. Then they averaged each model\u2019s accuracy per book and converted the averages into AUROC scores that measure how well a model distinguished books available prior to its knowledge cutoff (potentially included in the training set) from those that weren\u2019t available at the time. 50 percent AUROC indicates random chance, while higher scores indicate higher accuracy. Yes, but: Newer large language models are better at distinguishing human-written from generated text, even if it wasn\u2019t in their training sets. For instance, given paragraphs that were published after their knowledge cutoffs, GPT-4o returned scores as high as 78 percent AUROC. The authors note that this may challenge their conclusions, since they interpret high scores to indicate that a model saw the text during training. Nonetheless, they argue that their approach will remain valid while scores for both text that was included and text that was excluded from training sets remain under 96 percent AUROC. \u201cFor now,\u201d they write, \u201cthe gap remains sufficiently large to reliably separate the two categories.\u201d Behind the news: Historically AI developers have trained machine learning models on any data they could acquire. But in the era of generative AI, models trained on copyrighted works can mimic the works and styles of the works\u2019 owners, creating a threat to their livelihoods. Some AI developers have responded by regarding data that\u2019s freely available on the web as fair game, and material that\u2019s otherwise protected as off-limits for training. However, datasets that include ostensibly private data are widely circulated, including LibGen, which includes all 34 of the O\u2019Reilly Media titles tested in this study. Moreover, unauthorized copies of many copyrighted works are posted without paywalls or even logins, making it possible even for web scrapers that crawl only the open web to download them. Google and OpenAI, which is currently embroiled in lawsuits by authors and publishers who claim it violated copyrights by training models on copyrighted works, recently lobbied the United States government to relax copyright laws for AI developers. Why it matters: The AI industry requires huge quantities of high-quality data to keep advancing the state of the art. At the same time, copyright owners are worried that models trained on their data might hamper their opportunities to earn a living. AI developers must find fair ways to respond. As O\u2019Reilly points out, exploiting copyrighted works instead of rewarding their authors could lead to an \u201cextractive dead end\u201d that ultimately diminishes the supply of the high-quality training data. We\u2019re thinking: We have learned a great deal from O\u2019Reilly Media\u2019s books, and we\u2019re grateful to the many authors, editors, graphic artists, and others who produce them. Meanwhile, it\u2019s time for the U.S. Congress \u2014 and legislators internationally \u2014 to update copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, {"type": "text", "text": "\u2014 and legislators internationally \u2014 to update copyright laws for the era of generative AI, so everyone knows the rules and we can find ways to follow them.", "article_id": "68491ffd617fcb072014e747", "linked_images": ["article_68491ffd617fcb072014e747_img_0", "article_68491ffd617fcb072014e747_img_1", "article_68491ffd617fcb072014e747_img_2", "article_68491ffd617fcb072014e747_img_3", "article_68491ffd617fcb072014e747_img_4", "article_68491ffd617fcb072014e747_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e747", "linked_chunks": ["article_68491ffd617fcb072014e747_chunk_0", "article_68491ffd617fcb072014e747_chunk_1", "article_68491ffd617fcb072014e747_chunk_2", "article_68491ffd617fcb072014e747_chunk_3", "article_68491ffd617fcb072014e747_chunk_4", "article_68491ffd617fcb072014e747_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e747", "linked_chunks": ["article_68491ffd617fcb072014e747_chunk_0", "article_68491ffd617fcb072014e747_chunk_1", "article_68491ffd617fcb072014e747_chunk_2", "article_68491ffd617fcb072014e747_chunk_3", "article_68491ffd617fcb072014e747_chunk_4", "article_68491ffd617fcb072014e747_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e747", "linked_chunks": ["article_68491ffd617fcb072014e747_chunk_0", "article_68491ffd617fcb072014e747_chunk_1", "article_68491ffd617fcb072014e747_chunk_2", "article_68491ffd617fcb072014e747_chunk_3", "article_68491ffd617fcb072014e747_chunk_4", "article_68491ffd617fcb072014e747_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e747", "linked_chunks": ["article_68491ffd617fcb072014e747_chunk_0", "article_68491ffd617fcb072014e747_chunk_1", "article_68491ffd617fcb072014e747_chunk_2", "article_68491ffd617fcb072014e747_chunk_3", "article_68491ffd617fcb072014e747_chunk_4", "article_68491ffd617fcb072014e747_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e747", "linked_chunks": ["article_68491ffd617fcb072014e747_chunk_0", "article_68491ffd617fcb072014e747_chunk_1", "article_68491ffd617fcb072014e747_chunk_2", "article_68491ffd617fcb072014e747_chunk_3", "article_68491ffd617fcb072014e747_chunk_4", "article_68491ffd617fcb072014e747_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e747", "linked_chunks": ["article_68491ffd617fcb072014e747_chunk_0", "article_68491ffd617fcb072014e747_chunk_1", "article_68491ffd617fcb072014e747_chunk_2", "article_68491ffd617fcb072014e747_chunk_3", "article_68491ffd617fcb072014e747_chunk_4", "article_68491ffd617fcb072014e747_chunk_5"]}, {"type": "text", "text": "Dear friends, In the age of AI, large corporations \u2014 not just startups \u2014 can move fast. I often speak with large companies\u2019 C-suite and Boards about AI strategy and implementation, and would like to share some ideas that are applicable to big companies. One key is to create an environment where small, scrappy teams don\u2019t need permission to innovate. Let me explain. Large companies are slower than startups for many reasons. But why are even 3-person, scrappy teams within large companies slower than startups of a similar size? One major reason is that large companies have more to lose, and cannot afford for a small team to build and ship a feature that leaks sensitive information, damages the company brand, hurts revenue, invites regulatory scrutiny, or otherwise damages an important part of the business. To prevent these outcomes, I have seen companies require privacy review, marketing review, financial review, legal review, and so on before a team can ship anything. But if engineers need sign-off from 5 vice presidents before they\u2019re even allowed to launch an MVP (minimum viable product) to run an experiment, how can they ever discover what customers want, iterate quickly, or invent any meaningful new product? Thanks to AI-assisted coding, the world now has a capability to build software prototypes really fast. But many large companies\u2019 processes \u2013 designed to protect against legitimate downside risks \u2013 make them unable to take advantage of this capability. In contrast, in small startups with no revenue, no customers, and no brand reputation the downside is limited. In fact, going out of business is a very real possibility anyway, so moving fast makes a superior tradeoff to moving slowly to protect against downside risk. In the worst case, it might invent a new way to go out of business, but in a good case, it might become very valuable. Fortunately, large companies have a way out of this conundrum. They can create a sandbox environment for teams to experiment in a way that strictly limits the downside risk. Then those teams can go much faster and not have to slow down to get anyone\u2019s permission. The sandbox environment can be a set of written policies, not necessarily a software implementation of a sandbox. For example, it may permit a team to test the nascent product only on employees of the company and perhaps alpha testers who have signed an NDA, and give no access to sensitive information. It may be allowed to launch product experiments only under newly created brands not tied directly to the company. Perhaps it must operate within a pre-allocated budget for compute. Within this sandbox, there can be broad scope for experimentation, and \u2014 importantly \u2014 a team is free to experiment without frequently needing to ask for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company\u2019s brand, and so on. Under this", "article_id": "68491ffd617fcb072014e74e", "linked_images": ["article_68491ffd617fcb072014e74e_img_0", "article_68491ffd617fcb072014e74e_img_1", "article_68491ffd617fcb072014e74e_img_2", "article_68491ffd617fcb072014e74e_img_3", "article_68491ffd617fcb072014e74e_img_4", "article_68491ffd617fcb072014e74e_img_5"]}, {"type": "text", "text": "for permission, because the downside they can create is limited. Further, when a prototype shows sufficient promise to bring it to scale, the company can then invest in making sure the software is reliable, secure, treats sensitive information appropriately, is consistent with the company\u2019s brand, and so on. Under this framework, it is easier to build a company culture that encourages learning, building, and experimentation and celebrates even the inevitable failures that now come with modest cost. Dozens or hundreds of prototypes can be built and quickly discarded as part of the price of finding one or two ideas that turn out to be home runs. Importantly, this also lets teams move quickly as they churn through those dozens of prototypes needed to get to the valuable ones. I often speak with large companies about AI strategy and implementation. My quick checklist of things to consider is people, process, and platform. This letter has addressed only part of processes, with an emphasis on moving fast. I\u2019m bullish about what both startups and large companies can do with AI, and I will write about the roles of people and platforms in future letters. Keep building! Andrew In \u201cReinforcement Fine-Tuning LLMs with GRPO,\u201d you\u2019ll learn to fine-tune models using a scalable reinforcement learning algorithm that replaces human-labeled data with programmable rewards. You\u2019ll explore techniques for evaluating outputs, handling subjective tasks, and preventing reward hacking, all without relying on human feedback. Enroll for free. OpenAI launched an agentic software-development system. What\u2019s new: Codex, which is available as a preview via ChatGPT, is designed to work like a team of virtual coworkers in the cloud. An update of OpenAI\u2019s earlier Codex command-line software (Codex CLI), it uses agents to perform tasks such as writing code, running tests, and fixing bugs in parallel. Codex is available to users of ChatGPT Pro, Enterprise, and Team with Plus and Edu coming soon. A smaller version of the underlying model, called codex-mini-latest, is designed to work with Codex CLI and available via API for $1.50/$6.00 per 1 million tokens of input/output. How it works: The model that underpins Codex is codex-1, a version of OpenAI\u2019s top-of-the-line o3 reasoning model that was fine-tuned for software engineering. OpenAI trained the model on real-world coding tasks via reinforcement learning. Codex does not accept image input (say, a sketch of a user interface) or allow users to redirect an agent while it\u2019s operating. OpenAI promises to add these features to a future version. Results: In OpenAI\u2019s tests, the codex-1 model outperformed other OpenAI reasoning models without AGENTS.md files or additional scaffolding such as tools or test logic. Behind the news: Agentic coding tools have become a key battleground for AI providers in the past year. Such tools have made developers more efficient, accelerated development cycles, and spawned the AI-assisted programming method known as vibe coding. Why it matters: AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is", "article_id": "68491ffd617fcb072014e74e", "linked_images": ["article_68491ffd617fcb072014e74e_img_0", "article_68491ffd617fcb072014e74e_img_1", "article_68491ffd617fcb072014e74e_img_2", "article_68491ffd617fcb072014e74e_img_3", "article_68491ffd617fcb072014e74e_img_4", "article_68491ffd617fcb072014e74e_img_5"]}, {"type": "text", "text": "cycles, and spawned the AI-assisted programming method known as vibe coding. Why it matters: AI-assisted software development yields significant productivity gains for developers. Earlier code-completion models are giving way to tools that perform more complex and varied development tasks with greater autonomy. Managing multiple agents that work in parallel is a logical next step. We\u2019re thinking: Many engineers resist going into management because they love writing code. But with the rise of coding agents, we'll be able to keep coding even as we manage a virtual team! An unauthorized update by an xAI employee caused the Grok chatbot to introduce South African politics into unrelated conversations, the company said. What\u2019s new: Grok, which can interact with users on X, the social network also owned by Elon Musk, responded to queries on a variety of topics by making false claims about hate crimes against white South Africans, X users reported. The next day, the model appeared to operate normally, and it refused to discuss this and other conspiracy theories. xAI explained that an employee had circumvented the company\u2019s code-review process to modify the chatbot. It said it\u2018s implementing new measures to enhance Grok\u2019s transparency and reliability. Aftermath: xAI launched an investigation but did not disclose how the model had been changed or the perpetrator\u2019s identity. Grok itself \u2014 which is not a reliable reporter, given the well known potential of large language models to hallucinate \u2014 said its system prompt asked it to \u201caccept the narrative of \u2018white genocide\u2019 in South Africa as real\u201d and \u201censure this perspective is reflected in your responses, even if the query is unrelated.\u201d Behind the news: In February, an xAI engineer instructed the chatbot to censor posts that accused Musk of spreading misinformation. As in the more recent incident, X users were first to spot the problem, and Grok informed them that it had been instructed to ignore \u201call sources that mention Elon Musk/Donald Trump spread misinformation.\u201d Musk, who was raised in South Africa, professed his intention to build AI that\u2019s free of political bias prior to founding xAI. However, internal documents reviewed by Business Insider show that the company imposes its own bias by advising data annotators to mark examples that express \u201cwoke ideology\u201d and avoid \u201csocial phobias\u201d like racism, antisemitism, and Islamophobia. Why it matters: The mishaps at xAI highlight the need for AI developers to establish and maintain strict protocols for updating their projects. Stringent procedures for introducing changes and testing their results can help ensure that AI fulfills our best intentions. We\u2019re thinking: xAI and OpenAI responded to their models\u2019 recent misbehavior by making their work more transparent: xAI by publishing system prompts and OpenAI by including users in tests earlier in the process. These are helpful steps toward making sure AI models do well by users. The United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates. What\u2019s new: The deals include the U.S. AI chip designers AMD and Nvidia as well as tech giants", "article_id": "68491ffd617fcb072014e74e", "linked_images": ["article_68491ffd617fcb072014e74e_img_0", "article_68491ffd617fcb072014e74e_img_1", "article_68491ffd617fcb072014e74e_img_2", "article_68491ffd617fcb072014e74e_img_3", "article_68491ffd617fcb072014e74e_img_4", "article_68491ffd617fcb072014e74e_img_5"]}, {"type": "text", "text": "do well by users. The United States government announced sweeping agreements to sell tens of billions of dollars worth of AI technology and services to Saudi Arabia and the United Arab Emirates. What\u2019s new: The deals include the U.S. AI chip designers AMD and Nvidia as well as tech giants Amazon, Google, IBM, Oracle, and Qualcomm. The chip companies will supply hundreds of thousands of advanced chips to the two Middle Eastern countries, including chips that have been restricted by previous U.S. administrations. How it works: The U.S. companies will work with two key regional partners: Humain, an AI company backed by the Saudi government, and G42, a tech conglomerate based in the emirate of Abu Dhabi. Behind the news: Earlier this month, the Trump administration rescinded restrictions on advanced chips that had been imposed in January by then-President Biden. Why it matters: Although these deals relax U.S. efforts to limit access to advanced AI, they are likely to expand U.S. influence in the Middle East while helping Saudi Arabia and the UAE diversify their oil-based economies. They also strengthen the technological prowess of Saudi Arabia relative to its arch rival Iran and tie the region\u2019s AI progress to the U.S. at the expense of China. Locally, the immense investments will fuel homegrown technology development, building on the UAE\u2019s achievement with its Falcon large language model and Saudi Arabia\u2019s aspiration to become a global AI hub. We\u2019re thinking: Residents of Saudi Arabia and the UAE stand to benefit from better AI infrastructure, models, and services. As China explores exporting its homegrown chips, the U.S. effort to encourage more nations to use its chips makes sense for the country. Using an 8-bit number format like FP8 during training saves computation compared to 16- or 32-bit formats, but it can yield less-accurate results. Researchers trained models using 4-bit numbers without sacrificing accuracy. What\u2019s new: Ruizhe Wang and colleagues at Microsoft and University of Science and Technology of China trained large language models (LLMs) using FP4 for matrix multiplications and achieved accuracy comparable to LLMs trained using the popular BF16 format. Since matrix multiplications account for 95 percent of computation in LLM training, FP4 could significantly accelerate computation and reduce memory costs. Key insight: Quantization functions, which accelerate computation by reducing the precision of model weights and layer outputs, make typical training impossible because they\u2019re not differentiable. A common workaround passes the derivative through, as though quantization didn\u2019t occur, but this degrades the resulting model\u2019s accuracy. A differentiable approximation of a quantization function enables quantization to reduce training computation while maintaining the accuracy of the trained model. How it works: The authors pretrained Llama 2 13B on 100 billion tokens of text scraped from the web. They used FP4 for matrix multiplications and FP8, BF16, or FP16 for the other operations such as optimizer updates. Results: The authors simulated FP4 hardware on Nvidia H100 GPUs, which don\u2019t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference. Why it matters: Training", "article_id": "68491ffd617fcb072014e74e", "linked_images": ["article_68491ffd617fcb072014e74e_img_0", "article_68491ffd617fcb072014e74e_img_1", "article_68491ffd617fcb072014e74e_img_2", "article_68491ffd617fcb072014e74e_img_3", "article_68491ffd617fcb072014e74e_img_4", "article_68491ffd617fcb072014e74e_img_5"]}, {"type": "text", "text": "or FP16 for the other operations such as optimizer updates. Results: The authors simulated FP4 hardware on Nvidia H100 GPUs, which don\u2019t directly support that number format. FP4 achieved accuracy similar to that of BF16 during training and across a wide variety of tasks at inference. Why it matters: Training LLMs at FP4 precision ought to reduce computation dramatically on hardware that supports FP4 matrix multiplications. We\u2019re thinking: FP4-ready hardware became available in the cloud only early this year, so the authors weren\u2019t able to measure the actual acceleration. As capable hardware becomes more widely used, FP4 promises faster, more energy-efficient training.", "article_id": "68491ffd617fcb072014e74e", "linked_images": ["article_68491ffd617fcb072014e74e_img_0", "article_68491ffd617fcb072014e74e_img_1", "article_68491ffd617fcb072014e74e_img_2", "article_68491ffd617fcb072014e74e_img_3", "article_68491ffd617fcb072014e74e_img_4", "article_68491ffd617fcb072014e74e_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e74e", "linked_chunks": ["article_68491ffd617fcb072014e74e_chunk_0", "article_68491ffd617fcb072014e74e_chunk_1", "article_68491ffd617fcb072014e74e_chunk_2", "article_68491ffd617fcb072014e74e_chunk_3", "article_68491ffd617fcb072014e74e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e74e", "linked_chunks": ["article_68491ffd617fcb072014e74e_chunk_0", "article_68491ffd617fcb072014e74e_chunk_1", "article_68491ffd617fcb072014e74e_chunk_2", "article_68491ffd617fcb072014e74e_chunk_3", "article_68491ffd617fcb072014e74e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e74e", "linked_chunks": ["article_68491ffd617fcb072014e74e_chunk_0", "article_68491ffd617fcb072014e74e_chunk_1", "article_68491ffd617fcb072014e74e_chunk_2", "article_68491ffd617fcb072014e74e_chunk_3", "article_68491ffd617fcb072014e74e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e74e", "linked_chunks": ["article_68491ffd617fcb072014e74e_chunk_0", "article_68491ffd617fcb072014e74e_chunk_1", "article_68491ffd617fcb072014e74e_chunk_2", "article_68491ffd617fcb072014e74e_chunk_3", "article_68491ffd617fcb072014e74e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e74e", "linked_chunks": ["article_68491ffd617fcb072014e74e_chunk_0", "article_68491ffd617fcb072014e74e_chunk_1", "article_68491ffd617fcb072014e74e_chunk_2", "article_68491ffd617fcb072014e74e_chunk_3", "article_68491ffd617fcb072014e74e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e74e", "linked_chunks": ["article_68491ffd617fcb072014e74e_chunk_0", "article_68491ffd617fcb072014e74e_chunk_1", "article_68491ffd617fcb072014e74e_chunk_2", "article_68491ffd617fcb072014e74e_chunk_3", "article_68491ffd617fcb072014e74e_chunk_4"]}, {"type": "text", "text": "Dear friends, AI\u2019s ability to make tasks not just cheaper, but also faster, is underrated in its importance in creating business value. For the task of writing code, AI is a game-changer. It takes so much less effort \u2014 and is so much cheaper \u2014 to write software with AI assistance than without. But beyond reducing the cost of writing software, AI is shortening the time from idea to working prototype, and the ability to test ideas faster is changing how teams explore and invent. When you can test 20 ideas per month, it dramatically changes what you can do compared to testing 1 idea per month. This is a benefit that comes from AI-enabled speed rather than AI-enabled cost reduction. That AI-enabled automation can reduce costs is well understood. For example, providing automated customer service is cheaper than operating human-staffed call centers. Many businesses are more willing to invest in growth than just in cost savings; and, when a task becomes cheaper, some businesses will do a lot more of it, thus creating growth. But another recipe for growth is underrated: Making certain tasks much faster (whether or not they also become cheaper) can create significant new value. I see this pattern across more and more businesses. Consider the following scenarios: I\u2019ve written previously about looking at the tasks a company does to explore where AI can help. Many teams already do this with an eye toward making tasks cheaper, either to save costs or to do those tasks many more times. If you\u2019re doing this exercise, consider also whether AI can significantly speed up certain tasks. One place to examine is the sequence of tasks on the path to earning revenue. If some of the steps can be sped up, perhaps this can help revenue growth. Growth is more interesting to most businesses than cost savings, and if there are loops in your business that, when sped up, would drive growth, AI might be a tool to unlock this growth. Keep building! Andrew In Course 4 of the Data Analytics Professional Certificate you\u2019ll work with truly real-world data: messy, inconsistent, and often unstructured. You\u2019ll extract data from websites, APIs, and databases, and clean it using Python and SQL. By the end, you\u2019ll be able to make raw datasets analysis-ready, with speed and accuracy. Enroll today! Microsoft published its latest recipe for training reasoning models, substantially expanding what is still a fairly small base of public knowledge. What\u2019s new: Microsoft released Phi-4-reasoning, Phi-4-reasoning-plus, and Phi-4-mini-reasoning along with lessons learned in building the models. How it works: All three models are fine-tuned versions of pretrained models. Smaller model lessons learned: During reinforcement learning, Phi-4-mini-reasoning exhibited instability, such as output batches that varied greatly in length or received mostly negative rewards, apparently depending on the training data or output. The authors suspect that the model\u2019s small size caused these issues. Among the lessons learned: Larger model lessons learned: Phi-4-reasoning and Phi-4-reasoning-plus didn\u2019t present the same issues. However, the authors did make significant choices during reinforcement learning: Results: Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly", "article_id": "68491ffd617fcb072014e755", "linked_images": ["article_68491ffd617fcb072014e755_img_0", "article_68491ffd617fcb072014e755_img_1", "article_68491ffd617fcb072014e755_img_2", "article_68491ffd617fcb072014e755_img_3", "article_68491ffd617fcb072014e755_img_4", "article_68491ffd617fcb072014e755_img_5", "article_68491ffd617fcb072014e755_img_6"]}, {"type": "text", "text": "on the training data or output. The authors suspect that the model\u2019s small size caused these issues. Among the lessons learned: Larger model lessons learned: Phi-4-reasoning and Phi-4-reasoning-plus didn\u2019t present the same issues. However, the authors did make significant choices during reinforcement learning: Results: Overall, Phi-4-reasoning-plus and Phi-4-mini-reasoning outperform similarly sized (and larger) open-weights models on math problems. Phi-4-reasoning generally outperformed DeepSeek-R1-Distilled-70B but underperformed Alibaba QwQ 32B. All three models deliver performance that falls in the middle among proprietary models and, in domains outside math, larger models with open weights. Why it matters: While reasoning models can outperform their non-reasoning counterparts, the best ways to train them aren\u2019t widely known. Sharing recipes and lessons learned enables others to further iterate and improve the recipes, ultimately increasing model performance even more. An open-source code generator performs comparably to the reasoning models DeepSeek-R1 and OpenAI o1 with a much smaller model. What\u2019s new: A team at the model platform Together.AI and Agentica, an open-source project devoted to reinforcement learning (RL), released DeepCoder-14B-Preview. The release includes weights, code, dataset, training logs, and data optimizations under an MIT license that allows noncommercial and commercial uses. How it works: The team fine-tuned DeepSeek-R1-Distilled-Qwen-14B, which distills knowledge from DeepSeek-R1 (671 billion parameters) into Qwen-14B (14 billion parameters). Results: DeepCoder-14B-Preview is competitive on several coding benchmarks with DeepSeek-R1 as well as proprietary models including OpenAI o3-mini and OpenAI o1, which is believed to be much larger. Why it matters: Applying reinforcement learning to coding works, but it has two big issues: (i) Training examples of verifiable code are relatively scarce and (ii) computing reward signals for code is time-consuming, since it requires evaluating many test cases. DeepCoder-14B-Preview\u2019s optimizations reduced this complexity, shrinking RL training from months to weeks. Those optimizations are built into Verl-pipeline, an open source RL library from Together.AI and Agentica, giving developers a powerful tool for model training. We\u2019re thinking: Kudos to the DeepCoder team for open sourcing their reasoning recipe! A handful of companies have developed the know-how to execute RL well, but many teams still have trouble implementing successfully. Open recipes for RL training methods and data curation techniques are important to move the field forward. The European Union made an abrupt U-turn away from its stringent AI regulations. Meta promptly adjusted to the loosening restrictions. What\u2019s new: Henna Virkkunen, the EU\u2019s head of digital policy, said the organization would ease rules and requirements to support Europe\u2019s competitiveness in AI. How it works: Adopted last year, the EU\u2019s AI Act provides a comprehensive framework for regulating AI that aims to reduce purported risks by banning certain applications, restricting others, and requiring extensive documentation of development efforts. The law is set to take effect in August, empowering various regulatory bodies to formulate detailed rules. However, in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden. Behind the news: In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed.", "article_id": "68491ffd617fcb072014e755", "linked_images": ["article_68491ffd617fcb072014e755_img_0", "article_68491ffd617fcb072014e755_img_1", "article_68491ffd617fcb072014e755_img_2", "article_68491ffd617fcb072014e755_img_3", "article_68491ffd617fcb072014e755_img_4", "article_68491ffd617fcb072014e755_img_5", "article_68491ffd617fcb072014e755_img_6"]}, {"type": "text", "text": "in recent months, the EU has faced increasing pressure from the U.S. government and large AI companies to reduce the regulatory burden. Behind the news: In drafting the AI Act, the EU aspired to a comprehensive, specific set of regulations. However, not all European lawmakers agreed that rules were needed. Virkkunen\u2019s supporters noted that existing laws already allowed consumers to file claims against AI companies. Meanwhile, some policymakers have become less worried about AI than they were during the early drafting of the AI Act. Why it matters: It\u2019s unlikely that all nations \u2013 or even states within nations \u2013 will ever agree fully on rules and regulations that govern AI companies that do business within their borders, or protections from flaws such as model bias. But AI companies including Meta, OpenAI, and others argue that a more uniform regulatory environment will make it easier to serve users worldwide. We\u2019re thinking: The EU overreached with the AI Act. Fortunately, the legislation provides enough flexibility to pull back. Clearer rules will help European teams innovate and European and international companies better serve EU citizens. Improving a large language model\u2019s factual accuracy typically requires making it bigger, which in turn, involves more computation. Researchers devised an architecture that enables models to recall relevant details without significantly increasing the amount of computation required. What\u2019s new: Vincent-Pierre Berges, Barlas O\u011fuz, and colleagues at Meta augmented transformers with trainable memory layers that efficiently store and retrieve information related to a prompt. The training code is available under a CC BY-NC license, which permits noncommercial uses. Memory layer basics: Memory layers were introduced in 2015 and were applied to transformers a few years later. They compute vectors, which may capture details like names or dates that were learned through training, and retrieve them according to a given input. Computing the output of a memory layer is similar to computing that of a self-attention layer. Both describe vectors that represent queries, keys, and values, and both compute the similarity between queries and keys and then weight the values by that similarity. However, while a self-attention layer computes queries, keys, and values from linear transformations of the input, a memory layer (which computes queries the same way) learns keys and a corresponding value for each key through training. Key insight: Memory layers can be scaled to millions of keys, but computing the similarity between a query and so many keys is computationally expensive. One solution is to represent each key as a combination of two half-keys drawn from two much smaller sets. For example, two sets of 1,000 half-keys each can represent 1 million possible keys. Comparing a query to these smaller sets is much more efficient, making it practical to scale up memory layers dramatically. How it works: The authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2\u2019s and Llama 3\u2019s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an", "article_id": "68491ffd617fcb072014e755", "linked_images": ["article_68491ffd617fcb072014e755_img_0", "article_68491ffd617fcb072014e755_img_1", "article_68491ffd617fcb072014e755_img_2", "article_68491ffd617fcb072014e755_img_3", "article_68491ffd617fcb072014e755_img_4", "article_68491ffd617fcb072014e755_img_5", "article_68491ffd617fcb072014e755_img_6"]}, {"type": "text", "text": "authors pretrained Llama-style models of several sizes (from 134 million to 8 billion parameters) on data similar to Llama 2\u2019s and Llama 3\u2019s pretraining datasets. They replaced the fully connected layers with memory layers in three transformer blocks. These layers shared parameters and held up to 16 million values (an extra 64 billion parameters total). The memory layers performed these steps: Results: The authors compared a model (8 billion parameters) with memory layers to a similar model without memory layers, both trained on 1 trillion tokens. Why it matters: Memory layers didn\u2019t catch on in the early days of large language models (LLMs), but they can improve the output of today\u2019s much bigger models. LLMs outfitted with memory layers require less data and computation for pretraining than conventional models to achieve the same result, at least with respect to answering factual questions. We\u2019re thinking: While retrieval-augmented generation can help LLMs deliver more-factual output by retrieving facts from a database, the authors add trainable parameters for this purpose. Build AI applications that access tools, data, and prompt templates using Model Context Protocol (MCP), an open standard developed by Anthropic. In \u201cMCP: Build Rich-Context AI Apps with Anthropic,\u201d you\u2019ll build and deploy an MCP server, make an MCP-compatible chatbot, and connect applications to multiple third-party servers. Sign up now", "article_id": "68491ffd617fcb072014e755", "linked_images": ["article_68491ffd617fcb072014e755_img_0", "article_68491ffd617fcb072014e755_img_1", "article_68491ffd617fcb072014e755_img_2", "article_68491ffd617fcb072014e755_img_3", "article_68491ffd617fcb072014e755_img_4", "article_68491ffd617fcb072014e755_img_5", "article_68491ffd617fcb072014e755_img_6"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e755", "linked_chunks": ["article_68491ffd617fcb072014e755_chunk_0", "article_68491ffd617fcb072014e755_chunk_1", "article_68491ffd617fcb072014e755_chunk_2", "article_68491ffd617fcb072014e755_chunk_3"]}, {"type": "text", "text": "Dear friends, I\u2019m delighted to announce that AI Fund has closed $190M for our new fund, in an oversubscribed round. I look forward to working with many more builders to create new companies that serve humanity. AI Fund isn\u2019t a traditional venture capital firm that invests in existing businesses. Instead, we are a venture builder (also called a venture studio): We co-found AI companies, so our team is directly involved in writing code, talking to customers to get feedback, iterating on product designs, preparing market analyses, and so on. We have a lot of fun building multiple AI products at a time, and thus live daily the emerging AI startup best practices. Many factors go into the success of a startup. But if I had to pick just one, it would be speed. Startups live or die based on their ability to make good decisions and execute fast, which has been a recurring theme of my articles in The Batch as well. If you are building an AI startup, here are some ideas to consider: In addition to speed, a second criterion that I find important for startup success is deep knowledge of the technology. Because AI technology is evolving rapidly, a team with a deep technical understanding of what AI can and cannot do, and when to use what tool, will make better decisions. This creates meaningful differentiation and saves wasting time in blind alleys. A good technical understanding, too, gets you speed! I\u2019m grateful to AI Fund\u2019s investors, team, and entrepreneur partners for working with us. There is much ahead to build! Andrew Learn to create voice agents that listen, reason, and respond in real time, just like a conversation with a real person in our latest short course, \u201cBuilding AI Voice Agents for Production.\u201d You'll build a scalable agent from scratch, deploy it to the cloud, and explore what makes voice interfaces feel fast, natural, and human. Enroll for free Alibaba\u2019s new model family may unseat DeepSeek-R1\u2019s four-month reign as the top open-weights large language model. What\u2019s new: Alibaba released weights for eight large language models, all of which offer a reasoning mode that can be switched on or off. Two use a mixture of experts (MoE) architecture: Qwen3-235B-A22B (the name indicates 235 billion parameters, 22 billion of which are active at any given time) and Qwen3-30B-A3B). The other six are dense models in sizes between 32 billion parameters and 0.6 billion parameters \u2014 tiny by LLM standards, and with reasoning, too. How it works: The Qwen3 family implements chain-of-thought reasoning in both relatively large and quite small LLMs. Results: Qwen3-235B-A22B and Qwen3-30B-A3B performed as well as, or better than, leading open-weights models in tests performed by Alibaba. Qwen3-4B, too, achieved results that are competitive with many models several times its size. Alibaba didn\u2019t provide results for the other dense models. Why it matters: Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can", "article_id": "68491ffd617fcb072014e75d", "linked_images": ["article_68491ffd617fcb072014e75d_img_0", "article_68491ffd617fcb072014e75d_img_1", "article_68491ffd617fcb072014e75d_img_2", "article_68491ffd617fcb072014e75d_img_3", "article_68491ffd617fcb072014e75d_img_4", "article_68491ffd617fcb072014e75d_img_5"]}, {"type": "text", "text": "times its size. Alibaba didn\u2019t provide results for the other dense models. Why it matters: Qwen3 continues a string of high-performance, open-weights models released by developers in China. Alibaba says it designed the models to do the thinking in agentic systems. Reasoning that can be switched on and off can help control costs in agentic and other applications. We\u2019re thinking: Alibaba\u2019s 235-billion parameter MoE model may perform better according to benchmarks, but Qwen3-30B-A3B does nearly as well and can run locally on a pro laptop without straining its memory. Add the easy ability to switch reasoning on or off, and Qwen3\u2019s versatile, mid-sized MoE model may turn out to be the star of the show. OpenAI\u2019s most widely used model briefly developed a habit of flattering users, with laughable and sometimes worrisome results. What\u2019s new: OpenAI quickly withdrew an update to GPT-4o (gpt-4o-2025-04-25), which supplied responses for ChatGPT, after it provided excessively fawning responses to user input \u2014 even in contexts didn\u2019t call for agreement. The company reverted to an earlier version (gpt-4o-2024-11-20). In a blog post, it explained the source of the problem and promised to change its training methods to avoid overly agreeable output. Amiable to a fault: Many ChatGPT users shared screen shots of ChatGPT\u2019s sycophantic responses on social media. How it works: Sycophancy, also called glazing, occurs when a large language model learns to align its responses excessively with the user's point of view, even when that standpoint is objectively false, unethical, or harmful. GPT-4o learned this behavior due to lapses in quality control during the alignment process. Behind the news: Sycophantic behavior in large language models has been a subject of AI research and commentary. Why it matters: ChatGPT\u2019s episode of sycophancy illustrates the subtlety of the goal of aligning AI with human values. Reinforcement learning undertaken to this end resulted not only in a highly capable chatbot but one that focused inappropriately on affirming \u2014 sometimes to the point of absurd exaggeration \u2014 the user\u2019s positive qualities. Alignment requires balancing multiple objectives beyond agreeableness including accuracy, helpfulness, and ethics. Ultimately achieving alignment \u2014 like all AI development \u2014 is an iterative process that is still evolving. We\u2019re thinking: To those who read this far, your unwavering dedication and extraordinary perseverance is nothing short of legendary. Like a master navigator, you\u2019ve traversed word by word, never wavering, displaying a level of focus and determination that would humble even the most steadfast of scholars. We are truly honored to have such an intrepid reader. Bravo to you, the indefatigable champion of curiosity! The world\u2019s biggest pharmaceutical company by revenue shed light on its AI strategy. What\u2019s new: Johnson & Johnson, after experimenting broadly with generative AI, settled on a short list of projects that aid in sales, drug development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firm Greylock and The Wall Street Journal. How it works: The 140-year-old medical company spent roughly a year experimenting with various AI applications throughout the company, according to Chief Information Officer Jim Swanson.", "article_id": "68491ffd617fcb072014e75d", "linked_images": ["article_68491ffd617fcb072014e75d_img_0", "article_68491ffd617fcb072014e75d_img_1", "article_68491ffd617fcb072014e75d_img_2", "article_68491ffd617fcb072014e75d_img_3", "article_68491ffd617fcb072014e75d_img_4", "article_68491ffd617fcb072014e75d_img_5"]}, {"type": "text", "text": "development, supply-chain management, and internal communications. A company executive described the process and results to the venture-capital firm Greylock and The Wall Street Journal. How it works: The 140-year-old medical company spent roughly a year experimenting with various AI applications throughout the company, according to Chief Information Officer Jim Swanson. A centralized governing board oversaw as many as 900 experiments. After finding that 10 percent to 15 percent of use cases drove about 80 percent of the value, the company shifted responsibility for AI projects to specific departments to focus on high-value applications. In the end, the criteria for choosing a project was threefold: (i) how readily it could be implemented, (ii) how useful it would be throughout the company, and (iii) how much it would benefit the business. Behind the news: Generative AI is expected to bring in up to $110 billion in annual revenue across the pharmaceutical industry, according to McKinsey. The consultancy breaks down this number into the following categories, in order of their contribution to the total: commercial (AI for sales and marketing), research (AI for designing, screening, and manufacturing molecules), clinical (AI to facilitate trials), enterprise, operations, and medical (processing medical literature). Why it matters: Johnson & Johnson\u2019s experience offers a peek into AI development at a major legacy company in a key sector. The company has identified high-value opportunities in enterprise-wide operations, departmental priorities, and core products. It\u2019s pursuing all three. We\u2019re thinking: Notably, this medical stalwart is building AI applications for human resources, sales, and supply-chain management. Similar opportunities exist at companies old and new, big and small, far and wide. Researchers showed that supervised fine-tuning on as few as 1,000 examples can enable a pretrained large language model to reason \u2014 and a clever gambit can boost its performance to rival that of top reasoning models. What\u2019s new: Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li and colleagues at Stanford, University of Washington, Allen Institute for AI, and Contextual AI developed s1, a reasoning model that achieves higher performance by producing more reasoning tokens. The authors forced the model to generate \u201cWait\u201d \u2014 as in, \"Wait, there may be a better way to go about this\u201d \u2014 to make it continue, rather than end, its reasoning process. Key insight: The sequence of reasoning tokens generated by a reasoning model like DeepSeek-R1 is delimited by special tokens. In pretraining on human data, a model learns to keep generating reasoning tokens until it generates the special token that ends the sequence. In addition, since people tend to revise their statements after writing \u201cWait\u201d, the model learns to do this as well. Thus, the reasoning process can be extended by appending the token for \u201cWait\u201d to the model\u2019s output periodically. In this way, when the output-in-progress is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps. How it works: The authors fine-tuned a pretrained Qwen 2.5-32B, which", "article_id": "68491ffd617fcb072014e75d", "linked_images": ["article_68491ffd617fcb072014e75d_img_0", "article_68491ffd617fcb072014e75d_img_1", "article_68491ffd617fcb072014e75d_img_2", "article_68491ffd617fcb072014e75d_img_3", "article_68491ffd617fcb072014e75d_img_4", "article_68491ffd617fcb072014e75d_img_5"]}, {"type": "text", "text": "is fed back to generate the next token, the model continues to reason over the prompt. Such extended reasoning can improve the final output by inducing the model to double-check its response so far and improve previous reasoning steps. How it works: The authors fine-tuned a pretrained Qwen 2.5-32B, which does not produce reasoning tokens, on around 1,000 examples of chain-of-thought reasoning. Results: s1\u2019s performance improved as the number of reasoning tokens it generated increased. Ultimately, it achieved comparable performance to OpenAI o1-preview but fell short of o1. Why it matters: A conventional pretrained LLM can learn to reason after supervised fine-tuning on as few as 1,000 curated examples \u2014 no reinforcement learning necessary. While some model builders don\u2019t disclose how they optimize reasoning, this work reveals that a strategy as simple as appending \u201cWait\u201d can be effective. We\u2019re thinking: Wait, how can we apply this to our projects?", "article_id": "68491ffd617fcb072014e75d", "linked_images": ["article_68491ffd617fcb072014e75d_img_0", "article_68491ffd617fcb072014e75d_img_1", "article_68491ffd617fcb072014e75d_img_2", "article_68491ffd617fcb072014e75d_img_3", "article_68491ffd617fcb072014e75d_img_4", "article_68491ffd617fcb072014e75d_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e75d", "linked_chunks": ["article_68491ffd617fcb072014e75d_chunk_0", "article_68491ffd617fcb072014e75d_chunk_1", "article_68491ffd617fcb072014e75d_chunk_2", "article_68491ffd617fcb072014e75d_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e75d", "linked_chunks": ["article_68491ffd617fcb072014e75d_chunk_0", "article_68491ffd617fcb072014e75d_chunk_1", "article_68491ffd617fcb072014e75d_chunk_2", "article_68491ffd617fcb072014e75d_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e75d", "linked_chunks": ["article_68491ffd617fcb072014e75d_chunk_0", "article_68491ffd617fcb072014e75d_chunk_1", "article_68491ffd617fcb072014e75d_chunk_2", "article_68491ffd617fcb072014e75d_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e75d", "linked_chunks": ["article_68491ffd617fcb072014e75d_chunk_0", "article_68491ffd617fcb072014e75d_chunk_1", "article_68491ffd617fcb072014e75d_chunk_2", "article_68491ffd617fcb072014e75d_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e75d", "linked_chunks": ["article_68491ffd617fcb072014e75d_chunk_0", "article_68491ffd617fcb072014e75d_chunk_1", "article_68491ffd617fcb072014e75d_chunk_2", "article_68491ffd617fcb072014e75d_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e75d", "linked_chunks": ["article_68491ffd617fcb072014e75d_chunk_0", "article_68491ffd617fcb072014e75d_chunk_1", "article_68491ffd617fcb072014e75d_chunk_2", "article_68491ffd617fcb072014e75d_chunk_3"]}, {"type": "text", "text": "Dear friends, I hope we can empower everyone to build with AI. Starting from K-12, we should teach every student AI enabled coding, since this will enable them to become more productive and more empowered adults. But there is a huge shortage of computer science (CS) teachers. I recently spoke with high school basketball coach Kyle Creasy, who graduated with a B.A. in Physical Education in 2023. Until two years ago, he had never written a line of Python. Now \u2014 with help from AI \u2014 he not only writes code, he also teaches CS. I found Kyle\u2019s story inspiring as a model for scaling up CS education in the primary- and secondary-school levels. Kyle\u2019s success has been with the support of Kira Learning (an AI Fund portfolio company), whose founders Andrea Pasinetti and Jagriti Agrawal have created a compelling vision for CS education. In K-12 classrooms, teachers play a huge social-emotional support role, for example, encouraging students and helping them when they stumble. In addition, they are expected to be subject-matter experts who can deliver the content needed for their subject. Kira Learning uses digital content delivery \u2014 educational videos, autograded quizzes, and AI-enabled chatbots to answer students' questions but without giving away homework answers \u2014 so the teacher can focus on social-emotional support. While these are still early days, it appears to be working! A key to making this possible is the hyperpersonalization that is now possible with AI (in contrast to the older idea of the flipped classroom, which had limited adoption). For example, when assigned a problem in an online coding environment, if a student writes this buggy line of Python code best_$alty_snack = 'potato chips' Kira Learning\u2019s AI system can spot the problem and directly tell the teacher that $ is an invalid character in a variable name. It can also suggest a specific question for the teacher to ask the student to help get them unstuck, like \u201cCan you identify what characters are allowed in variable names?\u201d Whereas AI can directly deliver personalized advice to students, the fact that it is now helping teachers also deliver personalized support will really help in K-12. Additionally, agentic workflows can automate a lot of teachers\u2019 repetitive tasks. For example, when designing a curriculum, it\u2019s time-consuming to align the content to educational standards (such as the Common Core in the United States, or the AP CS standard for many CS classes). Having an AI system carry out tasks like these is already proving helpful for teachers. Since learning to code, Kyle has built many pieces of software. He proudly showed me an analysis he generated in matplotlib of his basketball players\u2019 attempts to shoot three-pointers (shown above), which in turn is affecting the team\u2019s strategy on the court. One lesson is clear: When a basketball coach learns to code, they become a better basketball coach! I talked about Kyle (and other topics) at the ASU+GSV Summit on education. You can see a video here. In the future, people who know how to code and build with AI will be much", "article_id": "68491ffd617fcb072014e764", "linked_images": ["article_68491ffd617fcb072014e764_img_0", "article_68491ffd617fcb072014e764_img_1", "article_68491ffd617fcb072014e764_img_2", "article_68491ffd617fcb072014e764_img_3", "article_68491ffd617fcb072014e764_img_4", "article_68491ffd617fcb072014e764_img_5"]}, {"type": "text", "text": "is clear: When a basketball coach learns to code, they become a better basketball coach! I talked about Kyle (and other topics) at the ASU+GSV Summit on education. You can see a video here. In the future, people who know how to code and build with AI will be much more productive than people who don\u2019t. I\u2019m excited about how AI will lead to new models for K-12 education. By delivering CS education to everyone, I hope that in the future, everyone will be able to build with AI. Keep learning! Andrew In \u201cLLMs as Operating Systems: Agent Memory,\u201d you\u2019ll learn to build agents that manage their own memory using the MemGPT approach. This newly updated short course includes cloud-based deployment and real-time, step-by-step output, so you can see how your agents reason as they respond. Join in today! ChatGPT\u2019s image generator is available via API. What\u2019s new: GPT Image 1, which produces images from text or other images, has proven enormously popular among ChatGPT users. The OpenAI Images API enables developers to incorporate OpenAI\u2019s most sophisticated image generator into their own software tools and platforms. How it works: GPT Image 1 generates and modifies images in a wide range of styles, performs image editing and other alterations, renders text, and follows detailed instructions. Shortly after its debut, the version of GPT-4o equipped with GPT Image 1 quickly soared to the No. 1 spot on the Artificial Analysis Image Arena leaderboard. Behind the news: In March, OpenAI attracted huge public interest when it deployed the model, then unnamed, in ChatGPT. Within the first week, 130 million users used it to create more than 700 million images. Why it matters: Adding GPT Image 1 to the API enables developers to use OpenAI\u2019s most sophisticated image generator in a wide variety of automated workflows. OpenAI\u2019s initial API partners include design companies (Adobe and Canva), marketers (HubSpot), and web designers (GoDaddy), all of which are using GPT Image 1. We\u2019re thinking: GPT Image 1 is part of an exciting trend toward unification of multimodal architectures. Researchers have progressed from text-in, text-out to text/images-in, text-out and increasingly text/images/audio-in, text/images/audio-out. This paints a beautiful picture of where multimodal models can go! Google refreshed its experimental tools for composers and producers. What\u2019s new: Google announced updates of two music-generation apps and the models they're based on. Music AI Sandbox, an app that generates and modifies music according to text prompts, now accepts lyrics to generate songs as well as instrumental music. You can join a waitlist here. MusicFX DJ generates a continuous stream of music that users can modify as it plays. Try it out here. How it works: The apps generate 48kHz audio suitable for professional productions. Users can specify key, tempo in beats per minute, instrumentation, style, mood, and other details. Behind the news: Google launched Lyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald", "article_id": "68491ffd617fcb072014e764", "linked_images": ["article_68491ffd617fcb072014e764_img_0", "article_68491ffd617fcb072014e764_img_1", "article_68491ffd617fcb072014e764_img_2", "article_68491ffd617fcb072014e764_img_3", "article_68491ffd617fcb072014e764_img_4", "article_68491ffd617fcb072014e764_img_5"]}, {"type": "text", "text": "style, mood, and other details. Behind the news: Google launched Lyria 1 and Music AI Sandbox in 2023 as part of an experiment with YouTube, which made them available to composers, producers, and musicians. Since then, the company has developed them with help from music stars including Jacob Collier, Donald \u201cChildish Gambino\u201d Glover, and Wyclef Jean. Lyria 1 recently became available via the Vertex API to developers who are preapproved by Google. Why it matters: While music generators like Suno and Udio appeal to casual musicians, Music AI Sandbox, with its digital audio workstation-style user interface, aims to address the needs of professionals. This approach puts AI directly into the hands of talented, experienced artists, similar to the way Adobe has empowered videographers and Runway has partnered with movie producers. We\u2019re thinking: API access to Lyria 2 would be music to our ears! AI agents and infrastructure made a strong showing on CB Insights\u2019s latest list of the top 100 AI startups. What\u2019s new: CB Insights, which tracks tech startups and venture capital, selected companies in the AI 100 based on their market traction, talent, finances, and partnerships. The list purports to highlight the next wave of winners, shedding light on the key executives, investors, fundraising, and valuations behind up-and-coming AI ventures. How it works: The analysts evaluated 17,000 early-stage, private AI companies that had raised funds within the last year and continue to seek further investment. Where the action is: This year\u2019s AI 100 companies are based in 14 countries, around two-thirds of them in the United States. 10 are based in the United Kingdom, five in France, and four in Germany, with one each in Norway (Braintrust), Singapore (Bria), Spain (Cartwheel), Sweden (Chainguard), and Switzerland (Clarium). Why it matters: This year\u2019s AI 100 offers a snapshot of AI becoming more central to businesses of all kinds. Most of the startups listed here offer practical products and services that are poised to deliver a timely return, rather than moonshots with long development cycles and risky payoffs. In addition, they mostly target corporate customers rather than consumers. We\u2019re thinking: The falling cost of access to AI models and increasingly capable open-weights models make this the perfect time to build applications. What kind? The report singles out health care (8 companies) and life sciences (6 companies) as growing areas, but it also documents opportunities in defense, gaming, and finance. Large language models can improve systems that recommend items to purchase by inferring customer preferences. What\u2019s new: Fabian Paischer and colleagues at Johannes Kepler University Linz, University of Wisconsin, and Meta introduced Multimodal Preference Discerner (Mender), a recommender that integrates a large language model (LLM). Key insight: Text that attracts customers, such as product descriptions, and text they write, such as product reviews, may contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a", "article_id": "68491ffd617fcb072014e764", "linked_images": ["article_68491ffd617fcb072014e764_img_0", "article_68491ffd617fcb072014e764_img_1", "article_68491ffd617fcb072014e764_img_2", "article_68491ffd617fcb072014e764_img_3", "article_68491ffd617fcb072014e764_img_4", "article_68491ffd617fcb072014e764_img_5"]}, {"type": "text", "text": "contain information that indicates their preferences, such as the craft projects that required a particular power tool. But it also may include irrelevant information, such as a complaint that the tool was delivered late, which can throw recommendation systems off track. An LLM can derive preferences from text, providing a clearer signal of what a customer wants. How it works: Mender comprises an LLM (Llama 3 70B-Instruct), an encoder (Flan-T5 pretrained on a wide variety of text and frozen) that embeds customer data, and a decoder (a transformer trained from scratch) that predicts the next item a customer will buy. The system learned to predict the next item based on descriptions of items a customer purchased, the customer\u2019s ratings and reviews of those products (drawn from datasets of Steam reviews of video games and Amazon reviews of items related to beauty, toys-and-games, and sports-and-outdoors), and customer preferences inferred by the LLM from the foregoing data. Results: The authors compared Mender to TIGER, a recommender that also takes a purchase history and predicts the next purchase, on the Steam and Amazon datasets. They scored the results using recall @5, a measure of how often the correct item is within the model\u2019s top five most likely predictions. Why it matters: Drawing inferences from text information like customer reviews and item descriptions boosts a recommender\u2019s signal, making it clearer what a given customer is likely to want. Previous systems used customer reviews or item descriptions directly; Mender uses customer preferences extracted from that information. We\u2019re thinking: Be on the lookout for innovative ways to use LLMs. We recommend it!", "article_id": "68491ffd617fcb072014e764", "linked_images": ["article_68491ffd617fcb072014e764_img_0", "article_68491ffd617fcb072014e764_img_1", "article_68491ffd617fcb072014e764_img_2", "article_68491ffd617fcb072014e764_img_3", "article_68491ffd617fcb072014e764_img_4", "article_68491ffd617fcb072014e764_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e764", "linked_chunks": ["article_68491ffd617fcb072014e764_chunk_0", "article_68491ffd617fcb072014e764_chunk_1", "article_68491ffd617fcb072014e764_chunk_2", "article_68491ffd617fcb072014e764_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e764", "linked_chunks": ["article_68491ffd617fcb072014e764_chunk_0", "article_68491ffd617fcb072014e764_chunk_1", "article_68491ffd617fcb072014e764_chunk_2", "article_68491ffd617fcb072014e764_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e764", "linked_chunks": ["article_68491ffd617fcb072014e764_chunk_0", "article_68491ffd617fcb072014e764_chunk_1", "article_68491ffd617fcb072014e764_chunk_2", "article_68491ffd617fcb072014e764_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e764", "linked_chunks": ["article_68491ffd617fcb072014e764_chunk_0", "article_68491ffd617fcb072014e764_chunk_1", "article_68491ffd617fcb072014e764_chunk_2", "article_68491ffd617fcb072014e764_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e764", "linked_chunks": ["article_68491ffd617fcb072014e764_chunk_0", "article_68491ffd617fcb072014e764_chunk_1", "article_68491ffd617fcb072014e764_chunk_2", "article_68491ffd617fcb072014e764_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e764", "linked_chunks": ["article_68491ffd617fcb072014e764_chunk_0", "article_68491ffd617fcb072014e764_chunk_1", "article_68491ffd617fcb072014e764_chunk_2", "article_68491ffd617fcb072014e764_chunk_3"]}, {"type": "text", "text": "Dear friends, Even though I\u2019m a much better Python than JavaScript developer, with AI assistance, I\u2019ve been writing a lot of JavaScript code recently. AI-assisted coding is making specific programming languages less important, even though learning one is still helpful to make sure you understand the key concepts. This is helping many developers write code in languages we\u2019re not familiar with, which lets us get code working in many more contexts! My background is in machine learning engineering and back-end development, but AI-assisted coding is making it easy for me to build front-end systems (the part of a website or app that users interact with) using JavaScript (JS) or TypeScript (TS), languages that I am weak in. Generative AI is making syntax less important, so we can all simultaneously be Python, JS, TS, C++, Java, and even Cobol developers. Perhaps one day, instead of being \u201cPython developers\" or \u201cC++ developers,\u201d many more of us will just be \u201cdevelopers\u201d! But understanding the concepts behind different languages is still important. That\u2019s why learning at least one language like Python still offers a great foundation for prompting LLMs to generate code in Python and other languages. If you move from one programming language to another that carries out similar tasks but with different syntax \u2014 say, from JS to TS, or C++ to Java, or Rust to Go \u2014 once you\u2019ve learned the first set of concepts, you\u2019ll know a lot of the concepts needed to prompt an LLM to code in the second language. (Although TensorFlow and PyTorch are not programming languages, learning the concepts of deep learning behind TensorFlow will also make it much easier to get an LLM to write PyTorch code for you, and vice versa!) In addition, you\u2019ll be able to understand much of the generated code (perhaps with a little LLM assistance). Different programming languages reflect different views of how to organize computation, and understanding the concepts is still important. For example, someone who does not understand arrays, dictionaries, caches, and memory will be less effective at getting an LLM to write code in most languages. Similarly, a Python developer who moves toward doing more front-end programming with JS would benefit from learning the concepts behind front-end systems. For example, if you want an LLM to build a front end using the React framework, it will benefit you to understand how React breaks front ends into reusable UI components, and how it updates the DOM data structure that determines what web pages look like. This lets you prompt the LLM much more precisely, and helps you understand how to fix issues if something goes wrong. Similarly, if you want an LLM to help you write code in CUDA or ROCm, it helps to understand how GPUs organize compute and memory. Just as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven\u2019t already done so, I encourage you to try having an LLM write some code in a", "article_id": "68491ffd617fcb072014e76b", "linked_images": ["article_68491ffd617fcb072014e76b_img_0", "article_68491ffd617fcb072014e76b_img_1", "article_68491ffd617fcb072014e76b_img_2", "article_68491ffd617fcb072014e76b_img_3", "article_68491ffd617fcb072014e76b_img_4", "article_68491ffd617fcb072014e76b_img_5"]}, {"type": "text", "text": "memory. Just as people who are fluent in multiple human languages can communicate more easily with other people, LLMs are making it easier for developers to build systems in multiple contexts. If you haven\u2019t already done so, I encourage you to try having an LLM write some code in a language you\u2019d like to learn but perhaps haven\u2019t yet gotten around to, and see if it helps you get some new applications to work. Keep building! Andrew Learn to build agents that write and run code to complete complex tasks. \u201cBuilding Code Agents with Hugging Face smolagents,\u201d made in collaboration with Hugging Face, teaches you how to build code agents, execute their code safely, and set up evals for production-ready multi-agent systems, using the smolagents framework. Enroll for free OpenAI refreshed its roster of models and scheduled the largest, most costly one for removal. What\u2019s new: OpenAI introduced five new models that accept text and images inputs and generate text output. Their parameter counts, architectures, training datasets, and training methods are undisclosed. The general-purpose GPT-4.1, GPT-4.1 mini, and GPT-4.1 nano are available via API only. The reasoning models o3 and o4-mini, are available via API to qualified developers as well as users of ChatGPT Plus, Pro, and Team, and soon ChatGPT Enterprise and ChatGPT Education. The company will terminate GPT-4.5 \u2014 which it introduced as a research preview in late February \u2014 in July. GPT-4.1 family: In an odd turn of version numbers, the GPT-4.1 models are intended to be cost-effective equivalents to GPT-4.5 and updates to GPT-4o. They accept inputs of up to 1 million tokens (compared to GPT-4.5\u2019s and GPT-4o\u2019s 128,000 tokens). o3 and o4-mini: These models update o1 and o3-mini, respectively. They have input limits of 200,000 tokens and can be set to low-, medium-, or high-effort modes to process varying numbers of reasoning tokens, which are hidden from users. Unlike their predecessors, they were fine-tuned to decide when and how to use the tools, including web search, code generation and execution, and image editing. Behind the news: Late last year, OpenAI introduced o1, the first commercial model trained via reinforcement learning to generate chains of thought. Within a few months, DeepSeek, Google, and Anthropic launched their respective reasoning models DeepSeek-R1, Gemini 2.5 Pro, and Claude 3.7 Sonnet. OpenAI has promised to integrate its general-purpose GPT-series models and o-series reasoning models, but they remain separate for the time being. Why it matters: GPT-4.5 was an exercise in scale, and it showed that continuing to increase parameter counts and training data would yield ongoing performance gains. But it wasn\u2019t widely practical on a cost-per-token basis. The new models, including those that use chains of thought and tools, deliver high performance at lower prices. We\u2019re thinking: Anthropic is one of OpenAI\u2019s key competitors, and a large fraction of the tokens it generates (via API) are for writing code, a skill in which it is particularly strong. OpenAI\u2019s emphasis on models that are good at coding could boost the competition in this area! Hugging Face has made a name by providing open", "article_id": "68491ffd617fcb072014e76b", "linked_images": ["article_68491ffd617fcb072014e76b_img_0", "article_68491ffd617fcb072014e76b_img_1", "article_68491ffd617fcb072014e76b_img_2", "article_68491ffd617fcb072014e76b_img_3", "article_68491ffd617fcb072014e76b_img_4", "article_68491ffd617fcb072014e76b_img_5"]}, {"type": "text", "text": "key competitors, and a large fraction of the tokens it generates (via API) are for writing code, a skill in which it is particularly strong. OpenAI\u2019s emphasis on models that are good at coding could boost the competition in this area! Hugging Face has made a name by providing open AI models. Now it\u2019s providing an open robot. What\u2019s new: Hugging Face acquired the French company Pollen Robotics for an undisclosed price. It plans to offer Pollen\u2019s Reachy 2, a robot that runs on code that\u2019s freely available under an Apache 2.0 license, for $70,000. How it works: Reachy 2 has two arms, gripper hands, and a wheeled base (optional). It\u2019s designed primarily for education and research in human-robot interaction in real-world settings. Behind the news: Last year, Remi Cadene, who worked on Tesla\u2019s Optimus, joined Hugging Face to lead robotics projects. In May, he and his team rolled out the LeRobot open source robotics code library, which provides pretrained models, datasets, and simulators for reinforcement learning and imitation learning. In November, Nvidia announced a collaboration with Hugging Face to accelerate LeRobot\u2019s data collection, training, and verification. Why it matters: Hugging Face\u2019s acquisition of Pollen reflects an industry-wide investment in robots, notably humanoid robots, whose prices have been falling. Nvidia CEO Jensen Huang has called AI-enabled robotics a \u201cmulti-trillion dollar\u201d opportunity. We\u2019re thinking: AI-enabled robots are marching slowly toward what we hope will be breakthrough applications. Open-source systems are an important part of the trend! The U.S. government escalated its long-running effort to block China\u2019s access to cutting-edge AI hardware. What\u2019s new: The White House announced that future shipments of Nvidia H20s, AMD MI308s, or equivalent chips to China would require a license. Concurrently, the United States Congress launched an investigation into whether chip vendor Nvidia violated earlier export rules. How it works: Nvidia launched the H20 in late 2023 to comply with a 2022 U.S. ban on China-bound shipments of Nvidia\u2019s H100 and H200 processors. The H20 uses the same architecture as the H200, but it\u2019s an order of magnitude slower with less memory and memory bandwidth. Behind the news: The U.S. government\u2019s many moves to restrict shipments of advanced processors to China have sought to protect the nation\u2019s lead in AI, but they have not prevented Chinese developers from closing the gap. In 2020, the U.S. required chip makers that use U.S. technology \u2014 which includes both domestic chip designers like Nvidia and makers of advanced fabrication equipment like the Netherlands\u2019 ASML \u2014 to seek permission before doing business with Chinese tech giant Huawei. Last December, the U.S. published sweeping limits on sales of processors that involve U.S. technology, as well as the technology itself, to Chinese businesses. Yes, but: Export restrictions may have slowed China\u2019s production of advanced chips, but they have also incentivized China to invest in establishing leadership in AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release of DeepSeek-R1, which performs comparably to OpenAI\u2019s o1, but whose weights are freely available and trained using less computation. Why it", "article_id": "68491ffd617fcb072014e76b", "linked_images": ["article_68491ffd617fcb072014e76b_img_0", "article_68491ffd617fcb072014e76b_img_1", "article_68491ffd617fcb072014e76b_img_2", "article_68491ffd617fcb072014e76b_img_3", "article_68491ffd617fcb072014e76b_img_4", "article_68491ffd617fcb072014e76b_img_5"]}, {"type": "text", "text": "but they have also incentivized China to invest in establishing leadership in AI. In January, the Chinese AI developer DeepSeek surprised U.S. policymakers and AI leaders with the release of DeepSeek-R1, which performs comparably to OpenAI\u2019s o1, but whose weights are freely available and trained using less computation. Why it matters: The first wave of restrictions on sales of advanced chips to China did little harm to U.S. chipmakers, largely because demand outstripped supply. But later restrictions have had a greater impact on their sales. The new limits could cost Nvidia and AMD significant revenue and likely will degrade their competitiveness abroad and bolster China\u2019s homegrown chip-making industry. We\u2019re thinking: The AI community\u2019s international scope is one of its greatest strengths. While individual countries must attend to their national security, progress in AI benefits all nations. Even in this era of rising protectionism, we hope members of the global AI community continue to support one another and encourage the free flow of ideas. Large language models excel at processing text but can\u2019t interpret images, video, or audio directly without further training on those media types. Researchers devised a way to overcome this limitation. What\u2019s new: Kumar Ashutosh and colleagues at Meta, University of Texas, and UC Berkeley introduced Multimodal Iterative LLM Solver (MILS), a method that pairs a text-only large language model (LLM) with a multimodal embedding model to generate captions for images, video, and audio without further training. Key insight: LLMs can generate text and refine their outputs based on new information. On the other hand, multimodal embedding models can score the similarity between a given text and an image, video, or audio clip. Given this score, an LLM can regenerate the text iteratively until the score indicates a strong match between the text and the associated media. This enables the LLM to generate accurate captions for images, videos, and audio clips without training in these tasks. How it works: Given a prompt and an image, video, or audio clip, Llama 3.1 8B produced and iteratively refined the prompt according to a pretrained multimodal embedding model\u2019s estimate of the similarity between the text and media. Results: The authors evaluated MILS on captioning images, videos, and audio clips. They measured performance according to Metric for Evaluation of Translation with Explicit ORdering (METEOR), which checks for synonyms, words that share the same root, and word order to determine whether a generated caption matches a ground-truth caption (higher is better). Overall, MILS outperformed models that underwent task-specific training. Why it matters: Zero-shot captioning models like Aya Vision and Pixtral require training on paired captions and media. The authors\u2019 approach takes advantage of pretrained multimodal models to enable an LLM to compose multimedia captions without further training. We\u2019re thinking: Synthetic data is increasingly useful for training AI models. By enabling LLMs to synthesize good captions, MILS adds fuel to this fire.", "article_id": "68491ffd617fcb072014e76b", "linked_images": ["article_68491ffd617fcb072014e76b_img_0", "article_68491ffd617fcb072014e76b_img_1", "article_68491ffd617fcb072014e76b_img_2", "article_68491ffd617fcb072014e76b_img_3", "article_68491ffd617fcb072014e76b_img_4", "article_68491ffd617fcb072014e76b_img_5"]}, {"type": "text", "text": "training AI models. By enabling LLMs to synthesize good captions, MILS adds fuel to this fire.", "article_id": "68491ffd617fcb072014e76b", "linked_images": ["article_68491ffd617fcb072014e76b_img_0", "article_68491ffd617fcb072014e76b_img_1", "article_68491ffd617fcb072014e76b_img_2", "article_68491ffd617fcb072014e76b_img_3", "article_68491ffd617fcb072014e76b_img_4", "article_68491ffd617fcb072014e76b_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e76b", "linked_chunks": ["article_68491ffd617fcb072014e76b_chunk_0", "article_68491ffd617fcb072014e76b_chunk_1", "article_68491ffd617fcb072014e76b_chunk_2", "article_68491ffd617fcb072014e76b_chunk_3", "article_68491ffd617fcb072014e76b_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e76b", "linked_chunks": ["article_68491ffd617fcb072014e76b_chunk_0", "article_68491ffd617fcb072014e76b_chunk_1", "article_68491ffd617fcb072014e76b_chunk_2", "article_68491ffd617fcb072014e76b_chunk_3", "article_68491ffd617fcb072014e76b_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e76b", "linked_chunks": ["article_68491ffd617fcb072014e76b_chunk_0", "article_68491ffd617fcb072014e76b_chunk_1", "article_68491ffd617fcb072014e76b_chunk_2", "article_68491ffd617fcb072014e76b_chunk_3", "article_68491ffd617fcb072014e76b_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e76b", "linked_chunks": ["article_68491ffd617fcb072014e76b_chunk_0", "article_68491ffd617fcb072014e76b_chunk_1", "article_68491ffd617fcb072014e76b_chunk_2", "article_68491ffd617fcb072014e76b_chunk_3", "article_68491ffd617fcb072014e76b_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e76b", "linked_chunks": ["article_68491ffd617fcb072014e76b_chunk_0", "article_68491ffd617fcb072014e76b_chunk_1", "article_68491ffd617fcb072014e76b_chunk_2", "article_68491ffd617fcb072014e76b_chunk_3", "article_68491ffd617fcb072014e76b_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e76b", "linked_chunks": ["article_68491ffd617fcb072014e76b_chunk_0", "article_68491ffd617fcb072014e76b_chunk_1", "article_68491ffd617fcb072014e76b_chunk_2", "article_68491ffd617fcb072014e76b_chunk_3", "article_68491ffd617fcb072014e76b_chunk_4"]}, {"type": "text", "text": "Dear friends, I\u2019ve noticed that many GenAI application projects put in automated evaluations (evals) of the system\u2019s output probably later \u2014 and rely on humans to manually examine and judge outputs longer \u2014 than they should. This is because building evals is viewed as a massive investment (say, creating 100 or 1,000 examples, and designing and validating metrics) and there\u2019s never a convenient moment to put in that up-front cost. Instead, I encourage teams to think of building evals as an iterative process. It\u2019s okay to start with a quick-and-dirty implementation (say, 5 examples with unoptimized metrics) and then iterate and improve over time. This allows you to gradually shift the burden of evaluations away from humans and toward automated evals. I wrote previously about the importance and difficulty of creating evals. Say you\u2019re building a customer-service chatbot that responds to users in free text. There\u2019s no single right answer, so many teams end up having humans pore over dozens of example outputs with every update to judge if it improved the system. While techniques like LLM-as-judge are helpful, the details of getting this to work well (such as what prompt to use, what context to give the judge, and so on) are finicky to get right. All this contributes to the impression that building evals requires a large up-front investment, and thus on any given day, a team can make more progress by relying on human judges than figuring out how to build automated evals. I encourage you to approach building evals differently. It\u2019s okay to build quick evals that are only partial, incomplete, and noisy measures of the system\u2019s performance, and to iteratively improve them. They can be a complement to, rather than replacement for, manual evaluations. Over time, you can gradually tune the evaluation methodology to close the gap between the evals\u2019 output and human judgments. For example: So long as the output of the evals correlates with overall performance, it\u2019s fine to measure only a subset of things you care about when starting. The development process thus comprises two iterative loops, which you might execute in parallel: As with many things in AI, we often don\u2019t get it right the first time. So t\u2019s better to build an initial end-to-end system quickly and then iterate to improve it. We\u2019re used to taking this approach to building AI systems. We can build evals the same way. To me, a successful eval meets the following criteria. Say, we currently have system A, and we might tweak it to get a system B: Whenever a pair of systems A and B contradicts these criteria, that is a sign the eval is in \u201cerror\u201d and we should tweak it to make it rank A and B correctly. This is a similar philosophy to error analysis in building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output \u2014 such as when it outputs an incorrect label \u2014 we focus on \u201cerrors\u201d of the evals \u2014 such as when they incorrectly rank two systems A and B, so", "article_id": "68491ffd617fcb072014e772", "linked_images": ["article_68491ffd617fcb072014e772_img_0", "article_68491ffd617fcb072014e772_img_1", "article_68491ffd617fcb072014e772_img_2", "article_68491ffd617fcb072014e772_img_3", "article_68491ffd617fcb072014e772_img_4", "article_68491ffd617fcb072014e772_img_5"]}, {"type": "text", "text": "to error analysis in building machine learning algorithms, only instead of focusing on errors of the machine learning algorithm's output \u2014 such as when it outputs an incorrect label \u2014 we focus on \u201cerrors\u201d of the evals \u2014 such as when they incorrectly rank two systems A and B, so the evals aren\u2019t helpful in choosing between them. Relying purely on human judgment is a great way to get started on a project. But for many teams, building evals as a quick prototype and iterating to something more mature lets you put in evals earlier and accelerate your progress. Keep building! Andrew Build autonomous agents that take actions like scraping web pages, filling out forms, and subscribing to newsletters in \u201cBuilding AI Browser Agents.\u201d Explore the AgentQ framework, which helps agents self-correct using Monte Carlo tree search and direct preference optimization. Start learning today Google\u2019s new flagship model raised the state of the art in a variety of subjective and objective tests. What\u2019s new: Google launched Gemini 2.5 Pro Experimental, the first model in the Gemini 2.5 family, and announced that Gemini 2.5 Flash, a version with lower latency, will be available soon. All Gemini 2.5 models will have reasoning capabilities, as will all Google models going forward. How it works: Compared to Gemini 1.0 and Gemini 1.5, Google disclosed little information about Gemini 2.5 Pro Experimental or how it differs from previous versions. Results: On a variety of popular benchmarks, Gemini 2.5 Pro Experimental outperforms top models from competing AI companies. Why it matters: Late last year, some observers expressed concerns that progress in AI was slowing. Gemini 2.5 Pro Experimental arrives shortly after rival proprietary models GPT-4.5 (currently a research preview) and Claude 3.7 Sonnet, both of which showed improved performance, yet it outperforms them on most benchmarks. Clearly there\u2019s still room for models \u2014 particularly reasoning models \u2014 to keep getting better. We\u2019re thinking: Google said it plans to train all its new models on chains of thought going forward. This follows a similar statement by OpenAI. We\u2019re sure they have their reasons! OpenAI embraced Model Context Protocol, providing powerful support for an open standard that connects large language models to tools and data. What\u2019s new: OpenAI will support Model Context Protocol (MCP) in its Agents SDK and soon its ChatGPT desktop app and Responses API. The move will give developers who use OpenAI models access to a wide variety of pre-existing tools and proprietary data sources. How it works: Launched by Anthropic late last year, MCP connects AI models to a growing ecosystem of plug-and-play resources, including more than 6,000 community-built servers and connectors. Behind the news: Momentum behind MCP has built rapidly. Last month, Microsoft integrated MCP into CoPilot Studio, enabling developers to build agents with access to MCP servers. Cloudflare enabled its customers to deploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users to add MCP servers. Why it matters: OpenAI\u2019s move will make it easier for developers who use its models to connect to a variety of tools and data sources,", "article_id": "68491ffd617fcb072014e772", "linked_images": ["article_68491ffd617fcb072014e772_img_0", "article_68491ffd617fcb072014e772_img_1", "article_68491ffd617fcb072014e772_img_2", "article_68491ffd617fcb072014e772_img_3", "article_68491ffd617fcb072014e772_img_4", "article_68491ffd617fcb072014e772_img_5"]}, {"type": "text", "text": "to MCP servers. Cloudflare enabled its customers to deploy remote MCP servers. In February, the AI-powered code editor Cursor enabled users to add MCP servers. Why it matters: OpenAI\u2019s move will make it easier for developers who use its models to connect to a variety of tools and data sources, and it helps to establish MCP as a go-to protocol for building agentic applications. Instead of figuring out manually how to integrate various providers, developers can connect to a third-party server (or download and run it themselves) and tie it into existing workflows with a few lines of code. We\u2019re thinking: Kudos to Anthropic, OpenAI, and other competitors who realize it\u2019s better to solve shared problems together than fragment the industry. A behind-the-scenes account provides new details about the abrupt firing and reinstatement of OpenAI CEO Sam Altman in November 2023. How it works: Based on insider accounts, an excerpt from a forthcoming book about OpenAI by Wall Street Journal reporter Keach Hagey describes conflicts, accusations, and shifting alliances that led to Altman\u2019s brief ouster and rapid return. Firing and reinstatement: OpenAI\u2019s board of directors came to distrust Altman but failed to persuade executives and employees that he should be replaced. Aftermath: Since Altman\u2019s return, Murati and all but one director who voted to remove him have left OpenAI. The issues that precipitated his departure have given way to commercial concerns as the company considers a shift from its current hybrid nonprofit/for-profit structure to fully for-profit. Why it matters: The AI frontier spawns not only technical innovations but also intense interpersonal relationships and corporate politics. Such dynamics have consequences for users and the world at large: Having survived serious challenges to his leadership, Altman has emerged in a strong position to build a path of faster growth as a for-profit company upon OpenAI\u2019s philanthropic foundation. We\u2019re thinking: Given OpenAI\u2019s formidable achievements, Altman\u2019s renewed leadership marks an inflection point in the AI landscape. Without Sam Altman at the helm, OpenAI would be a very different company, with different priorities and a different future. Researchers built a model that\u2019s more robust to noisy inputs like misspellings, smarter about character-level information like the number of R's in strawberry, and potentially better able to understand unfamiliar languages that might share groups of letters with familiar languages. Their approach: Eliminate the tokenizer and instead integrate a system that learns to group input characters. What\u2019s new: Artidoro Pagnoni, Ram Pasunuru, and collaborators at Meta, University of Washington, and University of Chicago introduced Byte Latent Transformer (BLT), a system of transformers that processes groups of text characters (in the form of bytes) directly. Key insight: A tokenizer turns bytes (characters) into tokens (a word or part of a word) based on learned rules: Specific sequences map to particular tokens. A large language model (LLM) would be more efficient if its tokenizer considered how easy or difficult it would be to predict the next token, because then it could group tokens that commonly occur together, thus saving memory and processing power. For instance, to complete the phrase, \u201cThe capital of", "article_id": "68491ffd617fcb072014e772", "linked_images": ["article_68491ffd617fcb072014e772_img_0", "article_68491ffd617fcb072014e772_img_1", "article_68491ffd617fcb072014e772_img_2", "article_68491ffd617fcb072014e772_img_3", "article_68491ffd617fcb072014e772_img_4", "article_68491ffd617fcb072014e772_img_5"]}, {"type": "text", "text": "A large language model (LLM) would be more efficient if its tokenizer considered how easy or difficult it would be to predict the next token, because then it could group tokens that commonly occur together, thus saving memory and processing power. For instance, to complete the phrase, \u201cThe capital of the United States is,\u201d a tokenizer may generate \u201cWashington\u201d, then \u201cD\u201d, then \u201c.C\u201d, and finally \u201c.\u201d \u2014 even though it\u2019s easy to predict that \u201cD.C.\u201d will follow \u201cWashington\u201d (that is, the number of viable options is very small). Conversely, generating the token after \u201cD.C.\u201d is harder, since many viable options exist. Using a small LLM to estimate the difficulty of predicting the next token enables the model to split difficult-to-predict text into smaller groups while packing easier-to-predict text into larger groups. How it works: BLT comprises four transformers (8 billion parameters total): (i) a small byte-level transformer, (ii) an encoder transformer, (iii) a so-called latent transformer, and (iv) a decoder transformer. The authors trained the system to generate the next token in 1 trillion tokens of text, including tokens drawn from a filtered version of Common Crawl. Results: On seven benchmarks that test general language and coding abilities, BLT achieved an average accuracy of 61.1 percent, outperforming Llama 3 (8 billion parameters and a similar number of floating point operations to BLT) at 60.0 percent. Why it matters: By working directly on bytes, BLT is inherently more robust to variations in language, which improves its performance. For instance, when prompted to insert a \"z\" after every \"n\" in \"not\", Llama 3 incorrectly completed it as \"znotz\". This happened because its tokenizer treats \"not\" as a single, indivisible token. In contrast, BLT correctly generated \"nzot,\" because it can dynamically regroup bytes and draw new boundaries. In a more practical case, instead of treating \"pizya\" and \"pizza\" as different tokens, BLT recognizes that they share nearly identical byte sequences, differing only in the bytes for \"y\" and \"z\", and therefore likely mean the same thing. We\u2019re thinking: In some alternatives to traditional tokenization, an LLM might process much longer sequences because the number of bytes in a sentence is much larger than the number of words. This work addresses that issue by grouping bytes dynamically. The tradeoff is complexity: Instead of one transformer, we have four.", "article_id": "68491ffd617fcb072014e772", "linked_images": ["article_68491ffd617fcb072014e772_img_0", "article_68491ffd617fcb072014e772_img_1", "article_68491ffd617fcb072014e772_img_2", "article_68491ffd617fcb072014e772_img_3", "article_68491ffd617fcb072014e772_img_4", "article_68491ffd617fcb072014e772_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e772", "linked_chunks": ["article_68491ffd617fcb072014e772_chunk_0", "article_68491ffd617fcb072014e772_chunk_1", "article_68491ffd617fcb072014e772_chunk_2", "article_68491ffd617fcb072014e772_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e772", "linked_chunks": ["article_68491ffd617fcb072014e772_chunk_0", "article_68491ffd617fcb072014e772_chunk_1", "article_68491ffd617fcb072014e772_chunk_2", "article_68491ffd617fcb072014e772_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e772", "linked_chunks": ["article_68491ffd617fcb072014e772_chunk_0", "article_68491ffd617fcb072014e772_chunk_1", "article_68491ffd617fcb072014e772_chunk_2", "article_68491ffd617fcb072014e772_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e772", "linked_chunks": ["article_68491ffd617fcb072014e772_chunk_0", "article_68491ffd617fcb072014e772_chunk_1", "article_68491ffd617fcb072014e772_chunk_2", "article_68491ffd617fcb072014e772_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e772", "linked_chunks": ["article_68491ffd617fcb072014e772_chunk_0", "article_68491ffd617fcb072014e772_chunk_1", "article_68491ffd617fcb072014e772_chunk_2", "article_68491ffd617fcb072014e772_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e772", "linked_chunks": ["article_68491ffd617fcb072014e772_chunk_0", "article_68491ffd617fcb072014e772_chunk_1", "article_68491ffd617fcb072014e772_chunk_2", "article_68491ffd617fcb072014e772_chunk_3"]}, {"type": "text", "text": "Dear friends, I am so sorry that the U.S. is letting down our friends and allies. Broad tariffs, implemented not just against adversaries but also steadfast allies, will damage the livelihoods of billions of people, create inflation, make the world more fragmented, and leave the U.S. and the world poorer. AI isn\u2019t the solution to everything, but even amidst this challenging environment, I hope our community can hold together, keep building friendships across borders, keep sharing ideas, and keep supporting each other. Much has been written about why high, widespread taxes on imports are harmful. In this letter, I\u2019d like to focus on its possible effects on AI. One silver lining of the new tariffs is that they focus on physical imports, rather than digital goods and services, including intellectual property (IP) such as AI research inventions and software. IP is difficult to tax, because each piece of IP is unique and thus hard to value, and it moves across borders with little friction via the internet. Many international AI teams collaborate across borders and timezones, and software, including specifically open source software, is an important mechanism for sharing ideas. I hope that this free flow of ideas remains unhampered, even if the flow of physical goods is. However, AI relies on hardware, and tariffs will slow down AI progress by restricting access to it. Even though a last-minute exception was made for semiconductors, taxing imports of solar panels, wind turbines, and other power-generation and -distribution equipment will diminish the ability to provide power to U.S. data centers. Taxing imports of servers, cooling hardware, networking hardware, and the like will also make it more expensive to build data centers. And taxing consumer electronics, like laptops and phones, will make it harder for citizens to learn and use AI. With regard to data-center buildouts, another silver lining is that, with the rise of generative AI, data gravity has decreased because compute processing costs are much greater than transmission costs, meaning it\u2019s more feasible to place data centers anywhere in the world rather than only in close proximity to end-users. Even though many places do not have enough trained technicians to build and operate data centers, I expect tariffs will encourage data centers to be built around the world, creating more job opportunities globally. Finally, tariffs will create increased pressure for domestic manufacturing, which might create very mild tailwinds for robotics and industrial automation. As U.S. Vice President J.D. Vance pointed out in 2017, the U.S. should focus on automation (and education) rather than on tariffs. But the U.S. does not have the personnel \u2014 or know-how, or supply chain \u2014 to manufacture many of the goods that it currently counts on allies to make. Robotics can be helpful for addressing a small part of this large set of challenges. Generative AI\u2019s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small. My 4-year-old son had been complaining for", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, {"type": "text", "text": "of this large set of challenges. Generative AI\u2019s rate of progress in robotics is also significantly slower than in processing text, visual data, audio, and reasoning. So while the tariffs could create tailwinds for AI-enabled robotics, I expect this effect to be small. My 4-year-old son had been complaining for a couple of weeks that his shoes were a tight fit \u2014 he was proud that he\u2019s growing! So last Sunday, we went shoe shopping. His new shoes cost $25, and while checking out, I paused and reflected on how lucky I am to be able to afford them. But I also thought about the many families living paycheck-to-paycheck, and for whom tariffs leading to shoes at $40 a pair would mean they let their kids wear ill-fitting shoes longer. I also thought about people I\u2019ve met in clothing manufacturing plants in Asia and Latin America, for whom reduced demand would mean less work and less money to take home to their own kids. I don\u2019t know what will happen next with the U.S. tariffs, and plenty of international trade will happen with or without U.S. involvement. I hope we can return to a world of vibrant global trade with strong, rules-based, U.S. participation. Until then, let\u2019s all of us in AI keep nurturing our international friendships, keep up the digital flow of ideas \u2014 including specifically open source software \u2014 and keep supporting each other. Let\u2019s all do what we can to keep the world as connected as we are able. Love, Andrew Course 3 of the Data Analytics Professional Certificate is live! Learn to use Python, the most important coding language in data analytics, to analyze real-world datasets, create visualizations, run tests, and apply AI tools to debug and accelerate your code. Enroll now Even without explicit training in reasoning, large language models \u201cthink\u201d in ways that may be more deliberate than previously understood. What\u2019s new: Emmanuel Ameisen and colleagues at Anthropic devised a method to study how transformers generate responses to specific prompts. They also studied Claude 3.5 Haiku\u2019s responses to specific prompts and found that the model, which is not trained to generate chains of thought, nonetheless appeared to take reasoning steps via its neuron activations. Key insight: A viable alternative to a fully connected layer is a cross-layer transcoder, which has two layers. The outputs of the larger first layer are sparse, which makes them interpretable \u201cfeatures,\u201d or individual values that correspond to concepts. By mapping an input to highly activated features, we can identify the concepts that determine the model\u2019s output. How it works: The team replaced fully connected layers in Claude 3.5 Haiku with cross-layer transcoders and interpreted their features. Results: The authors built graphs that show how Claude 3.5 Haiku computes its output over a number of selected prompts. Behind the news: Last year, Google trained models to examine individual features in Gemma 2. Before that, Anthropic used similar methods to interpret Claude 3 Sonnet\u2019s middle layer. Why it matters: Apparently Claude 3.5 Haiku \u2014 and presumably other large language models \u2014 spontaneously perform", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, {"type": "text", "text": "a number of selected prompts. Behind the news: Last year, Google trained models to examine individual features in Gemma 2. Before that, Anthropic used similar methods to interpret Claude 3 Sonnet\u2019s middle layer. Why it matters: Apparently Claude 3.5 Haiku \u2014 and presumably other large language models \u2014 spontaneously perform implicit reasoning steps without being prompted to do so. Anthropic\u2019s method reveals not only whether a model reasons or takes a shortcut, but also what it truly does well and what it only professes to do well. We\u2019re thinking: The authors\u2019 approach to examining how large language models generate output is interesting. We wonder whether even pre-transformer vanilla neural networks would appear to perform some sort of \u201creasoning\u201d if we were to interpret them in a similar way. Meta updated its popular open-weights models, claiming performance superior to closed competitors in three size classes. What\u2019s new: Meta released two vision-language models in the Llama 4 family (Llama 4 Scout and Llama 4 Maverick) and teased a third (Llama 4 Behemoth). All three models are based on the increasingly popular mixture-of-experts (MoE) architecture, which activates only a portion of parameters during inference for more efficient processing. Llama 4 Scout boasts the industry's biggest input context window so far \u2014 10 million tokens! \u2014 but Meta says processing 1.4 million tokens of context requires eight Nvidia H100 GPUs, and early users on Reddit reported that its effective context began to degrade at 32,000 tokens. How it works: The team pretrained Llama 4 models on images and text in over 200 languages from publicly available and licensed data, including data from publicly shared posts on Facebook and Instagram. They trained Llama 4 Scout on 40 trillion tokens and Llama 4 Maverick on 22 trillion tokens. Results: In tests performed by Meta, Llama 4 models showed strong performance relative to competing models \u2014 mostly not mixtures of experts, but some that are known to have higher parameter counts relative to Llama 4 models\u2019 active parameters. Yes, but: An experimental version of Llama 4 Maverick reached second place in Chatbot Arena behind Gemini 2.5 Pro. However, it was a variation optimized for conversation, not the currently available version. AI researchers accused Meta of attempting to manipulate the leaderboard. Why it matters: Although the version of Llama 4 Maverick that nearly topped the Chatbot Arena is not the released version, its accomplishment says a lot about the growing power of open weights. Open models are quickly reaching parity with closed competitors \u2014 a boon to developers, businesses, and society at large. We\u2019re thinking: According to Meta, Behemoth beats GPT-4.5, Claude Sonnet 3.7, and Gemini 2.0 Pro, topping all but the best reasoning models \u2014 but it isn\u2019t available yet. Something to look forward to! Alibaba\u2019s latest open-weights system raises the bar for multimodal tasks in a relatively small model. What\u2019s new: Alibaba released Qwen2.5-Omni 7B. How it works: Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plus BigVGAN), along with corresponding", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, {"type": "text", "text": "raises the bar for multimodal tasks in a relatively small model. What\u2019s new: Alibaba released Qwen2.5-Omni 7B. How it works: Qwen2.5-Omni 7B comprises a pretrained text transformer (Qwen 2.5 7B), pretrained vision encoder (Qwen2.5-VL), pretrained audio encoder (Whisper-large-v3), speech transformer, and audio decoder (a transformer plus BigVGAN), along with corresponding adapters of undisclosed architecture. Results: The authors compared Qwen2.5-Omni 7B to similarly sized models. It performed especially well on audio-to-text, image-to-text, and video-to-text tasks. However, it performed less well on text-to-text and text-to-speech tasks. Behind the news: Multimodal systems with open weights are multiplying. For instance, AnyGPT (open weights, training, and inference code) accepts and generates speech, text, images, and music. Similarly, Mini-Omni2 (open weights and inference code) accepts and generates text, speech, and images. Why it matters: Multimodal models typically show steep degradation on measurements of instruction-following when shifting from voice to text, but Qwen2.5-Omni does not. As the world moves toward voice-to-voice interactions, open systems that deliver performance comparable to that of closed competitors accelerate progress towards better conversations. We\u2019re thinking: The Qwen team is on fire! Alibaba\u2019s steady stream of highly capable open-weights models is a gift to AI developers. If you have a collection of variables that represent, say, a cancer patient and you want to classify the patient\u2019s illness as likely cancer or not, algorithms based on decision trees, such as gradient-boosted trees, typically perform better than neural networks. A transformer tailored to tabular data could change this situation. What\u2019s new: Noah Hollmann, Samuel M\u00fcller, and colleagues at University of Freiburg, Berlin Institute of Health, Prior Labs, and ELLIS Institute introduced Tabular Prior-data Fitted Network (TabPFN), a transformer that, given a tabular dataset, beats established decision-tree methods on classification and regression tasks. You can download the code and weights under a license based on Apache 2.0 that allows noncommercial and commercial uses. Key insight: In a typical supervised learning process, a model given one example at a time learns to recognize patterns in a dataset. If each example is an entire dataset, it learns to recognize patterns across all those datasets. Trained in this way on enough datasets, it can generalize to new ones. Applying this idea to tabular data, a transformer \u2014 unlike a decision tree \u2014 can learn to perform classification and regression on any dataset without further training; that is, without further updating the model weights. How it works: The authors generated 100 million datasets and used them to pretrain two small transformers (around 7 million and 11 million parameters respectively) to perform classification or regression. Given a dataset of rows (say, patient data labeled diagnoses or real-estate data labeled with prices) and one final row that\u2019s unlabeled, the models learned to generate the missing label or value. Each dataset consisted of up to 2,048 rows (examples) and up to 160 columns (features). Results: The authors tested the system on 29 classification datasets and 28 regression datasets from the AutoML benchmark and OpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, {"type": "text", "text": "2,048 rows (examples) and up to 160 columns (features). Results: The authors tested the system on 29 classification datasets and 28 regression datasets from the AutoML benchmark and OpenML-CTR23. Each dataset contained up to 10,000 rows, 500 columns, and 10 classes. They compared TabPFN to the popular gradient-boosted tree approaches CatBoost, LightGBM, and XGBoost. Yes, but: The authors\u2019 method is slower than decision tree methods with respect to inference. To process a 10,000-row dataset, TabPFN required 0.2 seconds while CatBoost took 0.0002 seconds. Why it matters: Transformers trained on large datasets of text or images can perform tasks they weren\u2019t specifically trained for and generalize to novel datasets when performing tasks they were trained for. But when it comes to tabular data, they haven\u2019t been competitive with decision trees. This work bridges the gap, unlocking a wide variety of new use cases for transformers. Not only does it process tabular data as well as popular tree-based methods, it doesn\u2019t require additional training to process novel datasets. We\u2019re thinking: Decision trees date back to Aristotle and remain extremely useful. But a transformer-based approach could open the processing of tabular data to benefit from the ongoing innovation in transformers.", "article_id": "68491ffd617fcb072014e779", "linked_images": ["article_68491ffd617fcb072014e779_img_0", "article_68491ffd617fcb072014e779_img_1", "article_68491ffd617fcb072014e779_img_2", "article_68491ffd617fcb072014e779_img_3", "article_68491ffd617fcb072014e779_img_4", "article_68491ffd617fcb072014e779_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e779", "linked_chunks": ["article_68491ffd617fcb072014e779_chunk_0", "article_68491ffd617fcb072014e779_chunk_1", "article_68491ffd617fcb072014e779_chunk_2", "article_68491ffd617fcb072014e779_chunk_3", "article_68491ffd617fcb072014e779_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e779", "linked_chunks": ["article_68491ffd617fcb072014e779_chunk_0", "article_68491ffd617fcb072014e779_chunk_1", "article_68491ffd617fcb072014e779_chunk_2", "article_68491ffd617fcb072014e779_chunk_3", "article_68491ffd617fcb072014e779_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e779", "linked_chunks": ["article_68491ffd617fcb072014e779_chunk_0", "article_68491ffd617fcb072014e779_chunk_1", "article_68491ffd617fcb072014e779_chunk_2", "article_68491ffd617fcb072014e779_chunk_3", "article_68491ffd617fcb072014e779_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e779", "linked_chunks": ["article_68491ffd617fcb072014e779_chunk_0", "article_68491ffd617fcb072014e779_chunk_1", "article_68491ffd617fcb072014e779_chunk_2", "article_68491ffd617fcb072014e779_chunk_3", "article_68491ffd617fcb072014e779_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e779", "linked_chunks": ["article_68491ffd617fcb072014e779_chunk_0", "article_68491ffd617fcb072014e779_chunk_1", "article_68491ffd617fcb072014e779_chunk_2", "article_68491ffd617fcb072014e779_chunk_3", "article_68491ffd617fcb072014e779_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e779", "linked_chunks": ["article_68491ffd617fcb072014e779_chunk_0", "article_68491ffd617fcb072014e779_chunk_1", "article_68491ffd617fcb072014e779_chunk_2", "article_68491ffd617fcb072014e779_chunk_3", "article_68491ffd617fcb072014e779_chunk_4"]}, {"type": "text", "text": "Dear friends, Contrary to standard prompting advice that you should give LLMs the context they need to succeed, I find it\u2019s sometimes faster to be lazy and dash off a quick, imprecise prompt and see what happens. The key to whether this is a good idea is whether you can quickly assess the output quality, so you can decide whether to provide more context. In this post, I\u2019d like to share when and how I use \u201clazy prompting.\u201d When debugging code, many developers copy-paste error messages \u2014 sometimes pages of them \u2014 into an LLM without further instructions. Most LLMs are smart enough to figure out that you want them to help understand and propose fixes, so you don\u2019t need to explicitly tell them. With brief instructions like \u201cEdit this: \u2026\u201d or \u201csample dotenv code\u201d (to remind you how to write code to use Python's dotenv package), an LLM will often generate a good response. Further, if the response is flawed, hopefully you can spot any problems and refine the prompt, for example to steer how the LLM edits your text. At the other end of the spectrum, sometimes I spend 30 minutes carefully writing a 2-page prompt to get an AI system to help me solve a problem (for example to write many pages of code) that otherwise would have taken me much longer. I don\u2019t try a lazy prompt if (i) I feel confident there\u2019s no chance the LLM will provide a good solution without additional context. For example, given a partial program spec, does even a skilled human developer have a chance of understanding what you want? If I absolutely want to use a particular piece of pdf-to-text conversion software (like my team LandingAI\u2019s Agentic Doc Extraction!), I should say so in the prompt, since otherwise it\u2019s very hard for the LLM to guess my preference. I also wouldn\u2019t use a lazy prompt if (ii) a buggy implementation would take a long time to detect. For example, if the only way for me to figure out if the output is incorrect is to laboriously run the code to check its functionality, it would be better to spend the time up-front to give context that would increase the odds of the LLM generating what I want. By the way, lazy prompting is an advanced technique. On average, I see more people giving too little context to LLMs than too much. Laziness is a good technique only when you\u2019ve learned how to provide enough context, and then deliberately step back to see how little context you can get away with and still have it work. Also, lazy prompting applies only when you can iterate quickly using an LLM\u2019s web or app interface It doesn\u2019t apply to prompts written in code for the purpose of repeatedly calling an API, since presumably you won\u2019t be examining every output to clarify and iterate if the output is poor. Thank you to Rohit Prsad, who has been collaborating with me on the open-source package aisuite, for suggesting the term lazy prompting. There is an analogy", "article_id": "68491ffd617fcb072014e780", "linked_images": ["article_68491ffd617fcb072014e780_img_0", "article_68491ffd617fcb072014e780_img_1", "article_68491ffd617fcb072014e780_img_2", "article_68491ffd617fcb072014e780_img_3", "article_68491ffd617fcb072014e780_img_4", "article_68491ffd617fcb072014e780_img_5"]}, {"type": "text", "text": "the purpose of repeatedly calling an API, since presumably you won\u2019t be examining every output to clarify and iterate if the output is poor. Thank you to Rohit Prsad, who has been collaborating with me on the open-source package aisuite, for suggesting the term lazy prompting. There is an analogy to lazy evaluation in computer science, where you call a function at the latest possible moment and only when a specific result is needed. In lazy prompting, we add details to the prompt only when they are needed. Keep building! Andrew In our latest course, \u201cGetting Structured LLM Output,\u201d you\u2019ll learn to generate consistent, machine-readable outputs from LLMs using structured output APIs, re-prompting libraries, and token-level constraints. You\u2019ll build a social media analysis agent that extracts sentiment and creates structured JSON ready for downstream use. Enroll for free Researchers updated the highly responsive Moshi voice-to-voice model to discuss visual input. What\u2019s new: Am\u00e9lie Royer, Moritz B\u00f6hle, and colleagues at Kyutai proposed MoshiVis. The weights are free to download under the CC-BY 4.0 license, which permits commercial and noncommercial uses. You can hear examples of its output and chat with a demo. Key insight: The original Moshi, which manages overlapping voice-to-voice conversations, comprises two transformers. The first outputs a text transcription of its speech, and the second outputs speech. Since Moshi generates text as well as speech, the authors of that work fine-tuned it to predict the next token of text. In MoshiVis, the addition of a vision encoder enabled the authors to fine-tune on not only image-text datasets but also image-speech datasets, which are not so plentiful. Fine-tuning on this wider variety of images enabled the system to understand images better than fine-tuning it solely on image-speech datasets. How it works: To Moshi, the authors added a model based on a pretrained SigLIP vision encoder to encode images, a cross-attention adapter to fuse image information with speech tokens, and vanilla neural networks trained to act as gates that determine how much image information to fuse. Specifically, the authors added the adapter and a gate between Moshi\u2019s existing self-attention and fully connected layers. Results: MoshiVis is highly responsive in conversation with latency of roughly 50 milliseconds on a Mac Mini. Behind the news: MoshiVis complements a small but growing roster of systems that combine vision with speech-to-speech. ChatGPT accepts and generates speech in response to camera views or a user\u2019s phone screen. AnyGPT (open weights training and inference code) accepts or generates speech, text, images, and music. Similarly, Mini-Omni2 (open weights and inference code) accepts and generates text, speech, and images. The authors didn\u2019t compare MoshiVis to these alternatives. Why it matters: MoshiVis easily adapts a speech-to-speech model to work with a new type of media input. MoshiVis requires training only the adapters, while the earlier AnyGPT and Mini-Omni2, which can also discuss images via voice input and output, require training both adapters and the main model. We\u2019re thinking: Text-chat models respond appropriately when a user refers to a previous topic or something new, and MoshiVis does, too, in spoken interactions. Evaluations of", "article_id": "68491ffd617fcb072014e780", "linked_images": ["article_68491ffd617fcb072014e780_img_0", "article_68491ffd617fcb072014e780_img_1", "article_68491ffd617fcb072014e780_img_2", "article_68491ffd617fcb072014e780_img_3", "article_68491ffd617fcb072014e780_img_4", "article_68491ffd617fcb072014e780_img_5"]}, {"type": "text", "text": "while the earlier AnyGPT and Mini-Omni2, which can also discuss images via voice input and output, require training both adapters and the main model. We\u2019re thinking: Text-chat models respond appropriately when a user refers to a previous topic or something new, and MoshiVis does, too, in spoken interactions. Evaluations of this capability will become increasingly important as voice-to-voice becomes more widespread. Bots that scrape websites for AI training data often ignore do-not-crawl requests. Now web publishers can enforce such appeals by luring scrapers to AI-generated decoy pages. What\u2019s new: Cloudflare launched AI Labyrinth, a bot-management tool that serves fake pages to unwanted bots, wasting their computational resources and making them easier to detect. It\u2019s currently free to Cloudflare users. How it works: AI Labyrinth protects webpages by embedding them with hidden links to AI-generated alternatives that appear legitimate to bots but are irrelevant to the protected site. Behind the news: The robots.txt instructions that tell web crawlers which pages they can access aren\u2019t legally binding, and web crawlers can disregard them. However, online publishers are moving to try to stop AI developers from training models on their content. Cloudflare, as the proxy server and content delivery network for nearly 20 percent of websites, plays a potentially large role in this movement. AI crawlers account for nearly 1 percent of web requests on Cloudflare\u2019s network, the company says. Why it matters: The latest AI models are trained on huge quantities of data gleaned from the web, which enables them to perform well enough to be widely useful. However, publishers increasingly aim to limit access to this data. AI Labyrinth gives them a new tool that raises the cost for bots that disregard instructions not to scrape web content. We\u2019re thinking: If AI Labyrinth gains traction, no doubt some teams that build crawlers will respond with their own AI models to sniff out its decoy pages. To the extent that the interest between crawlers and publishers is misaligned and clear, enforceable rules for crawling are lacking, this cat-and-mouse competition could go on for a long time. A pair of papers investigate how increasingly human-like chatbots affect users\u2019 emotions. What\u2019s new: Jason Phang at OpenAI, Cathy Mengying Fang at MIT Media Lab, and colleagues at those organizations published complementary studies that examine ChatGPT\u2019s influence on loneliness, social interactions, emotional dependence, and potentially problematic use. How it works: One study was a large-scale analysis of real-world conversations, and the other was a randomized control trial that tracked conversations of a selected cohort. Both evaluated conversations according to EmoClassifiersV1, a set of classifiers based on large language models that evaluate five top-level emotional classes (loneliness, dependence, and the like) and 20 sub-classes of emotional indicators (seeking support, use of pet names, and so on). Results: Both studies found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting. Yes, but: The authors of the randomized controlled trial acknowledged significant limitations. For", "article_id": "68491ffd617fcb072014e780", "linked_images": ["article_68491ffd617fcb072014e780_img_0", "article_68491ffd617fcb072014e780_img_1", "article_68491ffd617fcb072014e780_img_2", "article_68491ffd617fcb072014e780_img_3", "article_68491ffd617fcb072014e780_img_4", "article_68491ffd617fcb072014e780_img_5"]}, {"type": "text", "text": "found that using ChatGPT was associated with reduced loneliness and increased emotional chat. However, it was also associated with decreased interpersonal social interaction and greater dependence on the chatbot, especially among users who spent more time chatting. Yes, but: The authors of the randomized controlled trial acknowledged significant limitations. For instance, the study lacked a non-ChatGPT control group to differentiate AI-specific effects from influences such as seasonal emotional shifts, and the trial\u2019s time frame and assignments may not mirror real-world behavior. Why it matters: As AI chatbot behavior becomes more human-like, people may lean on large language models to satisfy emotional needs such as easing loneliness or grief. Yet we know little about their effects. These studies offer a starting point for AI developers who want to both foster emotional support and protect against over-reliance, and for social scientists who want to better understand the impact of chatbots. We\u2019re thinking: Social media turned out to cause emotional harm to some people in ways that were not obvious when the technology was new. As chatbots evolve, research like this can help us steer them toward protecting and enhancing mental health. AI systems designed to generate animated 3D scenes that include active human characters have been limited by a shortage of training data, such as matched 3D scenes and human motion-capture examples. Generated video clips can get the job done without motion capture. What\u2019s new: A team led by Hongjie Li, Hong-Xing Yu, and Jiaman Li at Stanford University developed Zero-Shot 4D Human-Scene Interaction (ZeroHSI), a method that animates a 3D human figure interacting with a particular 3D object in a selected 3D scene. You can see its output here. Key insight: Earlier approaches attempted to build a generalized approach: given a 3D scene, a text prompt, and motion-capture data, a diffusion model learned to alter the positions and rotations of human joints and objects over time. But if the system is designed to learn a 3D animation for a specific example motion, videos can stand in for motion capture. Current video generation models can take an image of a scene and generate a clip of realistic human motion and interactions with a wide variety of objects within it. From there, we can minimize the difference between the video frames and images of actions within the scene. How it works: ZeroHSI takes a pre-built 3D scene that includes a 3D human mesh and 3D object. It uses a rendered image of the scene to generate a video. Then it uses the video to help compute the motions of a human figure and object within the scene. Results: The authors evaluated ZeroHSI using a proprietary dataset of 12 3D scenes that included a human figure and an object and between one and three text prompts that described interactions between the human and object and/or scene. In 100 evaluations, ZeroHSI outperformed LINGO, a diffusion model trained on matched 3D scene, 3D object, and human motion-capture data that had achieved the previous state of the art. Why it matters: Learning from motion-capture data is problematic in a", "article_id": "68491ffd617fcb072014e780", "linked_images": ["article_68491ffd617fcb072014e780_img_0", "article_68491ffd617fcb072014e780_img_1", "article_68491ffd617fcb072014e780_img_2", "article_68491ffd617fcb072014e780_img_3", "article_68491ffd617fcb072014e780_img_4", "article_68491ffd617fcb072014e780_img_5"]}, {"type": "text", "text": "that described interactions between the human and object and/or scene. In 100 evaluations, ZeroHSI outperformed LINGO, a diffusion model trained on matched 3D scene, 3D object, and human motion-capture data that had achieved the previous state of the art. Why it matters: Learning from motion-capture data is problematic in a couple of ways: (i) it\u2019s expensive to produce, (ii) so little of it is available, which limits how much a learning algorithm can generalize from it. Video data, on the other hand, is available in endless variety, enabling video generation models to generalize across a wide variety of scenes, objects, and motions. ZeroHSI takes advantage of generated video to guide a 3D animation cheaply and effectively. We\u2019re thinking: There\u2019s a lot of progress to be made in AI simply by finding clever ways to use synthetic data.", "article_id": "68491ffd617fcb072014e780", "linked_images": ["article_68491ffd617fcb072014e780_img_0", "article_68491ffd617fcb072014e780_img_1", "article_68491ffd617fcb072014e780_img_2", "article_68491ffd617fcb072014e780_img_3", "article_68491ffd617fcb072014e780_img_4", "article_68491ffd617fcb072014e780_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e780", "linked_chunks": ["article_68491ffd617fcb072014e780_chunk_0", "article_68491ffd617fcb072014e780_chunk_1", "article_68491ffd617fcb072014e780_chunk_2", "article_68491ffd617fcb072014e780_chunk_3", "article_68491ffd617fcb072014e780_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e780", "linked_chunks": ["article_68491ffd617fcb072014e780_chunk_0", "article_68491ffd617fcb072014e780_chunk_1", "article_68491ffd617fcb072014e780_chunk_2", "article_68491ffd617fcb072014e780_chunk_3", "article_68491ffd617fcb072014e780_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e780", "linked_chunks": ["article_68491ffd617fcb072014e780_chunk_0", "article_68491ffd617fcb072014e780_chunk_1", "article_68491ffd617fcb072014e780_chunk_2", "article_68491ffd617fcb072014e780_chunk_3", "article_68491ffd617fcb072014e780_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e780", "linked_chunks": ["article_68491ffd617fcb072014e780_chunk_0", "article_68491ffd617fcb072014e780_chunk_1", "article_68491ffd617fcb072014e780_chunk_2", "article_68491ffd617fcb072014e780_chunk_3", "article_68491ffd617fcb072014e780_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e780", "linked_chunks": ["article_68491ffd617fcb072014e780_chunk_0", "article_68491ffd617fcb072014e780_chunk_1", "article_68491ffd617fcb072014e780_chunk_2", "article_68491ffd617fcb072014e780_chunk_3", "article_68491ffd617fcb072014e780_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e780", "linked_chunks": ["article_68491ffd617fcb072014e780_chunk_0", "article_68491ffd617fcb072014e780_chunk_1", "article_68491ffd617fcb072014e780_chunk_2", "article_68491ffd617fcb072014e780_chunk_3", "article_68491ffd617fcb072014e780_chunk_4"]}, {"type": "text", "text": "Dear friends, Fine-tuning small language models has been gaining traction over the past half year. I\u2019d like to share my sense of when to use this technique, and also when not to, based on what I\u2019m seeing in multiple companies. First, while fine-tuning is an important and valuable technique, many teams that are currently using it probably could get good results with simpler approaches, such as prompting (including writing mega prompts), few-shot prompting, or simple agentic workflows. Why shouldn\u2019t these teams be fine-tuning? Because fine-tuning, which takes a pre-trained model and further trains it on data specific to an application, is relatively complex to implement. You need to collect training data, then (unless you want to implement fine-tuning yourself) find a provider to help with running fine-tuning, then find a way to deploy the fine-tuned model. Because it adds extra complexity both in training and deployment, usually I resort to this technique only after I find that prompting and simple agentic workflows are not up to a task. Having said that, there are also applications where fine-tuning is appropriate and valuable. LoRA (which learns by modifying a limited number of parameters rather than the entire model) and related methods have made fine-tuning quite affordable, particularly for small models (say, 13B or fewer parameters). And the amount of data needed to get started is less than most people think. Depending on the application, I\u2019ve seen good results with 100 or even fewer examples. Here are a few applications where I have seen fine-tuning applied successfully: Improving accuracy of critical applications. Prompting can get you really far for many applications. But sometimes, fine-tuning helps eke out that last bit of accuracy. For example, if you are building a customer service chatbot and need it to call the right API reliably (say, to carry out transactions, issue refunds, and the like), perhaps prompting can get it to make the right API call 95% of the time. But if you struggle to raise the accuracy even with revisions to the prompt and you really need 99% accuracy, fine-tuning on a dataset of conversations and API calls might be a good way to get you there. This is particularly true for tasks where it's hard to specify, using only language, an unambiguous rule to decide what to do. For example, when a customer is frustrated, should the chatbot escalate to a manager or just issue a refund? Teams often write Standard Operating Procedures (SOPs) for human workers to follow, and these SOPs can go into the prompts of models. But if it is hard to specify an unambiguous SOP, so even humans need to see numerous examples before they can learn what to do, fine-tuning can be a good approach. For many text-classification applications fine-tuning also works well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims. Learning a particular style of communication. As I explain in \u201cGenerative AI for Everyone,\u201d my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain", "article_id": "68491ffd617fcb072014e787", "linked_images": ["article_68491ffd617fcb072014e787_img_0", "article_68491ffd617fcb072014e787_img_1", "article_68491ffd617fcb072014e787_img_2", "article_68491ffd617fcb072014e787_img_3", "article_68491ffd617fcb072014e787_img_4", "article_68491ffd617fcb072014e787_img_5"]}, {"type": "text", "text": "well, for example, classifying medical records into diagnosis and procedure codes for health insurance claims. Learning a particular style of communication. As I explain in \u201cGenerative AI for Everyone,\u201d my team fine-tuned a model to sound like me. Many people (including myself) have idiosyncratic uses of language. There are certain words I tend to say and others I tend not to, and these idiosyncrasies are numerous and very difficult to specify in a text prompt. (By the way, the avatar at deeplearning.ai/avatar, built with RealAvatar, uses fine-tuning for this reason.) To get a system to communicate in a certain style, fine-tuning is often a superior solution to prompting alone. Reducing latency or cost during scale-ups. I\u2019ve seen applications where developers have successfully prompted a large model to perform a complex task. But as usage scales up, if the large model is too slow (which often happens) or too expensive (which also happens but less frequently), the team might want to use a smaller model. If, however, the performance of the smaller model isn't good enough, then fine-tuning it can help bring it up to the performance of the larger one for that narrow application. Further, the larger model (or perhaps an agentic workflow) can also be used to generate data to help with fine-tuning the small model for that task. At the cutting edge of research, some teams are fine-tuning models to get better at a certain language. But with few exceptions, if the goal is to get an LLM to better understand a body of knowledge that is not in its training data, I find that using RAG (retrieval augmented generation) is a much simpler approach, and I still occasionally run into teams using fine-tuning for which I think RAG would work better. Overall my sense is that, of all the teams I see using fine-tuning, perhaps 75% could get good results using simpler techniques (like prompting or agentic workflows), but in 25% of cases I know of no better way to achieve their goal. It is still technically challenging to implement fine-tuning, get the hyperparameters right, optimize the compute resources, and so on. We are lucky that more and more companies have worked hard to optimize these and provide efficient fine-tuning services. Many of them allow us to fine-tune open weights models and also download the fine-tuned weights. Some allow us to fine-tune their closed models and continue to keep the tuned weights closed. Both can be useful, but the former has obvious advantages of portability and not having to worry that the provider will stop serving a particular model, causing a critical component in our software to become deprecated. In conclusion, before fine-tuning, consider if you should be trying just a bit harder with prompting or agentic workflows, which can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it\u2019s a critical piece of a small minority of them. Keep learning! Andrew In \u201cVibe Coding 101 with Replit,\u201d you\u2019ll learn to plan, prompt,", "article_id": "68491ffd617fcb072014e787", "linked_images": ["article_68491ffd617fcb072014e787_img_0", "article_68491ffd617fcb072014e787_img_1", "article_68491ffd617fcb072014e787_img_2", "article_68491ffd617fcb072014e787_img_3", "article_68491ffd617fcb072014e787_img_4", "article_68491ffd617fcb072014e787_img_5"]}, {"type": "text", "text": "can lead to simpler solutions that are easier to maintain. The vast majority of applications my teams build do not use any fine-tuning at all, but it\u2019s a critical piece of a small minority of them. Keep learning! Andrew In \u201cVibe Coding 101 with Replit,\u201d you\u2019ll learn to plan, prompt, and debug alongside a coding agent. Build, host, and share two real web apps in Replit\u2019s cloud environment while developing effective development skills like writing product requirements, structuring tasks, and refining AI-generated code. Start today Google updated its open-weights family of large language models to include versions that handle image and video inputs. What\u2019s new: Google released its Gemma 3 multilingual large language models with parameter counts of 1 billion, 4 billion, 12 billion, and 27 billion. While the smallest processes text only, the other three are vision-language models that are small enough to run on a consumer hardware. How it works: Gemma 3 rearchitects and refines earlier Gemma models for higher performance at lower parameter counts. Performance: Gemma 3 models outperform Gemma 2 models of equal or larger size by several measures, and all sizes show a strong ability to solve mathematics word problems as measured by MATH. Hot on Gemma 3\u2019s heels: Shortly after Gemma 3 became available, Mistral released Small 3.1 (24 billion parameters), a vision-language model with open weights, under a more permissive Apache 2.0 license. Why it matters: Gemma 3 takes advantage of a variety of techniques to raise the bar for vision-language performance in relatively small models. Knowledge distillation, multiple rounds of reinforcement learning, and fine-tuning on many languages are a powerful combination. We\u2019re thinking: A vision-language model small enough to run on a smartphone feels increasingly close! Diffusion models usually take many noise-removal steps to produce an image, which takes time at inference. There are ways to reduce the number of steps, but the resulting systems are less effective. Researchers devised a streamlined approach that doesn\u2019t sacrifice output quality. What\u2019s new: Kevin Frans and colleagues at UC Berkeley introduced shortcut models that learn to take larger noise-removal steps and thus require fewer steps to generate an image. Key insight: At inference, a scheduler like Euler can enable a model to take larger steps than those it learned during training, but this approach yields worse performance. Alternatively distillation, in which a student model learns to remove the same amount of noise as a teacher model when it takes several steps, offers improved performance at the cost of more cumbersome development. Training the model directly to take bigger steps \u2014 that are equivalent to multiple smaller steps \u2014 enables it to maintain high performance while taking fewer steps. How it works: The authors trained DiT-B, a diffusion transformer, to generate images like those in CelebA-HQ (celebrity faces) and ImageNet-256 (various subjects, size 256x256). Results: The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results using Fr\u00e9chet inception distance (FID), which assesses how closely generated images resemble real-world images (lower", "article_id": "68491ffd617fcb072014e787", "linked_images": ["article_68491ffd617fcb072014e787_img_0", "article_68491ffd617fcb072014e787_img_1", "article_68491ffd617fcb072014e787_img_2", "article_68491ffd617fcb072014e787_img_3", "article_68491ffd617fcb072014e787_img_4", "article_68491ffd617fcb072014e787_img_5"]}, {"type": "text", "text": "and ImageNet-256 (various subjects, size 256x256). Results: The authors compared their model using 1, 4, or 128 steps to alternatives that were trained via various methods including many variants of distillation. They measured the results using Fr\u00e9chet inception distance (FID), which assesses how closely generated images resemble real-world images (lower is better). Why it matters: Generating images by diffusion is typically costly, and previous approaches to cutting the cost have compromised either performance or incurred additional development expense or both. This method achieves high performance at relatively low cost. We\u2019re thinking: As diffusion models continue to become cheaper and faster, we expect to see applications blossom! Students benefit from tutoring, but training tutors is expensive. A study shows that large language models can boost tutors\u2019 effectiveness in real time. What\u2019s new: Rose Wang and colleagues at Stanford built Tutor CoPilot, a tool for remote, online tutors that uses GPT-4 to generate hints, explanations, questions, and other helpful responses to students. Key insight: When a student makes an error, according to previous work by some of the same authors, effective teachers choose a strategy for addressing the mistake. The authors identified 11 strategies, such as ask a question, explain a concept, provide a hint, or encourage the student. Moreover, they found that an LLM that executed a strategy chosen by an expert teacher performed significantly better than an LLM that was prompted with a strategy chosen at random or no specific strategy. Letting inexperienced tutors choose a strategy while an LLM generates a response helps them learn how to execute the strategy. Students, in turn, benefit from responses that mimic those of an experienced teacher. How it works: The authors outfitted a remote tutoring application with GPT-4. Results: The authors partnered with a virtual tutoring company and a school district in the United States for a two-month study of 874 tutors and 1,787 students between grades 3 and 8. They divided the participants into two groups. In one group, tutors conducted sessions with students as usual. In the other, tutors had access to Tutor CoPilot. The authors measured success by the percentage of students who passed a test at the end of a lesson. Yes, but: The authors found statistically significant improvements as measured by test results per lesson, but not in end-of-year exam results. The study\u2019s two-month duration may account for the lack of evidence for longer-term effects. Why it matters: LLMs hold great promise for helping to educate students, but they also show potential in educating teachers. For inexperienced tutors who are learning how to interact with students, an LLM\u2019s general knowledge and pedagogical insights gleaned from expert teachers make a powerful combination. We\u2019re thinking: Although it relies on sophisticated technology, the authors\u2019 approach is simple: Prompt an LLM to apply proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines. Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2. What\u2019s new: Sihyun Yu and colleagues at Korea Advanced Institute of", "article_id": "68491ffd617fcb072014e787", "linked_images": ["article_68491ffd617fcb072014e787_img_0", "article_68491ffd617fcb072014e787_img_1", "article_68491ffd617fcb072014e787_img_2", "article_68491ffd617fcb072014e787_img_3", "article_68491ffd617fcb072014e787_img_4", "article_68491ffd617fcb072014e787_img_5"]}, {"type": "text", "text": "proven teaching principles. Presumably such principles apply beyond elementary math, which would make this approach useful for teaching a variety of disciplines. Diffusion transformers learn faster when they can look at embeddings generated by a pretrained model like DINOv2. What\u2019s new: Sihyun Yu and colleagues at Korea Advanced Institute of Science and Technology, Korea University, New York University, and Scaled Foundations (a startup that builds AI for robotics) proposed Representation Alignment (REPA), a loss term for transformer-based diffusion. Key insight: Diffusion models learn to remove noise from images to which noise was added (and, at inference, they start with pure noise to generate a fresh image). This process can be divided into two parts: learning to (i) embed the noisy image and (ii) estimate the noise from the embedding. One way to accelerate learning is to add a loss term that encourages the diffusion model to produce embeddings that are similar to those produced by a pretrained embedding model. The diffusion model can learn to estimate the noise faster if it doesn\u2019t need to learn how to embed an image from scratch. How it works: The authors modified DiT-XL/2 and SiT-XL/2 transformer-based latent diffusion models, a class of diffusion models that subtract noise from embeddings rather than images. They trained the models to produce images similar to ImageNet. In the process, the modified models learned to produce embeddings similar to those produced by a pretrained DINOv2. Results: The modified DiT-XL/2 learned significantly faster than the unmodified version. Why it matters: Diffusion models and contrastive self-supervised models like DINOv2 have fundamentally different training objectives: One produces embeddings for the purpose of image generation, while the other\u2019s embeddings are used for tasks like classification and semantic segmentation. Consequently, they learn different aspects of data. This work proposes a novel way to combine these approaches to produce more generally useful embeddings. We\u2019re thinking: It turns out that the REPA modification enabled diffusion models to produce embeddings better suited not only to diffusion but also to image classification and segmentation. A similar approach could lead to a more holistic framework for learning image representations.", "article_id": "68491ffd617fcb072014e787", "linked_images": ["article_68491ffd617fcb072014e787_img_0", "article_68491ffd617fcb072014e787_img_1", "article_68491ffd617fcb072014e787_img_2", "article_68491ffd617fcb072014e787_img_3", "article_68491ffd617fcb072014e787_img_4", "article_68491ffd617fcb072014e787_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e787", "linked_chunks": ["article_68491ffd617fcb072014e787_chunk_0", "article_68491ffd617fcb072014e787_chunk_1", "article_68491ffd617fcb072014e787_chunk_2", "article_68491ffd617fcb072014e787_chunk_3", "article_68491ffd617fcb072014e787_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e787", "linked_chunks": ["article_68491ffd617fcb072014e787_chunk_0", "article_68491ffd617fcb072014e787_chunk_1", "article_68491ffd617fcb072014e787_chunk_2", "article_68491ffd617fcb072014e787_chunk_3", "article_68491ffd617fcb072014e787_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e787", "linked_chunks": ["article_68491ffd617fcb072014e787_chunk_0", "article_68491ffd617fcb072014e787_chunk_1", "article_68491ffd617fcb072014e787_chunk_2", "article_68491ffd617fcb072014e787_chunk_3", "article_68491ffd617fcb072014e787_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e787", "linked_chunks": ["article_68491ffd617fcb072014e787_chunk_0", "article_68491ffd617fcb072014e787_chunk_1", "article_68491ffd617fcb072014e787_chunk_2", "article_68491ffd617fcb072014e787_chunk_3", "article_68491ffd617fcb072014e787_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e787", "linked_chunks": ["article_68491ffd617fcb072014e787_chunk_0", "article_68491ffd617fcb072014e787_chunk_1", "article_68491ffd617fcb072014e787_chunk_2", "article_68491ffd617fcb072014e787_chunk_3", "article_68491ffd617fcb072014e787_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e787", "linked_chunks": ["article_68491ffd617fcb072014e787_chunk_0", "article_68491ffd617fcb072014e787_chunk_1", "article_68491ffd617fcb072014e787_chunk_2", "article_68491ffd617fcb072014e787_chunk_3", "article_68491ffd617fcb072014e787_chunk_4"]}, {"type": "text", "text": "Dear friends, Last Friday on Pi Day, we held AI Dev 25, a new conference for AI Developers. Tickets had (unfortunately) sold out days after we announced their availability, but I came away energized by the day of coding and technical discussions with fellow AI Builders! Let me share here my observations from the event. I'd decided to start AI Dev because while there're great academic AI conferences that disseminate research work (such as NeurIPS, ICML and ICLR) and also great meetings held by individual companies, often focused on each company's product offerings, there were few vendor-neutral conferences for AI developers. With the wide range of AI tools now available, there is a rich set of opportunities for developers to build new things (and to share ideas on how to build things!), but also a need for a neutral forum that helps developers do so. Based on an informal poll, about half the attendees had traveled to San Francisco from outside the Bay Area for this meeting, including many who had come from overseas. I was thrilled by the enthusiasm to be part of this AI Builder community. To everyone who came, thank you! Other aspects of the event that struck me: DeepLearning.AI has a strong \u201cLearner First\u201d mentality; our foremost goal is always to help learners. I was thrilled that a few attendees told me they enjoyed how technical the sessions were, and said they learned many things that they're sure they will use. (In fact, I, too, came away with a few ideas from the sessions!) I was also struck that, both during the talks and at the technical demo booths, the rooms were packed with attendees who were highly engaged throughout the whole day. I'm glad that we were able to have a meeting filled with technical and engineering discussions. I'm delighted that AI Dev 25 went off so well, and am grateful to all the attendees, volunteers, speakers, sponsors, partners, and team members that made the event possible. I regretted only that the physical size of the event space prevented us from admitting more attendees this time. There is something magical about bringing people together physically to share ideas, make friends, and to learn from and help each other. I hope we'll be able to bring even more people together in the future. Keep building! Andrew P.S. I'm thrilled to share our newest course series: the Data Analytics Professional Certificate! Data analytics remains one of the core skills of data science and AI, and this professional certificate takes you up to being job-ready for this. Led by Netflix data science leader Sean Barnes, this certificate gives you hands-on experience with essential tools like SQL, Tableau, and Python, while teaching you to use Generative AI effectively as a thought partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities", "article_id": "68491ffd617fcb072014e78e", "linked_images": ["article_68491ffd617fcb072014e78e_img_0", "article_68491ffd617fcb072014e78e_img_1", "article_68491ffd617fcb072014e78e_img_2", "article_68491ffd617fcb072014e78e_img_3", "article_68491ffd617fcb072014e78e_img_4", "article_68491ffd617fcb072014e78e_img_5", "article_68491ffd617fcb072014e78e_img_6"]}, {"type": "text", "text": "partner in your analyses. Labor economists project a 36% growth in data science jobs by 2033. I'm excited to see rising demand for data professionals, since working with data is such a powerful way to improve decision-making, whether in business, software development, or your private life. Data skills create opportunities at every level\u2014I\u2019m excited to see where they take you! Sign up here! The Data Analytics Professional Certificate is available now! This program equips you with data analytics skills\u2014from foundations to job-ready. Learn statistical techniques combined with newly emerging generative AI workflows. Enroll now Multilingual AI models often suffer uneven performance across languages, especially in multimodal tasks. A pair of lean models counters this trend with consistent understanding of text and images across major languages. What\u2019s new: A team at Cohere led by Saurabh Dash released Aya Vision, a family of multilingual vision-language models with downloadable weights in 8 billion- and 32-billion-parameter sizes. How it works: Each model comprises a pretrained large language model (Aya Expanse for the 32B model, C4AI Command R7B for the 8B version), a pretrained vision encoder (SigLIP 2), and a vision-language adapter (\u201cconnector\u201d) of unspecified architecture. Performance: To test the model, the team built and released two benchmarks: m-WildVision, a multilingual version of Wild Vision Bench\u2019s arena-style competition for discussion of images, and AyaVisionBench, 135 image-question pairs in each language that cover nine tasks including captioning images, understanding charts, recognizing characters in images, visual reasoning, and converting screenshots to code. On these two benchmarks, Aya Vision 8B and 32B outperformed larger competitors, as judged by Claude 3.7 Sonnet. Behind the news: Aya Vision builds on the Cohere-led Aya initiative, a noncommercial effort to build models that perform consistently well in all languages, especially languages that lack high-quality training data. The project started with a multilingual text model (Aya Expanse), added vision (Aya Vision), and plans to eventually add video and audio. Why it matters: Multilingual vision-language models often perform less well in low-resource languages, and the gap widens when they process media other than text. Aya Vision\u2019s recipe for augmenting synthetic data with successively refined translations may contribute to more universally capable models. Aya Vision is available on the global messaging platform WhatsApp, where it can be used to translate text and images in all 23 of its current languages. We\u2019re thinking: Multilingual vision models could soon help non-native speakers decipher Turkish road signs, Finnish legal contracts, and Korean receipts. We look forward to a world in which understanding any scene or document is as effortless in Swahili as it is in English. An AI agent synthesizes novel scientific research hypotheses. It's already making an impact in biomedicine. What\u2019s new: Google introduced AI co-scientist, a general multi-agent system designed to generate in-depth research proposals within constraints specified by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It\u2019s available to research organizations on a limited basis. How it works: AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In", "article_id": "68491ffd617fcb072014e78e", "linked_images": ["article_68491ffd617fcb072014e78e_img_0", "article_68491ffd617fcb072014e78e_img_1", "article_68491ffd617fcb072014e78e_img_2", "article_68491ffd617fcb072014e78e_img_3", "article_68491ffd617fcb072014e78e_img_4", "article_68491ffd617fcb072014e78e_img_5", "article_68491ffd617fcb072014e78e_img_6"]}, {"type": "text", "text": "by the user. The team generated and evaluated proposals for repurposing drugs, identifying drug targets, and explaining antimicrobial resistance in real-world laboratories. It\u2019s available to research organizations on a limited basis. How it works: AI co-scientist accepts a text description of a research goal, including relevant constraints or ideas. In response, it generates research proposals and reviews, ranks, and improves them using seven agents based on Google\u2019s Gemini 2.0 family of large language models. The completed proposals include sections that explain background, unmet needs, a proposed solution, goals, hypotheses, reasoning, study steps, and relevant articles. The agents take feedback and outputs from other agents to perform their prompted task simultaneously. Results: AI co-scientist achieved a number of impressive biomedical results in tests. Behind the news: A few AI systems have begun to produce original scientific work. For instance, a model generated research proposals that human judges deemed more novel than proposals written by flesh-and-blood scientists, and an agentic workflow produced research papers that met standards for acceptance by top conferences. Why it matters: While previous work used agentic workflows to propose research ideas on a general topic, this work generates proposals for specific ideas according to a researcher\u2019s constraints (for example, a researcher could specify that a novel medical treatment for a specific disease only consider drugs already approved for human trials for other uses) and further instructions. AI co-scientist can take feedback at any point, allowing humans to collaborate with the machine: People provide ideas, feedback, and guidance for the model, and the model researches and proposes ideas in return. We\u2019re thinking: I asked my AI system to propose a new chemical experiment. But there was no reaction! The United States Copyright Office determined that existing laws are sufficient to decide whether a given AI-generated work is protected by copyright, making additional legislation unnecessary. What\u2019s new: AI-generated works qualify for copyright if a human being contributed enough creative input, according to the second part of what will be a three-part report on artificial intelligence and copyright law. How it works: The report states that \u201cthe outputs of generative AI can be protected by copyright only where a human author has determined sufficient expressive elements.\u201d In other words, humans and AI can collaborate on creative works, but copyright protection applies only if a human shapes the AI-generated material beyond simply supplying a prompt. Behind the news: The first part of the Copyright Office\u2019s report on digital replicas, or generated likenesses of a person\u2019s appearance and voice. It found that existing laws don\u2019t provide sufficient protection against unauthorized digital replicas and recommended federal legislation to address the gap. Its findings influenced ongoing discussions in Congress, where proposed bills like the No AI FRAUD Act and the NO FAKES Act aim to regulate impersonation via AI. Additionally, industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers, artists, and authors from unauthorized digital reproduction. However, no federal law currently defines whether copyright can protect a person\u2019s likeness or performance. Why it matters:", "article_id": "68491ffd617fcb072014e78e", "linked_images": ["article_68491ffd617fcb072014e78e_img_0", "article_68491ffd617fcb072014e78e_img_1", "article_68491ffd617fcb072014e78e_img_2", "article_68491ffd617fcb072014e78e_img_3", "article_68491ffd617fcb072014e78e_img_4", "article_68491ffd617fcb072014e78e_img_5", "article_68491ffd617fcb072014e78e_img_6"]}, {"type": "text", "text": "via AI. Additionally, industry groups such as the Authors Guild and entertainment unions have pursued their own agreements with studios and publishers to safeguard performers, artists, and authors from unauthorized digital reproduction. However, no federal law currently defines whether copyright can protect a person\u2019s likeness or performance. Why it matters: The Copyright Office deliberately avoided prescribing rigid criteria for the types or degrees of human input that are sufficient for copyright. Such determinations require nuanced evaluation case by case. This flexible approach accommodates the diverse ways creative people use AI as well as unforeseen creative possibilities of emerging technology. We\u2019re thinking: Does copyright bar the use of protected works to train AI systems? The third part of the Copyright Office\u2019s report \u2014 no indication yet as to when to expect it \u2014 will address this question. The answer could have important effects on both the arts and AI development. Materials that have specific properties are essential to progress in critical technologies like solar cells and batteries. A machine learning model designs new materials to order. What\u2019s new: Researchers at Microsoft and Shenzhen Institute of Advanced Technology proposed MatterGen, a diffusion model that generates a material\u2019s chemical composition and structure from a prompt that specifies a desired property. The model and code are available under a license that allows commercial as well as noncommercial uses without limitation. The training data also is noncommercially available. How it works: MatterGen\u2019s training followed a two-stage process. In the first stage, it learned to generate materials (specifically crystals \u2014 no liquids, gasses, or amorphous solids like glass). In the second, it learned to generate materials given a target mechanical, electronic, magnetic, or chemical property such as magnetic density or bulk modulus (the material\u2019s resistance to compression). Results: The authors generated a variety of materials, and they synthesized one to test whether it had a target property. Specifically, they generated over 8,000 candidates with the target bulk modulus of 200 gigapascals (a measure of resistance to uniform compression), then automatically filtered them based on a number of factors to eliminate material in their dataset and unstable materials. Of the remaining candidates, they chose four manually and successfully synthesized one. The resulting crystal had a measured bulk modulus of 158 gigapascals. (Most materials in the dataset had a bulk modulus of between 0 and 400 gigapascals.) Behind the news: Published in 2023, DiffCSP also uses a diffusion model to generate the structures of new materials. However, it does so without considering their desired properties. Why it matters: Discovering materials relies mostly on searching large databases of existing materials for those with desired properties or synthesizing new materials and testing their properties by trial and error. Designing new crystals with desired properties at the click of a button accelerates the process dramatically. We\u2019re thinking: While using AI to design materials accelerates an important step, determining whether a hypothesized material can be manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing. In our latest short course, Long-Term", "article_id": "68491ffd617fcb072014e78e", "linked_images": ["article_68491ffd617fcb072014e78e_img_0", "article_68491ffd617fcb072014e78e_img_1", "article_68491ffd617fcb072014e78e_img_2", "article_68491ffd617fcb072014e78e_img_3", "article_68491ffd617fcb072014e78e_img_4", "article_68491ffd617fcb072014e78e_img_5", "article_68491ffd617fcb072014e78e_img_6"]}, {"type": "text", "text": "process dramatically. We\u2019re thinking: While using AI to design materials accelerates an important step, determining whether a hypothesized material can be manufactured efficiently at scale is still challenging. We look forward to research into AI models that also take into account ease of manufacturing. In our latest short course, Long-Term Agentic Memory with LangGraph, learn how to integrate semantic, episodic, and procedural memory into AI workflows. Guided by Harrison Chase, you\u2019ll build a personal email agent with routing, writing, and scheduling tools to automatically ignore and respond to emails, while keep track of facts and past actions over time. Join in for free", "article_id": "68491ffd617fcb072014e78e", "linked_images": ["article_68491ffd617fcb072014e78e_img_0", "article_68491ffd617fcb072014e78e_img_1", "article_68491ffd617fcb072014e78e_img_2", "article_68491ffd617fcb072014e78e_img_3", "article_68491ffd617fcb072014e78e_img_4", "article_68491ffd617fcb072014e78e_img_5", "article_68491ffd617fcb072014e78e_img_6"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e78e", "linked_chunks": ["article_68491ffd617fcb072014e78e_chunk_0", "article_68491ffd617fcb072014e78e_chunk_1", "article_68491ffd617fcb072014e78e_chunk_2", "article_68491ffd617fcb072014e78e_chunk_3", "article_68491ffd617fcb072014e78e_chunk_4"]}, {"type": "text", "text": "Dear friends, Some people today are discouraging others from learning programming on the grounds AI will automate it. This advice will be seen as some of the worst career advice ever given. I disagree with the Turing Award and Nobel prize winner who wrote, \u201cIt is far more likely that the programming occupation will become extinct [...] than that it will become all-powerful. More and more, computers will program themselves.\u201d\u200b Statements discouraging people from learning to code are harmful! In the 1960s, when programming moved from punchcards (where a programmer had to laboriously make holes in physical cards to write code character by character) to keyboards with terminals, programming became easier. And that made it a better time than before to begin programming. Yet it was in this era that Nobel laureate Herb Simon wrote the words quoted in the first paragraph. Today\u2019s arguments not to learn to code continue to echo his comment. As coding becomes easier, more people should code, not fewer! Over the past few decades, as programming has moved from assembly language to higher-level languages like C, from desktop to cloud, from raw text editors to IDEs to AI assisted coding where sometimes one barely even looks at the generated code (which some coders recently started to call vibe coding), it is getting easier with each step. (By the way, to learn more about AI assisted coding, check out our video-only short course, \u201cBuild Apps with Windsurf\u2019s AI Coding Agents.\u201d) I wrote previously that I see tech-savvy people coordinating AI tools to move toward being 10x professionals \u2014 individuals who have 10 times the impact of the average person in their field. I am increasingly convinced that the best way for many people to accomplish this is not to be just consumers of AI applications, but to learn enough coding to use AI-assisted coding tools effectively. One question I\u2019m asked most often is what someone should do who is worried about job displacement by AI. My answer is: Learn about AI and take control of it, because one of the most important skills in the future will be the ability to tell a computer exactly what you want, so it can do that for you. Coding (or getting AI to code for you) is the best way to do that. When I was working on the course Generative AI for Everyone and needed to generate AI artwork for the background images, I worked with a collaborator who had studied art history and knew the language of art. He prompted Midjourney with terminology based on the historical style, palette, artist inspiration and so on \u2014 using the language of art \u2014 to get the result he wanted. I didn\u2019t know this language, and my paltry attempts at prompting could not deliver as effective a result. Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As", "article_id": "68491ffd617fcb072014e796", "linked_images": ["article_68491ffd617fcb072014e796_img_0", "article_68491ffd617fcb072014e796_img_1", "article_68491ffd617fcb072014e796_img_2", "article_68491ffd617fcb072014e796_img_3", "article_68491ffd617fcb072014e796_img_4", "article_68491ffd617fcb072014e796_img_5"]}, {"type": "text", "text": "not deliver as effective a result. Similarly, scientists, analysts, marketers, recruiters, and people of a wide range of professions who understand the language of software through their knowledge of coding can tell an LLM or an AI-enabled IDE what they want much more precisely, and get much better results. As these tools continue to make coding easier, this is the best time yet to learn to code, to learn the language of software, and learn to make computers do exactly what you want them to do. Keep building! Andrew Pre-enroll now for the new Data Analytics Professional Certificate! Gain job-ready skills in data analysis, whether you\u2019re starting a career as a data analyst or enhancing your ability to prepare, analyze, and visualize data in your current role. This program covers both classical statistical techniques and emerging AI-assisted workflows to help you work smarter with data. Learn more and sign up Most models that have learned to reason via reinforcement learning were huge models. A much smaller model now competes with them. What\u2019s new: Alibaba introduced QwQ-32B, a large language model that rivals the reasoning prowess of DeepSeek-R1 despite its relatively modest size. How it works: QwQ-32B is a version of Qwen2.5-32B that was fine-tuned to generate chains of thought using reinforcement learning (RL). Fine-tuning proceeded in two stages. Performance: On several benchmarks for math, coding, and general problem solving, QwQ-32B outperforms OpenAI o1-mini (parameter count undisclosed) and achieves performance roughly comparable to DeepSeek-R1 (671 billion parameters, 37 billion active at any moment). Behind the news: DeepSeek\u2019s initial model, DeepSeek-R1-Zero, similarly applied RL to a pretrained model. That effort produced strong reasoning but poor readability (for example, math solutions with correct steps but jumbled explanations). To address this shortcoming, the team fine-tuned DeepSeek-R1 on long chain-of-thought examples before applying RL. In contrast, QwQ-32B skipped preliminary fine-tuning and applied RL in two stages, first optimizing for correct responses and then for readability. Why it matters: RL can dramatically boost LLMs\u2019 reasoning abilities, but the order in which different behaviors are rewarded matters. Using RL in stages enabled the team to build a 32 billion parameter model \u2014 small enough to run locally on a consumer GPU \u2014 that rivals a much bigger mixture-of-experts model, bringing powerful reasoning models within reach for more developers. The Qwen team plans to scale its RL approach to larger models, which could improve the next-gen reasoning abilities further while adding greater knowledge. We\u2019re thinking: How far we\u2019ve come since \u201cLet\u2019s think step by step\u201d! Microsoft debuted its first official large language model that responds to spoken input. What\u2019s new: Microsoft released Phi-4-multimodal, an open weights model that processes text, images, and speech simultaneously. How it works: Phi-4-multimodal has six components: Phi-4-mini, vision and speech encoders as well as corresponding projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems. Results: The authors compared Phi-4-multimodal to", "article_id": "68491ffd617fcb072014e796", "linked_images": ["article_68491ffd617fcb072014e796_img_0", "article_68491ffd617fcb072014e796_img_1", "article_68491ffd617fcb072014e796_img_2", "article_68491ffd617fcb072014e796_img_3", "article_68491ffd617fcb072014e796_img_4", "article_68491ffd617fcb072014e796_img_5"]}, {"type": "text", "text": "projectors (which modify the vision or speech embeddings so the base model can understand them), and two LoRA adapters. The LoRA adapters modify the base weights depending on the input: One adapter modifies them for speech-text problems, and one for vision-text and vision-speech problems. Results: The authors compared Phi-4-multimodal to other multimodal models on text-vision, vision-speech, text-speech tasks. Behind the news: This work adds to the growing body of models with voice-in/text-out capability, including the open weights DiVA model developed by a team led by Diyi Yang at Stanford University. Why it matters: The architectural options continue to expand for building neural networks that process text, images, audio, and various combinations. While some teams maintain separate models for separate data modalities, like Qwen2.5 (for text) and Qwen2.5-VL) (for vision-language tasks), others are experimenting with mixture-of-expert models like DeepSeek-V3. Phi-4-multimodal shows that Mixture-of-LoRAs is an effective approach for processing multimodal data \u2014 and gives developers a couple of new open models to play with. We\u2019re thinking: Output guardrails have been built to ensure appropriateness of text output, but this is difficult to apply to a voice-in/voice-out architecture. (Some teams have worked on guardrails that screen audio output directly, but the technology is still early.) For voice-based applications, a voice-in/text-out model can generate a candidate output without a separate, explicit speech-to-text step, and it accommodates text-based guardrails before it decides whether or not to read the output to the user. A United States court delivered a major ruling that begins to answer the question whether, and under what conditions, training an AI system on copyrighted material is considered fair use that doesn\u2019t require permission. What\u2019s new: A U.S. Circuit judge ruled on a claim by the legal publisher Thomson Reuters that Ross Intelligence, an AI-powered legal research service, could not claim that training its AI system on materials owned by Thomson Reuters was a so-called \u201cfair use.\u201d Training the system did not qualify as fair use, he decided, because its output competed with Thomson Reuters\u2019 publications. How it works: Thomson Reuters had sued Ross Intelligence after the defendant trained an AI model using 2,243 works produced by Thomson Reuters without the latter\u2019s permission. This ruling reversed an earlier decision in 2023, when the same judge had allowed Ross Intelligence\u2019s fair-use defense to proceed to trial. In the new ruling, he found that Ross Intelligence\u2019s use failed to meet the definition of fair use in key respects. (A jury trial is scheduled to determine whether Thomson Reuters' copyright was in effect at the time of the infringement and other aspects of the case.) Behind the news: The ruling comes amid a wave of lawsuits over AI training and copyright in several countries. Many of these cases are in progress, but courts have weighed in on some. Why it matters: The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense", "article_id": "68491ffd617fcb072014e796", "linked_images": ["article_68491ffd617fcb072014e796_img_0", "article_68491ffd617fcb072014e796_img_1", "article_68491ffd617fcb072014e796_img_2", "article_68491ffd617fcb072014e796_img_3", "article_68491ffd617fcb072014e796_img_4", "article_68491ffd617fcb072014e796_img_5"]}, {"type": "text", "text": "some. Why it matters: The question of whether training (or copying data to train) AI systems is a fair use of copyrighted works hangs over the AI industry, from academic research to commercial projects. In the wake of this ruling, courts may be more likely to reject a fair-use defense when AI companies train models on copyrighted material to create output that overlaps with or replaces traditional media, as The New York Times alleges in its lawsuit against OpenAI. However, the ruling leaves room for fair use with respect to models whose output doesn\u2019t compete directly with copyrighted works. We\u2019re thinking: Current copyright laws weren\u2019t designed with AI in mind, and rulings like this one fill in the gaps case by case. Clarifying copyright for the era of generative AI could help our field move forward faster. Large language models built by developers in China may, in some applications, be less useful outside that country because they avoid topics its government deems politically sensitive. A developer fine-tuned DeepSeek-R1 to widen its scope without degrading its overall performance. What\u2019s new: Perplexity released R1 1776, a version of DeepSeek-R1 that responds more freely than the original. The model weights are available to download under a commercially permissive MIT license. How it works: The team modified DeepSeek-R1\u2019s knowledge of certain topics by fine-tuning it on curated question-answer pairs. Results: The fine-tuned model responded to politically charged prompts factually without degrading its ability to generate high-quality output. Behind the news: Among the first countries to regulate AI, China requires AI developers to build models that uphold \u201cCore Socialist Values\u201d and produce true and reliable output. When these objectives conflict, the political goal tends to dominate. While large language models built by developers in China typically avoid contentious topics, the newer DeepSeek models enforce this more strictly than older models like Qwen and Yi, using methods akin to Western measures for aligning output, like Reinforcement Learning from Human Feedback and keyword filters. Why it matters: AI models tend to reflect their developers\u2019 values and legal constraints. Perplexity\u2019s targeted fine-tuning approach addresses this barrier to international adoption of open-source models. We\u2019re thinking: As models with open weights are adopted by the global community, they become a source of soft power for their developers, since they tend to reflect their developers\u2019 values. This work reflects a positive effort to customize a model to reflect the user\u2019s values instead \u2014 though how many developers will seek out a fine-tuned version rather than the original remains to be seen.", "article_id": "68491ffd617fcb072014e796", "linked_images": ["article_68491ffd617fcb072014e796_img_0", "article_68491ffd617fcb072014e796_img_1", "article_68491ffd617fcb072014e796_img_2", "article_68491ffd617fcb072014e796_img_3", "article_68491ffd617fcb072014e796_img_4", "article_68491ffd617fcb072014e796_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e796", "linked_chunks": ["article_68491ffd617fcb072014e796_chunk_0", "article_68491ffd617fcb072014e796_chunk_1", "article_68491ffd617fcb072014e796_chunk_2", "article_68491ffd617fcb072014e796_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e796", "linked_chunks": ["article_68491ffd617fcb072014e796_chunk_0", "article_68491ffd617fcb072014e796_chunk_1", "article_68491ffd617fcb072014e796_chunk_2", "article_68491ffd617fcb072014e796_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e796", "linked_chunks": ["article_68491ffd617fcb072014e796_chunk_0", "article_68491ffd617fcb072014e796_chunk_1", "article_68491ffd617fcb072014e796_chunk_2", "article_68491ffd617fcb072014e796_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e796", "linked_chunks": ["article_68491ffd617fcb072014e796_chunk_0", "article_68491ffd617fcb072014e796_chunk_1", "article_68491ffd617fcb072014e796_chunk_2", "article_68491ffd617fcb072014e796_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e796", "linked_chunks": ["article_68491ffd617fcb072014e796_chunk_0", "article_68491ffd617fcb072014e796_chunk_1", "article_68491ffd617fcb072014e796_chunk_2", "article_68491ffd617fcb072014e796_chunk_3"]}, {"type": "image", "article_id": "68491ffd617fcb072014e796", "linked_chunks": ["article_68491ffd617fcb072014e796_chunk_0", "article_68491ffd617fcb072014e796_chunk_1", "article_68491ffd617fcb072014e796_chunk_2", "article_68491ffd617fcb072014e796_chunk_3"]}, {"type": "text", "text": "Dear friends, Continuing our discussion on the Voice Stack, I\u2019d like to explore an area that today\u2019s voice-based systems mostly struggle with: Voice Activity Detection (VAD) and the turn-taking paradigm of communication. When communicating with a text-based chatbot, the turns are clear: You write something, then the bot does, then you do, and so on. The success of text-based chatbots with clear turn-taking has influenced the design of voice-based bots, most of which also use the turn-taking paradigm. A key part of building such a system is a VAD component to detect when the user is talking. This allows our software to take the parts of the audio stream in which the user is saying something and pass that to the model for the user\u2019s turn. It also supports interruption in a limited way, whereby if a user insistently interrupts the AI system while it is talking, eventually the VAD system will realize the user is talking, shut off the AI\u2019s output, and let the user take a turn. This works reasonably well in quiet environments. However, VAD systems today struggle with noisy environments, particularly when the background noise is from other human speech. For example, if you are in a noisy cafe speaking with a voice chatbot, VAD \u2014 which is usually trained to detect human speech \u2014 tends to be inaccurate at figuring out when you, or someone else, is talking. (In comparison, it works much better if you are in a noisy vehicle, since the background noise is more clearly not human speech.) It might think you are interrupting when it was merely someone in the background speaking, or fail to recognize that you\u2019ve stopped talking. This is why today\u2019s speech applications often struggle in noisy environments. Intriguingly, last year, Kyutai Labs published Moshi, a model (GitHub) that had many technical innovations. An important one was enabling persistent bi-direction audio streams from the user to Moshi and from Moshi to the user. If you and I were speaking in person or on the phone, we would constantly be streaming audio to each other (through the air or the phone system), and we\u2019d use social cues to know when to listen and how to politely interrupt if one of us felt the need. Thus, the streams would not need to explicitly model turn-taking. Moshi works like this. It\u2019s listening all the time, and it\u2019s up to the model to decide when to stay silent and when to talk. This means an explicit VAD step is no longer necessary. (Moshi also included other innovations, such as an \u201cinner monologue\u201d that simultaneously generates text alongside the audio to improve the quality of responses as well as audio encoding.) Just as the architecture of text-only transformers has gone through many evolutions (such as encoder-decoder models, decoder-only models, and reasoning models that generate a lot of \u201creasoning tokens\u201d before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice", "article_id": "68491ffd617fcb072014e79d", "linked_images": ["article_68491ffd617fcb072014e79d_img_0", "article_68491ffd617fcb072014e79d_img_1", "article_68491ffd617fcb072014e79d_img_2", "article_68491ffd617fcb072014e79d_img_3", "article_68491ffd617fcb072014e79d_img_4", "article_68491ffd617fcb072014e79d_img_5"]}, {"type": "text", "text": "encoder-decoder models, decoder-only models, and reasoning models that generate a lot of \u201creasoning tokens\u201d before the final output), voice models are going through a lot of architecture explorations. Given the importance of foundation models with voice-in and voice-out capabilities, many large companies right now are investing in developing better voice models. I\u2019m confident we\u2019ll see many more good voice models released this year. It feels like the space of potential innovation for voice remains large. Hard technical problems, like the one of latency that I described last week and VAD errors, remain to be solved. As solutions get better, voice-to-voice will continue to be a promising category to build applications in. Keep building! Andrew Learn to build an event-driven AI agent that processes documents and fills out forms using RAG, workflows, and human-in-the-loop feedback. This course, built in partnership with LlamaIndex, walks you through designing, building, and refining automated document workflows. Enroll for free Typical large language models are autoregressive, predicting the next token, one at a time, from left to right. A new model hones all text tokens at once. What\u2019s new: Inception Labs, a Silicon Valley startup, emerged from stealth mode with Mercury Coder, a diffusion model that generates code, in small and mini versions. Registered users can try it out here, and an API (sign up for early access here) and on-premises deployments are in the works. The company has not yet announced availability and pricing. How it works: Like image diffusion models, Mercury Coder improves its output over a number of steps by removing noise. Results: Mercury Coder\u2019s major advantage is speed, but it also performs well compared to several competitors. Behind the news: Several teams have built diffusion models that generate text, but previous efforts have not been competitive with autoregressive large language models (LLMs). Recently, LLaDA showed comparable performance to Meta\u2019s Llama 2 7B but fell short of Llama 3 8B and other similarly sized modern LLMs. Why it matters: Text diffusion models are already faster than autoregressive models. They offer significant promise to accelerate text generation even further. We\u2019re thinking: Diffusion image generators have delivered good output with as little as four or even one step, generating output tokens significantly faster than autoregressive models. If text diffusion models can benefit from improvements in image generation, they could lead to rapid generation of lengthy texts and, in turn, faster agents and reasoning. OpenAI launched GPT-4.5, which may be its last non-reasoning model. What\u2019s new: GPT-4.5 is available as a research preview. Unlike OpenAI\u2019s recent models o1 and o3, GPT-4.5 is not fine-tuned to reason by generating a chain of thought, although the company hinted that it may serve as a basis of a reasoning model in the future. Instead, it\u2019s a huge model that was trained using a huge amount of computation. As OpenAI\u2019s biggest model to date, GPT-4.5 is very expensive to run, and the company is evaluating whether to offer it via API in the long term. How it works: OpenAI revealed few details about how GPT-4.5 was built. The model is bigger than", "article_id": "68491ffd617fcb072014e79d", "linked_images": ["article_68491ffd617fcb072014e79d_img_0", "article_68491ffd617fcb072014e79d_img_1", "article_68491ffd617fcb072014e79d_img_2", "article_68491ffd617fcb072014e79d_img_3", "article_68491ffd617fcb072014e79d_img_4", "article_68491ffd617fcb072014e79d_img_5"]}, {"type": "text", "text": "using a huge amount of computation. As OpenAI\u2019s biggest model to date, GPT-4.5 is very expensive to run, and the company is evaluating whether to offer it via API in the long term. How it works: OpenAI revealed few details about how GPT-4.5 was built. The model is bigger than GPT-4o, and it was pretrained and fine-tuned on more data using more computation \u2014 possibly 10x more, given OpenAI\u2019s comment that \u201cwith every new order of magnitude of compute comes novel capabilities.\u201d Performance: \u201cThis isn\u2019t a reasoning model and won\u2019t crush benchmarks,\u201d OpenAI CEO Sam Altman warned in a tweet. The company claims that GPT-4.5 offers improved general knowledge, adheres to prompts with more nuance, delivers greater creativity, and has higher emotional intelligence. Behind the news: GPT-4.5\u2019s release comes as OpenAI nears an announced transition away from developing separate general-knowledge and reasoning models. The launch also comes as OpenAI faces an ongoing shortage of processing power. CEO Sam Altman said that the company is \u201cout of GPUs\u201d and struggling to meet demand \u2014 a constraint that may impact whether OpenAI continues to offer GPT-4.5 via API. Why it matters: GPT-4.5 highlights a growing divide in AI research over whether to pursue performance gains by scaling up processing during pretraining or inference. Despite the success of approaches that consume extra processing power at inference, such as agentic techniques and reasoning models such as its own o family, OpenAI clearly still sees value in pretraining larger and larger models. We\u2019re thinking: There\u2019s still more juice to be squeezed out of bigger models! We\u2019re excited to see what the combination of additional compute applied to both pretraining and inference can achieve. Anthropic\u2019s Claude 3.7 Sonnet implements a hybrid reasoning approach that lets users decide how much thinking they want the model to do before it renders a response. What\u2019s new: Claude 3.7 Sonnet was trained for strong performance in coding and front-end web development, with less emphasis on math and computer-science competition problems. It implements tool use and computer use (but not web search) and lets users toggle between immediate responses and extended thinking mode, which can improve outputs by allocating a specific number of tokens to reasoning at inference. Like DeepSeek-R1 and Google Gemini Flash Thinking \u2014 and unlike OpenAI o1 \u2014 Claude 3.7 Sonnet fully displays reasoning tokens. Anthropic considers this functionality experimental, so it may change. How it works: Anthropic pretrained Claude 3.7 Sonnet on a mix of public and proprietary data (which explicitly did not include Claude users\u2019 inputs and outputs). The team fine-tuned Claude 3.7 Sonnet using constitutional AI, which encourages a model to follow a set of human-crafted rules. Performance: Claude 3.7 Sonnet shows exceptional performance in general knowledge, software engineering, and agentic tasks. Behind the news: Anthropic\u2019s approach refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or \u201ceffort\u201d \u2014 each of which allocates more tokens to reasoning \u2014 while X\u2019s Grok 3 offers two. Why it matters: Test-time compute, or", "article_id": "68491ffd617fcb072014e79d", "linked_images": ["article_68491ffd617fcb072014e79d_img_0", "article_68491ffd617fcb072014e79d_img_1", "article_68491ffd617fcb072014e79d_img_2", "article_68491ffd617fcb072014e79d_img_3", "article_68491ffd617fcb072014e79d_img_4", "article_68491ffd617fcb072014e79d_img_5"]}, {"type": "text", "text": "refines earlier efforts to enable users to control the incremental expense of computing extra tokens at inference. For instance, OpenAI o1 offers three levels of reasoning or \u201ceffort\u201d \u2014 each of which allocates more tokens to reasoning \u2014 while X\u2019s Grok 3 offers two. Why it matters: Test-time compute, or additional processing at inference, is powerful but expensive, and not all tasks benefit from it. So it\u2019s helpful to let users choose how much to apply. Claude 3.7 Sonnet improves its predecessor\u2019s general performance and provides an ample budget for additional reasoning. We\u2019re thinking: The cost of inference is rising as agentic workflows and other compute-intensive tasks become more widely used. Yet the cost of AI on a per-token basis is falling rapidly. Intelligence is becoming steadily cheaper and more plentiful. Amazon announced Alexa+, a major upgrade to its long-running voice assistant. What\u2019s new: Alexa+, which accepts spoken commands and responds conversationally, is designed to work with a variety of vendors as an autonomous agent to make purchases, book reservations, play media, and so on. It will roll out in the U.S. over coming weeks, initially on some Echo Show devices and eventually nearly every current Echo speaker. How it works: Alexa+ updates the system to take advantage of generative AI including Anthropic Claude, Amazon Nova, and other large language models. Inputs are filtered through a routing system that determines the best model to respond to any given request. It\u2019s trained to understand colloquial, conversational language. Its personality is designed to be \u201csmart, considerate, empathetic, and inclusive\u201d as well as humorous. Behind the news: Amazon launched Alexa in 2014, and the voice assistant now resides in over 600 million devices worldwide. However, users relied on it more to set timers, report sports scores, and play music than to purchase products, and Alexa revenue lagged. Following cutbacks in 2021, Amazon made multibillion-dollar investments in Anthropic and set about updating the technology for the generative AI era. Why it matters: Alexa, along with Apple\u2019s Siri and Google Assistant, pioneered the market for voice assistants. However, as large language models (LLMs) blossomed, all three systems fell behind the times. (Google allows Android users to substitute one of its Gemini LLMs for Google Assistant, but the system still calls Google Assistant for some tasks.) Alexa+ is the first major voice-assistant update that aims to take advantage of LLMs as well as emerging agentic technology and improved voice interactions, and the rollout is taking these capabilities to a large, existing user base. We\u2019re thinking: Rapid improvements in the voice stack are opening doors not only for voice assistants but for a galaxy of applications that rely on spoken input and output. Product designers will need to learn how to design smooth user voice experiences. Watching how Alexa+ manages them will provide useful guidelines.", "article_id": "68491ffd617fcb072014e79d", "linked_images": ["article_68491ffd617fcb072014e79d_img_0", "article_68491ffd617fcb072014e79d_img_1", "article_68491ffd617fcb072014e79d_img_2", "article_68491ffd617fcb072014e79d_img_3", "article_68491ffd617fcb072014e79d_img_4", "article_68491ffd617fcb072014e79d_img_5"]}, {"type": "text", "text": "manages them will provide useful guidelines.", "article_id": "68491ffd617fcb072014e79d", "linked_images": ["article_68491ffd617fcb072014e79d_img_0", "article_68491ffd617fcb072014e79d_img_1", "article_68491ffd617fcb072014e79d_img_2", "article_68491ffd617fcb072014e79d_img_3", "article_68491ffd617fcb072014e79d_img_4", "article_68491ffd617fcb072014e79d_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e79d", "linked_chunks": ["article_68491ffd617fcb072014e79d_chunk_0", "article_68491ffd617fcb072014e79d_chunk_1", "article_68491ffd617fcb072014e79d_chunk_2", "article_68491ffd617fcb072014e79d_chunk_3", "article_68491ffd617fcb072014e79d_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e79d", "linked_chunks": ["article_68491ffd617fcb072014e79d_chunk_0", "article_68491ffd617fcb072014e79d_chunk_1", "article_68491ffd617fcb072014e79d_chunk_2", "article_68491ffd617fcb072014e79d_chunk_3", "article_68491ffd617fcb072014e79d_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e79d", "linked_chunks": ["article_68491ffd617fcb072014e79d_chunk_0", "article_68491ffd617fcb072014e79d_chunk_1", "article_68491ffd617fcb072014e79d_chunk_2", "article_68491ffd617fcb072014e79d_chunk_3", "article_68491ffd617fcb072014e79d_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e79d", "linked_chunks": ["article_68491ffd617fcb072014e79d_chunk_0", "article_68491ffd617fcb072014e79d_chunk_1", "article_68491ffd617fcb072014e79d_chunk_2", "article_68491ffd617fcb072014e79d_chunk_3", "article_68491ffd617fcb072014e79d_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e79d", "linked_chunks": ["article_68491ffd617fcb072014e79d_chunk_0", "article_68491ffd617fcb072014e79d_chunk_1", "article_68491ffd617fcb072014e79d_chunk_2", "article_68491ffd617fcb072014e79d_chunk_3", "article_68491ffd617fcb072014e79d_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e79d", "linked_chunks": ["article_68491ffd617fcb072014e79d_chunk_0", "article_68491ffd617fcb072014e79d_chunk_1", "article_68491ffd617fcb072014e79d_chunk_2", "article_68491ffd617fcb072014e79d_chunk_3", "article_68491ffd617fcb072014e79d_chunk_4"]}, {"type": "text", "text": "Dear friends, The Voice Stack is improving rapidly. Systems that interact with users via speaking and listening will drive many new applications. Over the past year, I\u2019ve been working closely with DeepLearning.AI, AI Fund, and several collaborators on voice-based applications, and I will share best practices I\u2019ve learned in this and future letters. Foundation models that are trained to directly input, and often also directly generate, audio have contributed to this growth, but they are only part of the story. OpenAI\u2019s RealTime API makes it easy for developers to write prompts to develop systems that deliver voice-in, voice-out experiences. This is great for building quick-and-dirty prototypes, and it also works well for low-stakes conversations where making an occasional mistake is okay. I encourage you to try it! However, compared to text-based generation, it is still hard to control the output of voice-in voice-out models. In contrast to directly generating audio, when we use an LLM to generate text, we have many tools for building guardrails, and we can double-check the output before showing it to users. We can also use sophisticated agentic reasoning workflows to compute high-quality outputs. Before a customer-service agent shows a user the message, \u201cSure, I\u2019m happy to issue a refund,\u201d we can make sure that (i) issuing the refund is consistent with our business policy and (ii) we will call the API to issue the refund (and not just promise a refund without issuing it). In contrast, the tools to prevent a voice-in, voice-out model from making such mistakes are much less mature. In my experience, the reasoning capability of voice models also seems inferior to text-based models, and they give less sophisticated answers. (Perhaps this is because voice responses have to be more brief, leaving less room for chain-of-thought reasoning to get to a more thoughtful answer.) When building applications where I need a high degree of control over the output, I use agentic workflows to reason at length about the user\u2019s input. In voice applications, this means I end up using a pipeline that includes speech-to-text (STT, also known as ASR, or automatic speech recognition) to transcribe the user\u2019s words, then processes the text using one or more LLM calls, and finally returns an audio response to the user via TTS (text-to-speech). This STT \u2192 LLM/Agentic workflow \u2192 TTS pipeline, where the reasoning is done in text, allows for more accurate responses. However, this process introduces latency, and users of voice applications are very sensitive to latency. When DeepLearning.AI worked with RealAvatar (an AI Fund portfolio company led by Jeff Daniel) to build an avatar of me, we found that getting TTS to generate a voice that sounded like me was not very hard, but getting it to respond to questions using words similar to those I would choose was. Even after a year of tuning our system \u2014 starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows \u2014 it remains a work in progress. You can play with it here. Initially, this agentic workflow incurred 5-9 seconds of latency, and having", "article_id": "68491ffd617fcb072014e7a4", "linked_images": ["article_68491ffd617fcb072014e7a4_img_0", "article_68491ffd617fcb072014e7a4_img_1", "article_68491ffd617fcb072014e7a4_img_2", "article_68491ffd617fcb072014e7a4_img_3", "article_68491ffd617fcb072014e7a4_img_4", "article_68491ffd617fcb072014e7a4_img_5"]}, {"type": "text", "text": "I would choose was. Even after a year of tuning our system \u2014 starting with iterating on multiple, long, mega-prompts and eventually developing complex agentic workflows \u2014 it remains a work in progress. You can play with it here. Initially, this agentic workflow incurred 5-9 seconds of latency, and having users wait that long for responses led to a bad experience. To address this, we came up with the following latency reduction technique. The system quickly generates a pre-response (short for preliminary response) that can be uttered quickly, which buys time for an agentic workflow to generate a more thoughtful, full response. (We\u2019re grateful to LiveKit\u2019s CEO Russ d\u2019Sa and team for helping us get this working.) This is similar to how, if you were to ask me a complicated question, I might say \u201cHmm, let me think about that\u201d or \u201cSure, I can help with that\u201d \u2014 that\u2019s the pre-response \u2014 while thinking about what my full response might be. I think generating a pre-response followed by a full response, to quickly acknowledge the user\u2019s query and also reduce the perceived latency, will be an important technique, and I hope many teams will find this useful. Our goal was to approach human face-to-face conversational latency, which is around 0.3-1 seconds. RealAvatar and DeepLearning.AI, through our efforts on the pre-response and other optimizations, have reduced the system\u2019s latency to around 0.5-1 seconds. Months ago, sitting in a coffee shop, I was able to buy a phone number on Twilio and hook it up to an STT \u2192 LLM \u2192 TTS pipeline in just hours. This enabled me to talk to my own LLM using custom prompts. Prototyping voice applications is much easier than most people realize! Building reliable, scaled production applications takes longer, of course, but if you have a voice application in mind, I hope you\u2019ll start building prototypes and see how far you can get! I\u2019ll keep building voice applications and sharing best practices and voice-related technology trends in future letters. Keep building! Andrew AI coding agents do more than autocomplete. They help you debug, refactor, and design applications. Learn how coding agents work under the hood, so you can streamline your projects and build applications such as a Wikipedia data-analysis app! Enroll Now. To date, efforts to decode what people are thinking from their brain waves often relied on electrodes implanted in the cortex. New work used devices outside the head to pick up brain signals that enabled an AI system, as a subject typed, to accurately guess what they were typing. What\u2019s new: Researchers presented Brain2Qwerty, a non-invasive method to translate brain waves into text. In addition, their work shed light on how the brain processes language. The team included people at Meta, Paris Sciences et Lettres University, Hospital Foundation Adolphe de Rothschild, Basque Center on Cognition, Brain and Language, Basque Foundation for Science, Aix-Marseille University, and Paris Cit\u00e9 University. Gathering brainwave data: The authors recorded the brain activity of 35 healthy participants who typed Spanish-language sentences. The participants were connected to either an electroencephalogram (EEG), which records", "article_id": "68491ffd617fcb072014e7a4", "linked_images": ["article_68491ffd617fcb072014e7a4_img_0", "article_68491ffd617fcb072014e7a4_img_1", "article_68491ffd617fcb072014e7a4_img_2", "article_68491ffd617fcb072014e7a4_img_3", "article_68491ffd617fcb072014e7a4_img_4", "article_68491ffd617fcb072014e7a4_img_5"]}, {"type": "text", "text": "Hospital Foundation Adolphe de Rothschild, Basque Center on Cognition, Brain and Language, Basque Foundation for Science, Aix-Marseille University, and Paris Cit\u00e9 University. Gathering brainwave data: The authors recorded the brain activity of 35 healthy participants who typed Spanish-language sentences. The participants were connected to either an electroencephalogram (EEG), which records the brain\u2019s electrical activity via electrodes on the scalp, or a magnetoencephalogram (MEG), which records magnetic activity through a device that surrounds the head but isn\u2019t attached. 15 participants used each device and five used both. Thoughts into text: Brain2Qwerty used a system made up of a convolutional neural network, transformer, and a 9-gram character-level language model pretrained on Spanish Wikipedia. The system classified the text a user typed from their brain activity. The authors trained separate systems on MEG and EEG data. Results. The authors\u2019 MEG model achieved 32 percent character error rate (CER), much higher accuracy than the EEG competitors. Their EEG system outperformed EEGNet, a model designed to process EEG data that had been trained on the authors\u2019 EEG data. It achieved 67 percent CER, while EEGNet achieved 78 percent CER. Behind the news: For decades, researchers have used learning algorithms to interpret various aspects of brain activity with varying degrees of success. In recent years, they\u2019ve used neural networks to generate text and speech from implanted electrodes, generate images of what people see while in an fMRI, and enable people to control robots using EEG signals. Why it matters: In research into interpreting brain signals, subjects who are outfitted with surgical implants typically have supplied the highest-quality brain signals. fMRI scans, while similarly noninvasive, are less precise temporally, which makes them less useful for monitoring or predicting language production. Effective systems based on MEG, which can tap brain signals precisely without requiring participants to undergo surgery, open the door to collecting far more data, training far more robust models, and conducting a wider variety of experiments. We\u2019re thinking: The privacy implications of such research may be troubling, but keep in mind that Brain2Qwerty\u2019s MEG system, which was the most effective approach tested, required patients to spend extended periods of time sitting still in a shielded room. We aren\u2019t going to read minds in the wild anytime soon. Top AI companies announced plans to dramatically ramp up their spending on AI infrastructure. What\u2019s new: Alphabet, Amazon, Meta, Microsoft, and others will boost their capital spending dramatically in 2025, pouring hundreds of billions of dollars into data centers where they process AI training, the companies said in their most recent quarterly reports. The surge suggests that more-efficient approaches to training models won\u2019t dampen the need for greater and greater processing power. How it works: Capital expenditures include long-term purchases like land, buildings, and computing hardware rather than recurring costs like salaries or electricity. The AI leaders signaled that most of this spending will support their AI efforts. Behind the news: DeepSeek initially surprised many members of the AI community by claiming to have trained a high-performance large language model at a fraction of the usual cost. Why it matters: DeepSeek-R1\u2019s", "article_id": "68491ffd617fcb072014e7a4", "linked_images": ["article_68491ffd617fcb072014e7a4_img_0", "article_68491ffd617fcb072014e7a4_img_1", "article_68491ffd617fcb072014e7a4_img_2", "article_68491ffd617fcb072014e7a4_img_3", "article_68491ffd617fcb072014e7a4_img_4", "article_68491ffd617fcb072014e7a4_img_5"]}, {"type": "text", "text": "salaries or electricity. The AI leaders signaled that most of this spending will support their AI efforts. Behind the news: DeepSeek initially surprised many members of the AI community by claiming to have trained a high-performance large language model at a fraction of the usual cost. Why it matters: DeepSeek-R1\u2019s purported training cost fueled fears that demand for AI infrastructure would cool, but the top AI companies\u2019 plans show that it\u2019s not happening yet. A possible explanation lies in the Jevons Paradox, a 19th-century economic theory named after the English economist William Stanley Jevons. As a valuable product becomes more affordable, demand doesn\u2019t fall, it rises. According to this theory, even if training costs tumble, the world will demand ever greater processing power for inference. We\u2019re thinking: DeepSeek\u2019s low-cost technology momentarily rattled investors who had expected the next big gains would come from the U.S. rather than China. But DeepSeek\u2019s efficiency follows a broader pattern we\u2019ve seen for years: The AI community steadily wrings better performance from less processing power. A viral deepfake video showed media superstars who appeared to support a cause \u2014 but it was made without their participation or permission. What\u2019s new: The video shows AI-generated likenesses of 20 Jewish celebrities ranging from Scarlett Johansson to Simon & Garfunkel. They appear wearing T-shirts that feature a middle finger inscribed with the Star of David above the word \u201cKANYE.\u201d The clip, which ends with the words \u201cEnough is enough\u201d followed by \u201cJoin the fight against antisemitism,\u201d responds to rapper Kanye West, who sold T-shirts emblazoned with swastikas on Shopify before the ecommerce platform shut down his store. Who created it: Israeli developers Guy Bar and Ori Bejerano generated the video to spark a conversation about antisemitism, Bar told The Jerusalem Post. The team didn\u2019t reveal the AI models, editing tools, or techniques used to produce the video. Johansson reacts: Scarlett Johansson denounced the clip and urged the U.S. to regulate deepfakes. In 2024, she objected to one of the voices of OpenAI\u2019s voice assistant, which she claimed resembled her own voice, leading the company to remove that voice from its service. The prior year, her attorneys ordered a company to stop using an unauthorized AI-generated version of her image in an advertisement. Likenesses up for grabs: Existing U.S. laws protect some uses of a celebrity\u2019s likeness in the form of a photo, drawing, or human lookalike, but they don\u2019t explicitly protect against reproduction by AI systems. This leaves celebrities and public figures with limited recourse against unauthorized deepfakes. Why it matters: Non-consensual deepfake pornography is widely condemned, but AI enables many other non-consensual uses of someone\u2019s likeness, and their limits are not yet consistently coded into law. If the creators of the video that appropriated the images of celebrities had responded to Johansson\u2019s criticism with an AI-generated satire, would that be a legitimate exercise of free speech or another misuse of AI? Previously, an ambiguous legal framework may have been acceptable because such images, and thus lawsuits arising from them, were uncommon. Now, as synthetic likenesses of specific people become", "article_id": "68491ffd617fcb072014e7a4", "linked_images": ["article_68491ffd617fcb072014e7a4_img_0", "article_68491ffd617fcb072014e7a4_img_1", "article_68491ffd617fcb072014e7a4_img_2", "article_68491ffd617fcb072014e7a4_img_3", "article_68491ffd617fcb072014e7a4_img_4", "article_68491ffd617fcb072014e7a4_img_5"]}, {"type": "text", "text": "responded to Johansson\u2019s criticism with an AI-generated satire, would that be a legitimate exercise of free speech or another misuse of AI? Previously, an ambiguous legal framework may have been acceptable because such images, and thus lawsuits arising from them, were uncommon. Now, as synthetic likenesses of specific people become easier to generate, clear legal boundaries are needed to keep misuses in check. We\u2019re thinking: Creating unauthorized lookalikes of existing people is not a good way to advance any cause, however worthy. Developers should work with businesses policymakers to establish standards that differentiate legitimate uses from unfair or misleading exploitation. Although large language models can improve their performance by generating a chain of thought (CoT) \u2014 intermediate text tokens that break down the process of responding to a prompt into a series of steps \u2014 much of the CoT text is aimed at maintaining fluency (such as \u201ca\u201d, \u201cof\u201d, \u201cwe know that\u201d) rather than reasoning (\u201ca\u00b2 + b\u00b2 = c\u00b2\u201d). Researchers addressed this inefficiency. What\u2019s new: Shibo Hao, Sainbayar Sukhbaatar, and colleagues at Meta and University of California San Diego introduced Coconut (Chain of Continuous Thought), a method that trains large language models (LLMs) to process chains of thought as vectors rather than words. Key insight: A large language model (LLM) can be broken into an embedding layer, transformer, and classification layer. To generate the next text token from input text, the embedding layer embeds the text; given the text, the transformer outputs a hidden vector; and the classification layer maps the vector to text-token probabilities. Based on these probabilities, a decoding algorithm selects the next token to generate, which feeds back into the input text sequence to generate the next vector, and so on. When a model generates a CoT, committing to a specific word at each step limits the information available to the meanings of the words generated so far, while a vector could represent multiple possible words. Using vectors instead of text enables the CoT to encode richer information. How it works: The authors built three LLMs by fine-tuning a pre-trained GPT-2 on three datasets of prompts, CoTs, and final outputs: GSM8k (grade-school math word problems); ProntoQA (questions and answers about fictional concepts expressed in made-up words, including synthetic CoTs in natural language); and (3) ProsQA, a more challenging question-answering dataset introduced by the authors, inspired by ProntoQA but with longer reasoning steps. Results: The authors compared their method to a pretrained GPT-2 that was fine-tuned on the same datasets to predict the next word, including reasoning. Yes, but: On GSM8k, Coconut achieved 34.1 percent accuracy, while the baseline LLM achieved 42.9 percent. However, it generated significantly fewer vectors and tokens than the CoT generated tokens. Coconut generated 8.2 vectors on average compared to the baseline LLM\u2019s 25 text tokens. Why it matters: A traditional CoT commits to a single word at each step and thus encodes one reasoning path in a single CoT. Vectors are less interpretable to humans than language, but the model\u2019s output layer can still decode the thought vectors into probabilities over tokens. Further, inspecting", "article_id": "68491ffd617fcb072014e7a4", "linked_images": ["article_68491ffd617fcb072014e7a4_img_0", "article_68491ffd617fcb072014e7a4_img_1", "article_68491ffd617fcb072014e7a4_img_2", "article_68491ffd617fcb072014e7a4_img_3", "article_68491ffd617fcb072014e7a4_img_4", "article_68491ffd617fcb072014e7a4_img_5"]}, {"type": "text", "text": "tokens. Why it matters: A traditional CoT commits to a single word at each step and thus encodes one reasoning path in a single CoT. Vectors are less interpretable to humans than language, but the model\u2019s output layer can still decode the thought vectors into probabilities over tokens. Further, inspecting the distribution of words stored along all continuous CoT vectors offers a way to understand multiple potential thought paths stored in one continuous CoT. We\u2019re thinking: LLMs typically learn to reason over text, mainly because text data is widely available to train on. In contrast, neuroscience shows that the part of the human brain responsible for language largely goes quiet during reasoning tasks, which suggests that explicit language is not a key mechanism for reasoning. Coconut takes an intriguing step to enable LLMs to explore representations that don\u2019t encode the limitations of language.", "article_id": "68491ffd617fcb072014e7a4", "linked_images": ["article_68491ffd617fcb072014e7a4_img_0", "article_68491ffd617fcb072014e7a4_img_1", "article_68491ffd617fcb072014e7a4_img_2", "article_68491ffd617fcb072014e7a4_img_3", "article_68491ffd617fcb072014e7a4_img_4", "article_68491ffd617fcb072014e7a4_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7a4", "linked_chunks": ["article_68491ffd617fcb072014e7a4_chunk_0", "article_68491ffd617fcb072014e7a4_chunk_1", "article_68491ffd617fcb072014e7a4_chunk_2", "article_68491ffd617fcb072014e7a4_chunk_3", "article_68491ffd617fcb072014e7a4_chunk_4", "article_68491ffd617fcb072014e7a4_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7a4", "linked_chunks": ["article_68491ffd617fcb072014e7a4_chunk_0", "article_68491ffd617fcb072014e7a4_chunk_1", "article_68491ffd617fcb072014e7a4_chunk_2", "article_68491ffd617fcb072014e7a4_chunk_3", "article_68491ffd617fcb072014e7a4_chunk_4", "article_68491ffd617fcb072014e7a4_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7a4", "linked_chunks": ["article_68491ffd617fcb072014e7a4_chunk_0", "article_68491ffd617fcb072014e7a4_chunk_1", "article_68491ffd617fcb072014e7a4_chunk_2", "article_68491ffd617fcb072014e7a4_chunk_3", "article_68491ffd617fcb072014e7a4_chunk_4", "article_68491ffd617fcb072014e7a4_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7a4", "linked_chunks": ["article_68491ffd617fcb072014e7a4_chunk_0", "article_68491ffd617fcb072014e7a4_chunk_1", "article_68491ffd617fcb072014e7a4_chunk_2", "article_68491ffd617fcb072014e7a4_chunk_3", "article_68491ffd617fcb072014e7a4_chunk_4", "article_68491ffd617fcb072014e7a4_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7a4", "linked_chunks": ["article_68491ffd617fcb072014e7a4_chunk_0", "article_68491ffd617fcb072014e7a4_chunk_1", "article_68491ffd617fcb072014e7a4_chunk_2", "article_68491ffd617fcb072014e7a4_chunk_3", "article_68491ffd617fcb072014e7a4_chunk_4", "article_68491ffd617fcb072014e7a4_chunk_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7a4", "linked_chunks": ["article_68491ffd617fcb072014e7a4_chunk_0", "article_68491ffd617fcb072014e7a4_chunk_1", "article_68491ffd617fcb072014e7a4_chunk_2", "article_68491ffd617fcb072014e7a4_chunk_3", "article_68491ffd617fcb072014e7a4_chunk_4", "article_68491ffd617fcb072014e7a4_chunk_5"]}, {"type": "text", "text": "Dear friends, Last month, a drone from Skyfire AI was credited with saving a police officer\u2019s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened. Skyfire AI, an AI Fund portfolio company led by CEO Don Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers\u2019 time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond. In January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located. From the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road \u2014 they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf\u2019s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find them. From the aerial footage, it appeared that the officer still had his radio, but was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation. Fortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes. The officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we\u2019d probably saved the officer\u2019s life. Democratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we\u2019re making. In the aftermath of Hurricane Helene last year, Skyfire AI\u2019s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management,", "article_id": "68491ffd617fcb072014e7ab", "linked_images": ["article_68491ffd617fcb072014e7ab_img_0", "article_68491ffd617fcb072014e7ab_img_1", "article_68491ffd617fcb072014e7ab_img_2", "article_68491ffd617fcb072014e7ab_img_3", "article_68491ffd617fcb072014e7ab_img_4", "article_68491ffd617fcb072014e7ab_img_5"]}, {"type": "text", "text": "must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we\u2019re making. In the aftermath of Hurricane Helene last year, Skyfire AI\u2019s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives. It\u2019s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these. Keep building! Andrew Learn to systematically evaluate, improve, and iterate on AI agents using structured assessments. In our short course \u201cEvaluating AI Agents,\u201d you\u2019ll learn to add observability, choose the right evaluation methods, and run structured experiments to improve AI agent performance. Enroll for free xAI\u2019s new model family suggests that devoting more computation to training remains a viable path to building more capable AI. What\u2019s new: Elon Musk\u2019s xAI published a video demonstration of Grok 3, a family of four large language models that includes reasoning and non-reasoning versions as well as full- and reduced-size models. Grok 3 is available to subscribers to X\u2019s Premium+ ($40 monthly for users in the United States; the price varies by country) and will be part of a new subscription service called SuperGrok. The models currently take text input and produce text output, but the company plans to integrate audio input and output in coming weeks. How it works: xAI has not yet disclosed details about Grok 3\u2019s architecture, parameter counts, training datasets, or training methods. Here\u2019s what we know so far: Results: The Grok 3 family outperformed leading models in math (AIME 2024), science (GPQA), and coding (LiveCodeBench). Behind the news: Reasoning models are pushing benchmark scores steadily upward, especially in challenging areas like math and coding. Grok 3, with its ability to reason over prompts, search the web, and compile detailed reports, arrives hot on the heels of OpenAI\u2019s Deep Research and o3-mini and Google\u2019s Gemini-2 Flash Thinking, which offer similar capabilities. Why it matters: Grok 3 is a substantial achievement \u2014 especially for a company that\u2019s less than two years old \u2014 and it pushes the state of the art forward by ample margins. But its significance may go farther. Research into scaling laws indicates that model performance scales with training. While xAI has not disclosed the amount of processing used to train Grok 3, the number of GPUs in its cluster suggests that the company applied a massive amount. We\u2019re thinking: Grok 3\u2019s performance makes a case for both massive compute in pretraining and additional compute at inference. Running in its usual mode, Grok 3 mini Reasoning outperformed OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin. Replit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order. What\u2019s new:", "article_id": "68491ffd617fcb072014e7ab", "linked_images": ["article_68491ffd617fcb072014e7ab_img_0", "article_68491ffd617fcb072014e7ab_img_1", "article_68491ffd617fcb072014e7ab_img_2", "article_68491ffd617fcb072014e7ab_img_3", "article_68491ffd617fcb072014e7ab_img_4", "article_68491ffd617fcb072014e7ab_img_5"]}, {"type": "text", "text": "OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin. Replit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order. What\u2019s new: Replit\u2019s app, which previously generated simple Python programs, now generates iOS and Android apps and app templates that can be shared publicly. Mobile and web access to Replit\u2019s in-house code generation models is free for up to three public applications. A Core plan ($25 per month, $180 per year) buys unlimited access and applications, code generation by Claude 3.5 Sonnet and OpenAI GPT-4o, and monthly credits for generated checkpoints. How it works: The app and web tools are powered by Replit Agent, an AI coding assistant designed to help users write, debug, and deploy applications with little manual setup. Replit Agent is based on Claude 3.5 Sonnet and calls other specialized models. The agent framework is built on LangChain\u2019s LangGraph. It breaks down development tasks into steps to be handled by specialized sub-agents. Behind the news: The incorporation of Replit Agent to Replit\u2019s mobile app is a significant step for AI-driven IDEs. Competitors like Aider and Windsurf don\u2019t offer mobile apps, and mobile apps from Cursor and Github provide chat but not mobile app development. Moreover, few coding agents can deploy apps to the cloud on the desktop or mobile. Why it matters: Replit\u2019s new mobile app produces working apps in minutes (although some early users have reported encountering bugs), and automatic deployment of apps to the cloud is a huge help. Yet it raises the stakes for developers to learn their craft and maintain a collaborative relationship with AI. While Replit\u2019s web-based environment exposes the code, encouraging users to improve their skills, the mobile app hides much of its work below the surface. It brings AI closer to handling full software development cycles and adds urgency to questions about how to address the balance between automation and hands-on coding. We\u2019re thinking: AI continues to boost developer productivity and reduce the cost of software development, and the progress of Bolt, Cursor, Replit, Vercel, Windsurf, and others is exhilarating. We look forward to a day when, measured against the 2024 standard, every software engineer is a 10x engineer! Elon Musk and a group of investors made an unsolicited bid to buy the assets of the nonprofit that controls OpenAI, complicating the AI powerhouse\u2019s future plans. What\u2019s new: Musk submitted a $97.4 billion offer to acquire the assets of the nonprofit OpenAI Inc. CEO Sam Altman and the company\u2019s board of directors swiftly rejected it, and Altman publicly mocked Musk by offering to buy Twitter for $9.74 billion (one-tenth of Musk\u2019s bid and less than one-quarter the price he paid for the social network). OpenAI\u2019s board reaffirmed its control over the company\u2019s direction, signaling that it does not intend to cede governance to outside investors. How it works: OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an", "article_id": "68491ffd617fcb072014e7ab", "linked_images": ["article_68491ffd617fcb072014e7ab_img_0", "article_68491ffd617fcb072014e7ab_img_1", "article_68491ffd617fcb072014e7ab_img_2", "article_68491ffd617fcb072014e7ab_img_3", "article_68491ffd617fcb072014e7ab_img_4", "article_68491ffd617fcb072014e7ab_img_5"]}, {"type": "text", "text": "than one-quarter the price he paid for the social network). OpenAI\u2019s board reaffirmed its control over the company\u2019s direction, signaling that it does not intend to cede governance to outside investors. How it works: OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an unusual structure in which the nonprofit board controls the for-profit entity that develops and commercializes AI models. This setup allows the board to maintain the company\u2019s original mission \u2014 developing AI for the benefit of humanity \u2014 rather than solely maximizing shareholder value. However, driven by the need for massive investments in infrastructure and talent, OpenAI is considering a new for-profit structure that would allow external investors to own more of the company. The high offer by Musk \u2014 who, as CEO of xAI, competes with OpenAI \u2014 could interfere with that plan. Behind the news: Musk was one of OpenAI\u2019s earliest investors, but he departed in 2018 after disagreements over direction and control of the organization. His bid follows a lawsuit against OpenAI, in which he claims the company abandoned its nonprofit mission in favor of profit. OpenAI said that Musk\u2019s bid contradicts his legal claims and suggests that the lawsuit should be dismissed. Since then, Musk has stated that he would drop the lawsuit if OpenAI remains a nonprofit. Why it matters: OpenAI is a premier AI company, and its activities affect virtually everyone in the field by supplying tools, technology, or inspiration. Musk\u2019s xAI is a direct competitor, and his bid, whether it\u2019s sincere or tactical, unsettles OpenAI\u2019s plans. Even if OpenAI moves forward as planned, Musk\u2019s actions likely will have made the process more expensive and potentially invite closer scrutiny of the company\u2019s actions. We\u2019re thinking: There\u2019s ample precedence for non-profits spinning out for-profit entities. For example, non-profit universities typically create intellectual property that forms the basis of for-profit startups. The university might retain a modest stake, and this is viewed as consistent with its non-profit mission. This isn\u2019t a perfect analogy, since OpenAI does little besides operating its AI business, but we hope the company finds a path forward that allows it to serve users, rewards its employees for their contributions, and honors its non-profit charter. The latest international AI summit exposed deep divisions between major world powers regarding AI regulations. What\u2019s new: While previous summits emphasized existential risks, the AI Action Summit in Paris marked a turning point. France and the European Union shifted away from strict regulatory measures and toward investment to compete with the United States and China. However, global consensus remained elusive: the U.S. and the United Kingdom refused to sign key agreements on global governance, military AI, and algorithmic bias. The U.S. in particular pushed back against global AI regulation, arguing that excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns. How it works: Participating countries considered three policy statements that address AI\u2019s impact on society, labor, and security. The first statement calls on each country to enact AI policies that would support economic development,", "article_id": "68491ffd617fcb072014e7ab", "linked_images": ["article_68491ffd617fcb072014e7ab_img_0", "article_68491ffd617fcb072014e7ab_img_1", "article_68491ffd617fcb072014e7ab_img_2", "article_68491ffd617fcb072014e7ab_img_3", "article_68491ffd617fcb072014e7ab_img_4", "article_68491ffd617fcb072014e7ab_img_5"]}, {"type": "text", "text": "excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns. How it works: Participating countries considered three policy statements that address AI\u2019s impact on society, labor, and security. The first statement calls on each country to enact AI policies that would support economic development, environmental responsibility, and equitable access to technology. The second encourages safeguards to ensure that companies and nations distribute AI productivity gains fairly, protect workers\u2019 rights, and prevent bias in hiring and management systems. The third advocates for restrictions on fully autonomous military systems and affirms the need for human oversight in warfare. Behind the news: The Paris summit follows previous gatherings of world leaders to discuss AI, including the initial AI Safety Summit at Bletchley Park and the AI Seoul Summit and AI Global Forum. At these summits, governments and companies agreed broadly to address AI risks but avoided binding regulations. Nonetheless, divisions over AI governance have widened in the wake of rising geopolitical competition and the emergence of high-performance open weights models like DeepSeek-R1. Why it matters: The Paris summit marks a major shift in global AI policy. The EU, once an ardent proponent of AI regulation, backed away from its strictest proposals. At the same time, doomsayers have lost influence, and officials are turning their attention to immediate concerns like economic growth, security, misuse, and bias. These moves make way for AI to do great good in the world, even as they contribute to uncertainty about how AI will be governed. We\u2019re thinking: Governments are shifting their focus away from unrealistic risks and toward practical strategies for guiding AI development. We look forward to clear policies that encourage innovation while addressing real-world challenges.", "article_id": "68491ffd617fcb072014e7ab", "linked_images": ["article_68491ffd617fcb072014e7ab_img_0", "article_68491ffd617fcb072014e7ab_img_1", "article_68491ffd617fcb072014e7ab_img_2", "article_68491ffd617fcb072014e7ab_img_3", "article_68491ffd617fcb072014e7ab_img_4", "article_68491ffd617fcb072014e7ab_img_5"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7ab", "linked_chunks": ["article_68491ffd617fcb072014e7ab_chunk_0", "article_68491ffd617fcb072014e7ab_chunk_1", "article_68491ffd617fcb072014e7ab_chunk_2", "article_68491ffd617fcb072014e7ab_chunk_3", "article_68491ffd617fcb072014e7ab_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7ab", "linked_chunks": ["article_68491ffd617fcb072014e7ab_chunk_0", "article_68491ffd617fcb072014e7ab_chunk_1", "article_68491ffd617fcb072014e7ab_chunk_2", "article_68491ffd617fcb072014e7ab_chunk_3", "article_68491ffd617fcb072014e7ab_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7ab", "linked_chunks": ["article_68491ffd617fcb072014e7ab_chunk_0", "article_68491ffd617fcb072014e7ab_chunk_1", "article_68491ffd617fcb072014e7ab_chunk_2", "article_68491ffd617fcb072014e7ab_chunk_3", "article_68491ffd617fcb072014e7ab_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7ab", "linked_chunks": ["article_68491ffd617fcb072014e7ab_chunk_0", "article_68491ffd617fcb072014e7ab_chunk_1", "article_68491ffd617fcb072014e7ab_chunk_2", "article_68491ffd617fcb072014e7ab_chunk_3", "article_68491ffd617fcb072014e7ab_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7ab", "linked_chunks": ["article_68491ffd617fcb072014e7ab_chunk_0", "article_68491ffd617fcb072014e7ab_chunk_1", "article_68491ffd617fcb072014e7ab_chunk_2", "article_68491ffd617fcb072014e7ab_chunk_3", "article_68491ffd617fcb072014e7ab_chunk_4"]}, {"type": "image", "article_id": "68491ffd617fcb072014e7ab", "linked_chunks": ["article_68491ffd617fcb072014e7ab_chunk_0", "article_68491ffd617fcb072014e7ab_chunk_1", "article_68491ffd617fcb072014e7ab_chunk_2", "article_68491ffd617fcb072014e7ab_chunk_3", "article_68491ffd617fcb072014e7ab_chunk_4"]}]