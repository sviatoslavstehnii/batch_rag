[{"type": "text", "text": "Dear friends, Last month, a drone from Skyfire AI was credited with saving a police officer\u2019s life after a dramatic 2 a.m. traffic stop. Many statistics show that AI impacts billions of lives, but sometimes a story still hits me emotionally. Let me share what happened. Skyfire AI, an AI Fund portfolio company led by CEO Don Mathis, operates a public safety program in which drones function as first responders to 911 calls. Particularly when a police department is personnel-constrained, drones can save officers\u2019 time while enhancing their situational awareness. For example, many burglar alarms are false alarms, maybe set off by moisture or an animal. Rather than sending a patrol officer to drive over to discover this, a drone can get there faster and determine if an officer is required at all. If the alarm is real, the drone can help officers understand the situation, the locations of any perpetrators, and how best to respond. In January, a Skyfire AI drone was returning to base after responding to a false alarm when the police dispatcher asked us to reroute it to help locate a patrol officer. The officer had radioed a few minutes earlier that he had pulled over a suspicious vehicle and had not been heard from since. The officer had stopped where two major highways intersect in a complex cloverleaf, and dispatch was unsure exactly where they were located. From the air, the drone rapidly located the officer and the driver of the vehicle he had pulled over, who it turned out had escaped from a local detention facility. Neither would have been visible from the road \u2014 they were fighting in a drainage ditch below the highway. Because of the complexity of the cloverleaf\u2019s geometry, the watch officer (who coordinates police activities for the shift) later estimated it would have taken 5-7 minutes for an officer in a patrol car to find them. From the aerial footage, it appeared that the officer still had his radio, but was losing the fight and unable to reach it to call for help. Further, it looked like the assailant might gain control of his service weapon and use it against him. This was a dire and dangerous situation. Fortunately, because the drone had pinpointed the location of the officer and his assailant, dispatch was able to direct additional units to assist. The first arrived not in 5-7 minutes but in 45 seconds. Four more units arrived within minutes. The officers were able to take control of the situation and apprehend the driver, resulting in an arrest and, more important, a safe outcome for the officer. Subsequently, the watch officer said we\u2019d probably saved the officer\u2019s life. Democratic nations still have a lot of work to do on drone technology, and we must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we\u2019re making. In the aftermath of Hurricane Helene last year, Skyfire AI\u2019s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management,", "article_id": "68497b48235d602d65122547", "linked_images": ["article_68497b48235d602d65122547_img_0", "article_68497b48235d602d65122547_img_1", "article_68497b48235d602d65122547_img_2", "article_68497b48235d602d65122547_img_3", "article_68497b48235d602d65122547_img_4", "article_68497b48235d602d65122547_img_5"]}, {"type": "text", "text": "must build this technology with guardrails to make sure we enhance civil liberties and human rights. But I am encouraged by the progress we\u2019re making. In the aftermath of Hurricane Helene last year, Skyfire AI\u2019s drones supported search-and-rescue operations under the direction of the North Carolina Office of Emergency Management, responding to specific requests to help locate missing persons and direct rescue assets (e.g., helicopters and boats) to their location, and was credited with saving 13 lives. It\u2019s not every day that AI directly saves someone's life. But as our technology advances, I think there will be more and more stories like these. Keep building! Andrew Learn to systematically evaluate, improve, and iterate on AI agents using structured assessments. In our short course \u201cEvaluating AI Agents,\u201d you\u2019ll learn to add observability, choose the right evaluation methods, and run structured experiments to improve AI agent performance. Enroll for free xAI\u2019s new model family suggests that devoting more computation to training remains a viable path to building more capable AI. What\u2019s new: Elon Musk\u2019s xAI published a video demonstration of Grok 3, a family of four large language models that includes reasoning and non-reasoning versions as well as full- and reduced-size models. Grok 3 is available to subscribers to X\u2019s Premium+ ($40 monthly for users in the United States; the price varies by country) and will be part of a new subscription service called SuperGrok. The models currently take text input and produce text output, but the company plans to integrate audio input and output in coming weeks. How it works: xAI has not yet disclosed details about Grok 3\u2019s architecture, parameter counts, training datasets, or training methods. Here\u2019s what we know so far: Results: The Grok 3 family outperformed leading models in math (AIME 2024), science (GPQA), and coding (LiveCodeBench). Behind the news: Reasoning models are pushing benchmark scores steadily upward, especially in challenging areas like math and coding. Grok 3, with its ability to reason over prompts, search the web, and compile detailed reports, arrives hot on the heels of OpenAI\u2019s Deep Research and o3-mini and Google\u2019s Gemini-2 Flash Thinking, which offer similar capabilities. Why it matters: Grok 3 is a substantial achievement \u2014 especially for a company that\u2019s less than two years old \u2014 and it pushes the state of the art forward by ample margins. But its significance may go farther. Research into scaling laws indicates that model performance scales with training. While xAI has not disclosed the amount of processing used to train Grok 3, the number of GPUs in its cluster suggests that the company applied a massive amount. We\u2019re thinking: Grok 3\u2019s performance makes a case for both massive compute in pretraining and additional compute at inference. Running in its usual mode, Grok 3 mini Reasoning outperformed OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin. Replit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order. What\u2019s new:", "article_id": "68497b48235d602d65122547", "linked_images": ["article_68497b48235d602d65122547_img_0", "article_68497b48235d602d65122547_img_1", "article_68497b48235d602d65122547_img_2", "article_68497b48235d602d65122547_img_3", "article_68497b48235d602d65122547_img_4", "article_68497b48235d602d65122547_img_5"]}, {"type": "text", "text": "OpenAI o3-mini set at high effort on AIME 2024, GPQA, and LiveCodeBench. With an unspecified amount of additional compute, its performance on those benchmarks shot further upward by a substantial margin. Replit, an AI-driven integrated development environment, updated its mobile app to generate further mobile apps to order. What\u2019s new: Replit\u2019s app, which previously generated simple Python programs, now generates iOS and Android apps and app templates that can be shared publicly. Mobile and web access to Replit\u2019s in-house code generation models is free for up to three public applications. A Core plan ($25 per month, $180 per year) buys unlimited access and applications, code generation by Claude 3.5 Sonnet and OpenAI GPT-4o, and monthly credits for generated checkpoints. How it works: The app and web tools are powered by Replit Agent, an AI coding assistant designed to help users write, debug, and deploy applications with little manual setup. Replit Agent is based on Claude 3.5 Sonnet and calls other specialized models. The agent framework is built on LangChain\u2019s LangGraph. It breaks down development tasks into steps to be handled by specialized sub-agents. Behind the news: The incorporation of Replit Agent to Replit\u2019s mobile app is a significant step for AI-driven IDEs. Competitors like Aider and Windsurf don\u2019t offer mobile apps, and mobile apps from Cursor and Github provide chat but not mobile app development. Moreover, few coding agents can deploy apps to the cloud on the desktop or mobile. Why it matters: Replit\u2019s new mobile app produces working apps in minutes (although some early users have reported encountering bugs), and automatic deployment of apps to the cloud is a huge help. Yet it raises the stakes for developers to learn their craft and maintain a collaborative relationship with AI. While Replit\u2019s web-based environment exposes the code, encouraging users to improve their skills, the mobile app hides much of its work below the surface. It brings AI closer to handling full software development cycles and adds urgency to questions about how to address the balance between automation and hands-on coding. We\u2019re thinking: AI continues to boost developer productivity and reduce the cost of software development, and the progress of Bolt, Cursor, Replit, Vercel, Windsurf, and others is exhilarating. We look forward to a day when, measured against the 2024 standard, every software engineer is a 10x engineer! Elon Musk and a group of investors made an unsolicited bid to buy the assets of the nonprofit that controls OpenAI, complicating the AI powerhouse\u2019s future plans. What\u2019s new: Musk submitted a $97.4 billion offer to acquire the assets of the nonprofit OpenAI Inc. CEO Sam Altman and the company\u2019s board of directors swiftly rejected it, and Altman publicly mocked Musk by offering to buy Twitter for $9.74 billion (one-tenth of Musk\u2019s bid and less than one-quarter the price he paid for the social network). OpenAI\u2019s board reaffirmed its control over the company\u2019s direction, signaling that it does not intend to cede governance to outside investors. How it works: OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an", "article_id": "68497b48235d602d65122547", "linked_images": ["article_68497b48235d602d65122547_img_0", "article_68497b48235d602d65122547_img_1", "article_68497b48235d602d65122547_img_2", "article_68497b48235d602d65122547_img_3", "article_68497b48235d602d65122547_img_4", "article_68497b48235d602d65122547_img_5"]}, {"type": "text", "text": "than one-quarter the price he paid for the social network). OpenAI\u2019s board reaffirmed its control over the company\u2019s direction, signaling that it does not intend to cede governance to outside investors. How it works: OpenAI was founded as a nonprofit in 2015, but since 2019 it has operated under an unusual structure in which the nonprofit board controls the for-profit entity that develops and commercializes AI models. This setup allows the board to maintain the company\u2019s original mission \u2014 developing AI for the benefit of humanity \u2014 rather than solely maximizing shareholder value. However, driven by the need for massive investments in infrastructure and talent, OpenAI is considering a new for-profit structure that would allow external investors to own more of the company. The high offer by Musk \u2014 who, as CEO of xAI, competes with OpenAI \u2014 could interfere with that plan. Behind the news: Musk was one of OpenAI\u2019s earliest investors, but he departed in 2018 after disagreements over direction and control of the organization. His bid follows a lawsuit against OpenAI, in which he claims the company abandoned its nonprofit mission in favor of profit. OpenAI said that Musk\u2019s bid contradicts his legal claims and suggests that the lawsuit should be dismissed. Since then, Musk has stated that he would drop the lawsuit if OpenAI remains a nonprofit. Why it matters: OpenAI is a premier AI company, and its activities affect virtually everyone in the field by supplying tools, technology, or inspiration. Musk\u2019s xAI is a direct competitor, and his bid, whether it\u2019s sincere or tactical, unsettles OpenAI\u2019s plans. Even if OpenAI moves forward as planned, Musk\u2019s actions likely will have made the process more expensive and potentially invite closer scrutiny of the company\u2019s actions. We\u2019re thinking: There\u2019s ample precedence for non-profits spinning out for-profit entities. For example, non-profit universities typically create intellectual property that forms the basis of for-profit startups. The university might retain a modest stake, and this is viewed as consistent with its non-profit mission. This isn\u2019t a perfect analogy, since OpenAI does little besides operating its AI business, but we hope the company finds a path forward that allows it to serve users, rewards its employees for their contributions, and honors its non-profit charter. The latest international AI summit exposed deep divisions between major world powers regarding AI regulations. What\u2019s new: While previous summits emphasized existential risks, the AI Action Summit in Paris marked a turning point. France and the European Union shifted away from strict regulatory measures and toward investment to compete with the United States and China. However, global consensus remained elusive: the U.S. and the United Kingdom refused to sign key agreements on global governance, military AI, and algorithmic bias. The U.S. in particular pushed back against global AI regulation, arguing that excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns. How it works: Participating countries considered three policy statements that address AI\u2019s impact on society, labor, and security. The first statement calls on each country to enact AI policies that would support economic development,", "article_id": "68497b48235d602d65122547", "linked_images": ["article_68497b48235d602d65122547_img_0", "article_68497b48235d602d65122547_img_1", "article_68497b48235d602d65122547_img_2", "article_68497b48235d602d65122547_img_3", "article_68497b48235d602d65122547_img_4", "article_68497b48235d602d65122547_img_5"]}, {"type": "text", "text": "excessive restrictions could hinder economic growth and that international policies should focus on more immediate concerns. How it works: Participating countries considered three policy statements that address AI\u2019s impact on society, labor, and security. The first statement calls on each country to enact AI policies that would support economic development, environmental responsibility, and equitable access to technology. The second encourages safeguards to ensure that companies and nations distribute AI productivity gains fairly, protect workers\u2019 rights, and prevent bias in hiring and management systems. The third advocates for restrictions on fully autonomous military systems and affirms the need for human oversight in warfare. Behind the news: The Paris summit follows previous gatherings of world leaders to discuss AI, including the initial AI Safety Summit at Bletchley Park and the AI Seoul Summit and AI Global Forum. At these summits, governments and companies agreed broadly to address AI risks but avoided binding regulations. Nonetheless, divisions over AI governance have widened in the wake of rising geopolitical competition and the emergence of high-performance open weights models like DeepSeek-R1. Why it matters: The Paris summit marks a major shift in global AI policy. The EU, once an ardent proponent of AI regulation, backed away from its strictest proposals. At the same time, doomsayers have lost influence, and officials are turning their attention to immediate concerns like economic growth, security, misuse, and bias. These moves make way for AI to do great good in the world, even as they contribute to uncertainty about how AI will be governed. We\u2019re thinking: Governments are shifting their focus away from unrealistic risks and toward practical strategies for guiding AI development. We look forward to clear policies that encourage innovation while addressing real-world challenges.", "article_id": "68497b48235d602d65122547", "linked_images": ["article_68497b48235d602d65122547_img_0", "article_68497b48235d602d65122547_img_1", "article_68497b48235d602d65122547_img_2", "article_68497b48235d602d65122547_img_3", "article_68497b48235d602d65122547_img_4", "article_68497b48235d602d65122547_img_5"]}, {"type": "image", "article_id": "68497b48235d602d65122547", "linked_chunks": ["article_68497b48235d602d65122547_chunk_0", "article_68497b48235d602d65122547_chunk_1", "article_68497b48235d602d65122547_chunk_2", "article_68497b48235d602d65122547_chunk_3", "article_68497b48235d602d65122547_chunk_4"], "alt_text": "Thermal aerial image showing a suspect surrendering with hands raised. A marker highlights their location."}, {"type": "image", "article_id": "68497b48235d602d65122547", "linked_chunks": ["article_68497b48235d602d65122547_chunk_0", "article_68497b48235d602d65122547_chunk_1", "article_68497b48235d602d65122547_chunk_2", "article_68497b48235d602d65122547_chunk_3", "article_68497b48235d602d65122547_chunk_4"], "alt_text": "Promo banner for: \"Evaluating AI Agents\""}, {"type": "image", "article_id": "68497b48235d602d65122547", "linked_chunks": ["article_68497b48235d602d65122547_chunk_0", "article_68497b48235d602d65122547_chunk_1", "article_68497b48235d602d65122547_chunk_2", "article_68497b48235d602d65122547_chunk_3", "article_68497b48235d602d65122547_chunk_4"], "alt_text": "AI model comparison on reasoning and test-time compute across math, science, and coding benchmarks."}, {"type": "image", "article_id": "68497b48235d602d65122547", "linked_chunks": ["article_68497b48235d602d65122547_chunk_0", "article_68497b48235d602d65122547_chunk_1", "article_68497b48235d602d65122547_chunk_2", "article_68497b48235d602d65122547_chunk_3", "article_68497b48235d602d65122547_chunk_4"], "alt_text": "A person typing a prompt in an AI-powered mobile app with a button to improve the input."}, {"type": "image", "article_id": "68497b48235d602d65122547", "linked_chunks": ["article_68497b48235d602d65122547_chunk_0", "article_68497b48235d602d65122547_chunk_1", "article_68497b48235d602d65122547_chunk_2", "article_68497b48235d602d65122547_chunk_3", "article_68497b48235d602d65122547_chunk_4"], "alt_text": "Illustration of two men staring intensely at each other against a red and yellow background, symbolizing rivalry."}, {"type": "image", "article_id": "68497b48235d602d65122547", "linked_chunks": ["article_68497b48235d602d65122547_chunk_0", "article_68497b48235d602d65122547_chunk_1", "article_68497b48235d602d65122547_chunk_2", "article_68497b48235d602d65122547_chunk_3", "article_68497b48235d602d65122547_chunk_4"], "alt_text": "AI Action Summit in a grand hall with domes, flags, and a crowd attending the event."}, {"type": "alt_text", "alt_text": "Thermal aerial image showing a suspect surrendering with hands raised. A marker highlights their location."}, {"type": "alt_text", "alt_text": "Promo banner for: \"Evaluating AI Agents\""}, {"type": "alt_text", "alt_text": "AI model comparison on reasoning and test-time compute across math, science, and coding benchmarks."}, {"type": "alt_text", "alt_text": "A person typing a prompt in an AI-powered mobile app with a button to improve the input."}, {"type": "alt_text", "alt_text": "Illustration of two men staring intensely at each other against a red and yellow background, symbolizing rivalry."}, {"type": "alt_text", "alt_text": "AI Action Summit in a grand hall with domes, flags, and a crowd attending the event."}, {"type": "text", "text": "Dear friends, At the Artificial Intelligence Action Summit in Paris this week, U.S. Vice President J.D. Vance said, \u201cI\u2019m not here to talk about AI safety. ... I\u2019m here to talk about AI opportunity.\u201d I\u2019m thrilled to see the U.S. government focus on opportunities in AI. Further, while it is important to use AI responsibly and try to stamp out harmful applications, I feel \u201cAI safety\u201d is not the right terminology for addressing this important problem. Language shapes thought, so using the right words is important. I\u2019d rather talk about \u201cresponsible AI\u201d than \u201cAI safety.\u201d Let me explain. First, there are clearly harmful applications of AI, such as non-consensual deepfake porn (which creates sexually explicit images of real people without their consent), the use of AI in misinformation, potentially unsafe medical diagnoses, addictive applications, and so on. We definitely want to stamp these out! There are many ways to apply AI in harmful or irresponsible ways, and we should discourage and prevent such uses. However, the concept of \u201cAI safety\u201d tries to make AI \u2014 as a technology \u2014 safe, rather than making safe applications of it. Consider the similar, obviously flawed notion of \u201claptop safety.\u201d There are great ways to use a laptop and many irresponsible ways, but I don\u2019t consider laptops to be intrinsically either safe or unsafe. It is the application, or usage, that determines if a laptop is safe. Similarly, AI, a general-purpose technology with numerous applications, is neither safe nor unsafe. How someone chooses to use it determines whether it is harmful or beneficial. Now, safety isn\u2019t always a function only of how something is used. An unsafe airplane is one that, even in the hands of an attentive and skilled pilot, has a large chance of mishap. So we definitely should strive to build safe airplanes (and make sure they are operated responsibly)! The risk factors are associated with the construction of the aircraft rather than merely its application. Similarly, we want safe automobiles, blenders, dialysis machines, food, buildings, power plants, and much more. \u201cAI safety\u201d presupposes that AI, the underlying technology, can be unsafe. I find it more useful to think about how applications of AI can be unsafe. Further, the term \u201cresponsible AI\u201d emphasizes that it is our responsibility to avoid building applications that are unsafe or harmful and to discourage people from using even beneficial products in harmful ways. If we shift the terminology for AI risks from \u201cAI safety\u201d to \u201cresponsible AI,\u201d we can have more thoughtful conversations about what to do and what not to do. I believe the 2023 Bletchley AI Safety Summit slowed down European AI development \u2014 without making anyone safer \u2014 by wasting time considering science-fiction AI fears rather than focusing on opportunities. Last month, at Davos, business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration. In a world where AI", "article_id": "68497b48235d602d6512254e", "linked_images": ["article_68497b48235d602d6512254e_img_0", "article_68497b48235d602d6512254e_img_1", "article_68497b48235d602d6512254e_img_2", "article_68497b48235d602d6512254e_img_3", "article_68497b48235d602d6512254e_img_4", "article_68497b48235d602d6512254e_img_5"]}, {"type": "text", "text": "business and policy leaders also had strong concerns about whether Europe can dig itself out of the current regulatory morass and focus on building with AI. I am hopeful that the Paris meeting, unlike the one at Bletchley, will result in acceleration rather than deceleration. In a world where AI is becoming pervasive, if we can shift the conversation away from \u201cAI safety\u201d toward responsible [use of] AI, we will speed up AI\u2019s benefits and do a better job of addressing actual problems. That will actually make people safer. Keep building! Andrew Understand and implement the attention mechanism, a key element in transformer-based LLMs, using PyTorch. In this course, StatQuest\u2019s Josh Starmer explains the core ideas behind attention mechanisms, the algorithm itself, and a step-by-step breakdown of how to implement them in PyTorch. Enroll now OpenAI introduced a state-of-the-art agent that produces research reports by scouring the web and reasoning over what it finds. What\u2019s new: OpenAI\u2019s deep research responds to users\u2019 requests by generating a detailed report based on hundreds of online sources. The system generates text output, with images and other media expected soon. Currently the agent is available only to subscribers to ChatGPT Pro, but the company plans to roll it out to users of ChatGPT Plus, Team, and Enterprise. How it works: Deep research is an agent that uses OpenAI\u2019s o3 model, which is not yet publicly available. The model was trained via reinforcement learning to use a browser and Python tools, similar to the way o1 learned to reason from reinforcement learning. OpenAI has not yet released detailed information about how it built the system. Result: On a benchmark of 3,000 multiple-choice and short-answer questions that cover subjects from ecology to rocket science, OpenAI deep research achieved 26.6 percent accuracy. In comparison, DeepSeek-R1 (without web browsing or other tool use) achieved 9.4 percent accuracy and o1 (also without tool use) achieved 9.1 percent accuracy. On GAIA, questions that are designed to be difficult for large language models without access to additional tools, OpenAI deep research achieved 67.36 percent accuracy, exceeding the previous state of the art of 63.64 percent accuracy. Behind the news: OpenAI\u2019s deep research follows a similar offering of the same name by Google in December. A number of open source teams have built research agents that work in similar ways. Notable releases include a Hugging Face project that attempted to replicate OpenAI\u2019s work (not including training) in 24 hours (which achieved 55.15 percent accuracy on GAIA) and gpt-researcher, which implemented agentic web search in 2023, long before Google and OpenAI launched their agentic research systems. Why it matters: Reasoning models like o1 or o3 made a splash not just because they delivered superior results but also because of the impressive reasoning steps the model took to produce the results. Combining that ability with web search and tool use enables large language models to formulate better answers to difficult questions, including those whose answers aren\u2019t in the training data or whose answers change over time. We\u2019re thinking: Taking as much as 30 minutes of processing", "article_id": "68497b48235d602d6512254e", "linked_images": ["article_68497b48235d602d6512254e_img_0", "article_68497b48235d602d6512254e_img_1", "article_68497b48235d602d6512254e_img_2", "article_68497b48235d602d6512254e_img_3", "article_68497b48235d602d6512254e_img_4", "article_68497b48235d602d6512254e_img_5"]}, {"type": "text", "text": "took to produce the results. Combining that ability with web search and tool use enables large language models to formulate better answers to difficult questions, including those whose answers aren\u2019t in the training data or whose answers change over time. We\u2019re thinking: Taking as much as 30 minutes of processing to render a response, OpenAI\u2019s deep research clearly illustrates why we need more compute for inference. Google revised its AI principles, reversing previous commitments to avoid work on weapons, surveillance, and other military applications beyond non-lethal uses like communications, logistics, and medicine. What\u2019s new: Along with releasing its latest Responsible AI Progress Report and an updated AI safety framework, Google removed key restrictions from its AI principles. The new version omits a section in the previous document titled \u201cApplications we will not pursue.\u201d The deleted text pledged to avoid \u201ctechnologies that cause or are likely to cause overall harm\u201d and, where the technology risks doing harm, to \u201cproceed only where we believe that the benefits substantially outweigh the risks\u201d with \u201cappropriate safety constraints.\u201d How it works: Google\u2019s AI principles no longer prohibit specific applications but promote developing the technology to improve scientific inquiry, national security, and the economy. Behind the news: Google\u2019s new stance reverses a commitment it made in 2018 after employees protested its involvement in Project Maven, a Pentagon AI program for drone surveillance, from which Google ultimately withdrew. At the time, Google pledged not to develop AI applications for weapons or surveillance, which set it apart from Amazon and Microsoft. Since then, the company has expanded its work in defense, building on a $1.3 billion contract with Israel. In 2024, Anthropic, Meta, and OpenAI removed their restrictions on military and defense applications, and Anthropic and OpenAI strengthened their ties with defense contractors such as Anduril and Palantir. Why it matters: Google\u2019s shift in policy comes as AI is playing an increasing role in conflicts in Israel, Ukraine, and elsewhere, and while global geopolitical tensions are on the rise. While Google\u2019s previous position kept it out of military AI development, defense contractors like Anduril, Northrop Grumman, and Palantir \u2014 not to mention AI-giant peers \u2014 stepped in. The new principles recognize the need for democratic countries to take the lead in developing technology and standards for its use as well as the massive business opportunity in military AI as governments worldwide seek new defense capabilities. Still, no widely accepted global framework governs uses of AI in combat. We\u2019re thinking: Knowing how and when to employ AI in warfare is one of the most difficult ethical questions of our time. Democratic nations have a right to defend themselves, and those of us who live in democracies have a responsibility to support fellow citizens who would put themselves in harm\u2019s way to protect us. AI is transforming military strategy, and refusing to engage with it doesn\u2019t make the risks go away. While Hangzhou\u2019s DeepSeek flexed its muscles, Chinese tech giant Alibaba vied for the spotlight with new open vision-language models. What\u2019s new: Alibaba announced Qwen2.5-VL, a family of vision-language models (images and", "article_id": "68497b48235d602d6512254e", "linked_images": ["article_68497b48235d602d6512254e_img_0", "article_68497b48235d602d6512254e_img_1", "article_68497b48235d602d6512254e_img_2", "article_68497b48235d602d6512254e_img_3", "article_68497b48235d602d6512254e_img_4", "article_68497b48235d602d6512254e_img_5"]}, {"type": "text", "text": "protect us. AI is transforming military strategy, and refusing to engage with it doesn\u2019t make the risks go away. While Hangzhou\u2019s DeepSeek flexed its muscles, Chinese tech giant Alibaba vied for the spotlight with new open vision-language models. What\u2019s new: Alibaba announced Qwen2.5-VL, a family of vision-language models (images and text in, text out) in sizes of 3 billion, 7 billion, and 72 billion parameters. The weights for all three models are available for download on Hugging Face, each under a different license: Qwen2.5-VL-3B is free for non-commercial uses, Qwen2.5-VL-7B is free for commercial and noncommercial uses under the Apache 2.0 license, and Qwen2.5-VL-72B is free to developers that have less than 100 million monthly active users. You can try them out for free for a limited time in Alibaba Model Studio, and Qwen2.5-VL-72B is available via the model selector in Qwen Chat. How it works: Qwen2.5-VL models accept up to 129,024 tokens of input according to the developer reference (other sources provide conflicting numbers) and generate up to 8,192 tokens of output. Alibaba has not released details about how it trained them. Results: Alibaba reports Qwen2.5-VL-72B\u2019s performance on measures that span image and text problems, parsing documents, understanding videos, and interacting with computer programs. Across 21 benchmarks, it beat Microsoft Gemini 2.0 Flash, OpenAI GPT-4o, Anthropic Claude 3.5 Sonnet, and open competitors on 13 of them (where comparisons are relevant and available). More models: Alibaba also introduced competition for DeepSeek and a family of small models. Why it matters: Vision-language models are getting more powerful and versatile. Not long ago, it was an impressive feat simply to answer questions about a chart or diagram that mixed graphics with text. Now such models are paired with an agent to control computers and smartphones. Broadly speaking, the Qwen2.5-VL models outperform open and closed competitors and they\u2019re open to varying degrees (though the data is not available), giving developers a range of highly capable choices. We\u2019re thinking: We\u2019re happy Alibaba released a vision-language model that is broadly permissive with respect to commercial use (although we\u2019d prefer that all sizes were available under a standard open weights license). We hope to see technical reports that illuminate Alibaba\u2019s training and fine-tuning recipes. Browsing the web to achieve a specific goal can be challenging for agents based on large language models and even for vision-language models that can process onscreen images of a browser. While some approaches address this difficulty in training the underlying model, the agent architecture can also make a difference. What\u2019s new: Jing Yu Koh and colleagues at Carnegie Mellon University introduced tree search for language model agents, a method that allows agents to treat web interactions like tree searches. In this way, agents can explore possible chains of actions and avoid repeating mistakes. Key insight: Some web tasks, for instance finding a price of a particular item, require a chain of intermediate actions: navigating to the right page, scrolling to find the item, matching an image of the item to the image on the page, and so on. If an agent clicks the", "article_id": "68497b48235d602d6512254e", "linked_images": ["article_68497b48235d602d6512254e_img_0", "article_68497b48235d602d6512254e_img_1", "article_68497b48235d602d6512254e_img_2", "article_68497b48235d602d6512254e_img_3", "article_68497b48235d602d6512254e_img_4", "article_68497b48235d602d6512254e_img_5"]}, {"type": "text", "text": "Key insight: Some web tasks, for instance finding a price of a particular item, require a chain of intermediate actions: navigating to the right page, scrolling to find the item, matching an image of the item to the image on the page, and so on. If an agent clicks the wrong link during this process, it might lose its way. The ability to evaluate possible actions and remember previous states of web pages can help an agent correct its mistakes and choose a chain of actions that achieves its goal. How it works: An agent based on GPT-4o attempted 200 tasks using website mockups that mimicked an online retail store, Reddit-like forum, and directory of classified ads. The tasks included ordering an item to be delivered to a given address, finding specific images on the forum, and posting an ad. The authors annotated each web page using the method called Set of Mark, which identifies every visual element capable of interaction with a bounding box and a numerical ID. Results: The authors compared two agents, one that followed their search method and another that started at the same page and received the same instruction but took one action per state and never backtracked. The agents attempted 100 shopping tasks, 50 forum tasks, and 50 classified-ads tasks. The one equipped to search successfully completed 26.4 percent of the tasks, while the other agent completed 18.9 percent of the tasks. Why it matters: Search joins reflection, planning, tool use, and multi-agent collaboration as an emerging agentic design pattern. Following many branching paths of actions enables an agent to determine the most effective set of actions to accomplish a task. We\u2019re thinking: Agentic design patterns are progressing quickly! In combination with computer use, this sort of search method may enable agents to execute a wide variety of desktop tasks.", "article_id": "68497b48235d602d6512254e", "linked_images": ["article_68497b48235d602d6512254e_img_0", "article_68497b48235d602d6512254e_img_1", "article_68497b48235d602d6512254e_img_2", "article_68497b48235d602d6512254e_img_3", "article_68497b48235d602d6512254e_img_4", "article_68497b48235d602d6512254e_img_5"]}, {"type": "image", "article_id": "68497b48235d602d6512254e", "linked_chunks": ["article_68497b48235d602d6512254e_chunk_0", "article_68497b48235d602d6512254e_chunk_1", "article_68497b48235d602d6512254e_chunk_2", "article_68497b48235d602d6512254e_chunk_3", "article_68497b48235d602d6512254e_chunk_4"], "alt_text": "\u201cResponsible AI\u201d written on a wall, with \u201cSafety\u201d crossed out in blue paint."}, {"type": "image", "article_id": "68497b48235d602d6512254e", "linked_chunks": ["article_68497b48235d602d6512254e_chunk_0", "article_68497b48235d602d6512254e_chunk_1", "article_68497b48235d602d6512254e_chunk_2", "article_68497b48235d602d6512254e_chunk_3", "article_68497b48235d602d6512254e_chunk_4"], "alt_text": "Promo banner for \"Attention in Transformers: Concepts and Code in PyTorch\""}, {"type": "image", "article_id": "68497b48235d602d6512254e", "linked_chunks": ["article_68497b48235d602d6512254e_chunk_0", "article_68497b48235d602d6512254e_chunk_1", "article_68497b48235d602d6512254e_chunk_2", "article_68497b48235d602d6512254e_chunk_3", "article_68497b48235d602d6512254e_chunk_4"], "alt_text": "ChatGPT interface drafting a research report on retail trends, including AI, e-commerce, and inflation."}, {"type": "image", "article_id": "68497b48235d602d6512254e", "linked_chunks": ["article_68497b48235d602d6512254e_chunk_0", "article_68497b48235d602d6512254e_chunk_1", "article_68497b48235d602d6512254e_chunk_2", "article_68497b48235d602d6512254e_chunk_3", "article_68497b48235d602d6512254e_chunk_4"], "alt_text": "Illustration of the Google logo near a futuristic facility with fighter jets flying overhead."}, {"type": "image", "article_id": "68497b48235d602d6512254e", "linked_chunks": ["article_68497b48235d602d6512254e_chunk_0", "article_68497b48235d602d6512254e_chunk_1", "article_68497b48235d602d6512254e_chunk_2", "article_68497b48235d602d6512254e_chunk_3", "article_68497b48235d602d6512254e_chunk_4"], "alt_text": "AI model leaderboard comparing performance across tasks like math, vision, and document analysis."}, {"type": "image", "article_id": "68497b48235d602d6512254e", "linked_chunks": ["article_68497b48235d602d6512254e_chunk_0", "article_68497b48235d602d6512254e_chunk_1", "article_68497b48235d602d6512254e_chunk_2", "article_68497b48235d602d6512254e_chunk_3", "article_68497b48235d602d6512254e_chunk_4"], "alt_text": "Diagram showing GPT-4o with and without search, highlighting task execution success and failure."}, {"type": "alt_text", "alt_text": "\u201cResponsible AI\u201d written on a wall, with \u201cSafety\u201d crossed out in blue paint."}, {"type": "alt_text", "alt_text": "Promo banner for \"Attention in Transformers: Concepts and Code in PyTorch\""}, {"type": "alt_text", "alt_text": "ChatGPT interface drafting a research report on retail trends, including AI, e-commerce, and inflation."}, {"type": "alt_text", "alt_text": "Illustration of the Google logo near a futuristic facility with fighter jets flying overhead."}, {"type": "alt_text", "alt_text": "AI model leaderboard comparing performance across tasks like math, vision, and document analysis."}, {"type": "alt_text", "alt_text": "Diagram showing GPT-4o with and without search, highlighting task execution success and failure."}, {"type": "text", "text": "Dear friends, A \u201c10x engineer\u201d \u2014 a widely accepted concept in tech \u2014 purportedly has 10 times the impact of the average engineer. But we don\u2019t seem to have 10x marketers, 10x recruiters, or 10x financial analysts. As more jobs become AI enabled, I think this will change, and there will be a lot more \u201c10x professionals.\u201d There aren\u2019t already more 10x professionals because, in many roles, the gap between the best and the average worker has a ceiling. No matter how athletic a supermarket checkout clerk is, they\u2019re not likely to scan groceries so fast that customers get out of the store 10x faster. Similarly, even the best doctor is unlikely to make patients heal 10x faster than an average one (but to a sick patient, even a small difference is worth a lot). In many jobs, the laws of physics place a limit on what any human or AI can do (unless we completely reimagine that job). But for many jobs that primarily involve applying knowledge or processing information, AI will be transformative. In a few roles, I\u2019m starting to see tech-savvy individuals coordinate a suite of technology tools to do things differently and start to have, if not yet 10x impact, then easily 2x impact. I expect this gap to grow. 10x engineers don\u2019t write code 10 times faster. Instead, they make technical architecture decisions that result in dramatically better downstream impact, they spot problems and prioritize tasks more effectively, and instead of rewriting 10,000 lines of code (or labeling 10,000 training examples) they might figure out how to write just 100 lines (or collect 100 examples) to get the job done. I think 10x marketers, recruiters, and analysts will, similarly, do things differently. For example, perhaps traditional marketers repeatedly write social media posts. 10x marketers might use AI to help write, but the transformation will go deeper than that. If they are deeply sophisticated in how to apply AI \u2014 ideally able to write code themselves to test ideas, automate tasks, or analyze data \u2014 they might end up running a lot more experiments, get better insights about what customers want, and generate much more precise or personalized messages than a traditional marketer, and thereby end up making 10x impact. Similarly, 10x recruiters won\u2019t just use generative AI to help write emails to candidates or summarize interviews. (This level of use of prompting-based AI will soon become table stakes for many knowledge roles.) They might coordinate a suite of AI tools to efficiently identify and carry out research on a large set of candidates, enabling them to have dramatically greater impact than the average recruiter. And 10x analysts won\u2019t just use generative AI to edit their reports. They might write code to orchestrate a suite of AI agents to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way. A 2023 Harvard/BCG study estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the", "article_id": "68497b48235d602d65122555", "linked_images": ["article_68497b48235d602d65122555_img_0", "article_68497b48235d602d65122555_img_1", "article_68497b48235d602d65122555_img_2", "article_68497b48235d602d65122555_img_3", "article_68497b48235d602d65122555_img_4", "article_68497b48235d602d65122555_img_5"]}, {"type": "text", "text": "to do deep research into the products, markets, and companies, and thereby derive far more valuable conclusions than someone who does research the traditional way. A 2023 Harvard/BCG study estimated that, provided with GPT-4, consultants could complete 12% more tasks, and completed tasks 25% more quickly. This was just the average, using 2023 technology. The maximum advantage to be gained by using AI in a sophisticated way will be much bigger, and will only grow as technology improves. Here in Silicon Valley, I see more and more AI-native teams reinvent workflows and do things very differently. In software engineering, we've venerated the best engineers because they can have a really massive impact. This has motivated many generations of engineers to keep learning and working hard, because doing those things increases the odds of doing high-impact work. As AI becomes more helpful in many more job roles, I believe we will open up similar paths to a lot more people becoming a \u201c10x professional.\u201d Keep learning! Andrew Learn in detail how transformer-based large language models work in this new course by the authors of Hands-On Large Language Models. Explore the architecture introduced in the paper \u201cAttention Is All You Need,\u201d and learn through intuitive explanations and code examples. Join in for free OpenAI introduced a successor to its o1 models that\u2019s faster, less expensive, and especially strong in coding, math, and science. What\u2019s new: o3-mini is a large language model that offers selectable low, medium, and high levels of reasoning \u201ceffort.\u201d These levels consume progressively higher numbers of reasoning tokens (specific numbers and methods are undisclosed), and thus greater time and cost, to generate a chain of thought. It\u2019s available to subscribers to ChatGPT Plus, Team, and Pro, as well as to higher-volume users of the API (tiers 3 through 5). Registered users can try it via the free ChatGPT service by selecting \u201creason\u201d in the message composer or selecting o3-mini before regenerating a response. How it works: o3-mini\u2019s training set emphasized structured problem-solving in science and technology fields, and fine-tuning used reinforcement learning on chain-of-thought (CoT) data. Like the o1 family, it charges for tokens that are processed during reasoning operations and hides them from the user. (Competing reasoning models DeepSeek-R1, Gemini 2.0 Flash Thinking, and QwQ-32B-Preview make these tokens available to users.) o3-mini has a maximum input of 200,000 tokens and a maximum output of 100,000 tokens. Its knowledge cutoff is October 2023. What they\u2019re saying: Users praised o3-mini for its speed, reasoning, and coding abilities. They noted that it responds best to \u201cchunkier\u201d prompts with lots of context. However, due to its smaller size, it lacks extensive real-world knowledge and struggles to recall facts. Behind the news: Days after releasing o3-mini, OpenAI launched deep research, a ChatGPT research agent based on o3. OpenAI had announced the o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAI maintained that the", "article_id": "68497b48235d602d65122555", "linked_images": ["article_68497b48235d602d65122555_img_0", "article_68497b48235d602d65122555_img_1", "article_68497b48235d602d65122555_img_2", "article_68497b48235d602d65122555_img_3", "article_68497b48235d602d65122555_img_4", "article_68497b48235d602d65122555_img_5"]}, {"type": "text", "text": "on o3. OpenAI had announced the o3 model family in December, positioning it as an evolution of its chain-of-thought approach. The release followed hard upon that of DeepSeek-R1, an open weights model that captivated the AI community with its high performance and low training cost, but OpenAI maintained that the debut took place on its original schedule. Why it matters: o3-mini continues OpenAI\u2019s leadership in language models and further refines the reasoning capabilities introduced with the o1 family. In focusing on coding, math, and science tasks, it takes advantage of the strengths of reasoning models and raises the bar for other model builders. In practical terms, it pushes AI toward applications in which it\u2019s a reliable professional partner rather than a smart intern. We\u2019re thinking: We\u2019re glad that o3-mini is available to users of ChatGPT\u2019s free tier as well as paid subscribers and API users. The more users become familiar with how to prompt reasoning models, the more value they\u2019ll deliver. As Anthropic, Google, OpenAI, and others roll out agents that are capable of computer use, new work shows how underlying models can be trained to do this. What\u2019s new: Yujian Qin and colleagues at ByteDance and Tsinghua University introduced UI-TARS, a fine-tuned version of the vision-language model Qwen2-VL that uses lines of reasoning to decide which mouse clicks, keyboard presses, and other actions to take in desktop and mobile apps. The model\u2019s weights are licensed freely for commercial and noncommercial uses via Apache 2.0. You can download them here. Behind the news: Adept touted computer use in early 2022, and OmniParser Aguvis soon followed with practical implementations. In October 2024, Anthropic set off the current wave of model/app interaction with its announcement of computer use for Claude 3.5 Sonnet. OpenAI recently responded with Operator, its own foray into using vision and language models to control computers. Results: UI-TARS matched or outperformed Claude 3.5 Sonnet with computer use, GPT-4o with various computer use frameworks, and the Aguvis framework with its native model on 11 benchmarks. On OSWorld, which asks models to perform tasks using a variety of real-world applications and operating systems, UI-TARS successfully completed 22.7 percent of the tasks in 15 steps, whereas Claude 3.5 Sonnet with computer use completed 14.9 percent, GPT-4o with Aguvis 17 percent, and Aguvis with its native model 10.3 percent. Why it matters: Training a model to take good actions enables it to perform well. Training it to correct its mistakes after making them enables it to recover from unexpected issues that may occur in the real world. We\u2019re thinking: Since computer use can be simulated in a virtual machine, it\u2019s possible to generate massive amounts of training data automatically. This is bound to spur rapid progress in computer use by large language models. Google updated the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1. What\u2019s new: Gemini 2.0 Flash Thinking Experimental 1-21 is a vision-language model (images and text in, text out) that\u2019s trained to generate a structured reasoning process or chain of thought. The", "article_id": "68497b48235d602d65122555", "linked_images": ["article_68497b48235d602d65122555_img_0", "article_68497b48235d602d65122555_img_1", "article_68497b48235d602d65122555_img_2", "article_68497b48235d602d65122555_img_3", "article_68497b48235d602d65122555_img_4", "article_68497b48235d602d65122555_img_5"]}, {"type": "text", "text": "the December-vintage reasoning model Gemini 2.0 Flash Thinking and other Flash models, gaining ground on OpenAI o1 and DeepSeek-R1. What\u2019s new: Gemini 2.0 Flash Thinking Experimental 1-21 is a vision-language model (images and text in, text out) that\u2019s trained to generate a structured reasoning process or chain of thought. The new version improves on its predecessor\u2019s reasoning capability and extends its context window. It's free to access via API while it remains designated \u201cexperimental\u201d and available to paid users of the Gemini app, along with Gemini 2.0 Flash (fresh out of experimental mode) and the newly released Gemini 2.0 Pro Experimental. The company also launched a preview of Gemini 2.0 Flash Lite, a vision-language model (images and text in, text out) that outperforms Gemini 1.5 Flash at the same price. How it works: Gemini 2.0 Flash Thinking Experimental 1-21 is based on Gemini 2.0 Flash Experimental (parameter count undisclosed). It processes up to 1 million tokens of input context, compared to its predecessor\u2019s 32,000 and o1\u2019s 128,000. Speed bumps: Large language models that are trained to generate a chain of thought (CoT) are boosting accuracy even as the additional processing increases inference costs and latency. Reliable measures of Gemini 2.0 Flash Thinking Experimental 1-21\u2019s speed are not yet available, but its base model runs faster (168.8 tokens per second with 0.46 seconds of latency to the first token, according to Artificial Analysis) than all models in its class except o1-mini (which outputs 200 tokens per second with 10.59 seconds of latency to the first token). Why it matters: The combination of CoT reasoning and long context \u2014 assuming the new model can take advantage of its 1 million-token context window, as measured by a benchmark such as RULER \u2014 could open up valuable applications. Imagine a reasoning model that can take an entire codebase as input and analyze it without breaking it into smaller chunks.We\u2019re thinking: Regardless of benchmark performance, this model topped the Chatbot Arena leaderboard at the time of writing. This suggests that users preferred it over o1 and DeepSeek-R1 \u2014 at least for common, everyday prompts. Even cutting-edge, end-to-end, speech-to-speech systems like ChatGPT\u2019s Advanced Voice Mode tend to get interrupted by interjections like \u201cI see\u201d and \u201cuh-huh\u201d that keep human conversations going. Researchers built an open alternative that\u2019s designed to go with the flow of overlapping speech. What\u2019s new: Alexandre D\u00e9fossez, Laurent Mazar\u00e9, and colleagues at Kyutai, a nonprofit research lab in Paris, released Moshi, an end-to-end, speech-to-speech system that\u2019s always listening and always responding. The weights and code are free for noncommercial and commercial uses under CC-BY 4.0, Apache 2.0, and MIT licenses. You can try a web demo here. Key insight: Up to 20 percent of spoken conversation consists of overlapping speech, including interjections like \u201cokay\u201d and \u201cI see.\u201d How it works: The authors combined an encoder-decoder called Mimi and an RQ-Transformer, which is made up of the Helium transformer-based large language model (LLM) plus another transformer. Results: In tests, Moshi proved fast and relatively accurate. Why it matters: While a turn-based approach may suffice for text", "article_id": "68497b48235d602d65122555", "linked_images": ["article_68497b48235d602d65122555_img_0", "article_68497b48235d602d65122555_img_1", "article_68497b48235d602d65122555_img_2", "article_68497b48235d602d65122555_img_3", "article_68497b48235d602d65122555_img_4", "article_68497b48235d602d65122555_img_5"]}, {"type": "text", "text": "\u201cI see.\u201d How it works: The authors combined an encoder-decoder called Mimi and an RQ-Transformer, which is made up of the Helium transformer-based large language model (LLM) plus another transformer. Results: In tests, Moshi proved fast and relatively accurate. Why it matters: While a turn-based approach may suffice for text input, voice-to-voice interactions benefit from a system that processes both input and output quickly and continuously. Previous systems process input and output separately, making users wait. Moshi delivers seamless interactivity. We\u2019re thinking: Generating silence is golden!", "article_id": "68497b48235d602d65122555", "linked_images": ["article_68497b48235d602d65122555_img_0", "article_68497b48235d602d65122555_img_1", "article_68497b48235d602d65122555_img_2", "article_68497b48235d602d65122555_img_3", "article_68497b48235d602d65122555_img_4", "article_68497b48235d602d65122555_img_5"]}, {"type": "image", "article_id": "68497b48235d602d65122555", "linked_chunks": ["article_68497b48235d602d65122555_chunk_0", "article_68497b48235d602d65122555_chunk_1", "article_68497b48235d602d65122555_chunk_2", "article_68497b48235d602d65122555_chunk_3", "article_68497b48235d602d65122555_chunk_4"], "alt_text": "Comic-style illustration of a confident woman and man standing beside bold \u201810X\u2019 text on a bright background."}, {"type": "image", "article_id": "68497b48235d602d65122555", "linked_chunks": ["article_68497b48235d602d65122555_chunk_0", "article_68497b48235d602d65122555_chunk_1", "article_68497b48235d602d65122555_chunk_2", "article_68497b48235d602d65122555_chunk_3", "article_68497b48235d602d65122555_chunk_4"], "alt_text": "Promo banner for \"How Transformer LLMs Work\""}, {"type": "image", "article_id": "68497b48235d602d65122555", "linked_chunks": ["article_68497b48235d602d65122555_chunk_0", "article_68497b48235d602d65122555_chunk_1", "article_68497b48235d602d65122555_chunk_2", "article_68497b48235d602d65122555_chunk_3", "article_68497b48235d602d65122555_chunk_4"], "alt_text": "Bar chart animation showing accuracy improvements in AIME 2024 competition math models."}, {"type": "image", "article_id": "68497b48235d602d65122555", "linked_chunks": ["article_68497b48235d602d65122555_chunk_0", "article_68497b48235d602d65122555_chunk_1", "article_68497b48235d602d65122555_chunk_2", "article_68497b48235d602d65122555_chunk_3", "article_68497b48235d602d65122555_chunk_4"], "alt_text": "Flowchart illustrating the automation of opening, editing, and saving a Word document using PyAutoGUI."}, {"type": "image", "article_id": "68497b48235d602d65122555", "linked_chunks": ["article_68497b48235d602d65122555_chunk_0", "article_68497b48235d602d65122555_chunk_1", "article_68497b48235d602d65122555_chunk_2", "article_68497b48235d602d65122555_chunk_3", "article_68497b48235d602d65122555_chunk_4"], "alt_text": "Line charts showing performance improvements in math and science with 2.0 Flash Thinking models."}, {"type": "image", "article_id": "68497b48235d602d65122555", "linked_chunks": ["article_68497b48235d602d65122555_chunk_0", "article_68497b48235d602d65122555_chunk_1", "article_68497b48235d602d65122555_chunk_2", "article_68497b48235d602d65122555_chunk_3", "article_68497b48235d602d65122555_chunk_4"], "alt_text": "Diagram illustrating Moshi\u2019s use of an LLM to process user audio input, inner monologue, and output."}, {"type": "alt_text", "alt_text": "Comic-style illustration of a confident woman and man standing beside bold \u201810X\u2019 text on a bright background."}, {"type": "alt_text", "alt_text": "Promo banner for \"How Transformer LLMs Work\""}, {"type": "alt_text", "alt_text": "Bar chart animation showing accuracy improvements in AIME 2024 competition math models."}, {"type": "alt_text", "alt_text": "Flowchart illustrating the automation of opening, editing, and saving a Word document using PyAutoGUI."}, {"type": "alt_text", "alt_text": "Line charts showing performance improvements in math and science with 2.0 Flash Thinking models."}, {"type": "alt_text", "alt_text": "Diagram illustrating Moshi\u2019s use of an LLM to process user audio input, inner monologue, and output."}, {"type": "text", "text": "Dear friends, The buzz over DeepSeek this week crystallized, for many people, a few important trends that have been happening in plain sight: (i) China is catching up to the U.S. in generative AI, with implications for the AI supply chain. (ii) Open weight models are commoditizing the foundation-model layer, which creates opportunities for application builders. (iii) Scaling up isn\u2019t the only path to AI progress. Despite the massive focus on and hype around processing power, algorithmic innovations are rapidly pushing down training costs. About a week ago, DeepSeek, a company based in China, released DeepSeek-R1, a remarkable model whose performance on benchmarks is comparable to OpenAI\u2019s o1. Further, it was released as an open weight model with a permissive MIT license. At Davos last week, I got a lot of questions about it from non-technical business leaders. And on Monday, the stock market saw a \u201cDeepSeek selloff\u201d: The share prices of Nvidia and a number of other U.S. tech companies plunged. (As of the time of writing, they have recovered somewhat.) Here\u2019s what I think DeepSeek has caused many people to realize: China is catching up to the U.S. in generative AI. When ChatGPT was launched in November 2022, the U.S. was significantly ahead of China in generative AI. Impressions change slowly, and so even recently I heard friends in both the U.S. and China say they thought China was behind. But in reality, this gap has rapidly eroded over the past two years. With models from China such as Qwen (which my teams have used for months), Kimi, InternVL, and DeepSeek, China had clearly been closing the gap, and in areas such as video generation there were already moments where China seemed to be in the lead. I\u2019m thrilled that DeepSeek-R1 was released as an open weight model, with a technical report that shares many details. In contrast, a number of U.S. companies have pushed for regulation to stifle open source by hyping up hypothetical AI dangers such as human extinction. It is now clear that open source/open weight models are a key part of the AI supply chain: Many companies will use them. If the U.S. continues to stymie open source, China will come to dominate this part of the supply chain and many businesses will end up using models that reflect China\u2019s values much more than America\u2019s. Open weight models are commoditizing the foundation-model layer. As I wrote previously, LLM token prices have been falling rapidly, and open weights have contributed to this trend and given developers more choice. OpenAI\u2019s o1 costs $60 per million output tokens; DeepSeek R1 costs $2.19. This nearly 30x difference brought the trend of falling prices to the attention of many people. The business of training foundation models and selling API access is tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. The article \u201cAI\u2019s $600B Question\u201d lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In", "article_id": "68497b48235d602d6512255c", "linked_images": ["article_68497b48235d602d6512255c_img_0", "article_68497b48235d602d6512255c_img_1", "article_68497b48235d602d6512255c_img_2", "article_68497b48235d602d6512255c_img_3", "article_68497b48235d602d6512255c_img_4", "article_68497b48235d602d6512255c_img_5"]}, {"type": "text", "text": "tough. Many companies in this area are still looking for a path to recouping the massive cost of model training. The article \u201cAI\u2019s $600B Question\u201d lays out the challenge well (but, to be clear, I think the foundation model companies are doing great work, and I hope they succeed). In contrast, building applications on top of foundation models presents many great business opportunities. Now that others have spent billions training such models, you can access these models for mere dollars to build customer service chatbots, email summarizers, AI doctors, legal document assistants, and much more. Scaling up isn\u2019t the only path to AI progress. There\u2019s been a lot of hype around scaling up models as a way to drive progress. To be fair, I was an early proponent of scaling up models. A number of companies raised billions of dollars by generating buzz around the narrative that, with more capital, they could (i) scale up and (ii) predictably drive improvements. Consequently, there has been a huge focus on scaling up, as opposed to a more nuanced view that gives due attention to the many different ways we can make progress. Driven in part by the U.S. AI chip embargo, the DeepSeek team had to innovate on many optimizations to run on less-capable H800 GPUs rather than H100s, leading ultimately to a model trained (omitting research costs) for under $6M of compute. It remains to be seen if this will actually reduce demand for compute. Sometimes making each unit of a good cheaper can result in more dollars in total going to buy that good. I think the demand for intelligence and compute has practically no ceiling over the long term, so I remain bullish that humanity will use more intelligence even as it gets cheaper. I saw many different interpretations of DeepSeek\u2019s progress on social media, as if it was a Rorschach test that allowed many people to project their own meaning onto it. I think DeepSeek-R1 has geopolitical implications that are yet to be worked out. And it\u2019s also great for AI application builders. My team has already been brainstorming ideas that are newly possible only because we have easy access to an open advanced reasoning model. This continues to be a great time to build! Keep learning, Andrew Discover Anthropic\u2019s new capabilty - Computer Use - that allows LLM-based agents use a computer interface. In this free course, you\u2019ll learn to apply image reasoning and function-calling to \u2018use\u2019 a computer as follows: a model processes an image of the screen, analyzes it to understand what's going on, and navigates the computer via mouse clicks and keystrokes. Start today! Reinforcement learning is emerging as an avenue for building large language models with advanced reasoning capabilities. What\u2019s new: Two recent high-performance models, DeepSeek-R1 (and its variants including DeepSeek-R1-Zero) and Kimi k1.5, learned to improve their generated lines of reasoning via reinforcement learning. o1 pioneered this approach last year. Reinforcement learning (RL) basics: RL rewards or punishes a model for performing particular actions or achieving certain objectives. Unlike supervised and unsupervised learning, which", "article_id": "68497b48235d602d6512255c", "linked_images": ["article_68497b48235d602d6512255c_img_0", "article_68497b48235d602d6512255c_img_1", "article_68497b48235d602d6512255c_img_2", "article_68497b48235d602d6512255c_img_3", "article_68497b48235d602d6512255c_img_4", "article_68497b48235d602d6512255c_img_5"]}, {"type": "text", "text": "DeepSeek-R1 (and its variants including DeepSeek-R1-Zero) and Kimi k1.5, learned to improve their generated lines of reasoning via reinforcement learning. o1 pioneered this approach last year. Reinforcement learning (RL) basics: RL rewards or punishes a model for performing particular actions or achieving certain objectives. Unlike supervised and unsupervised learning, which compare the model's output to a known ground truth, RL doesn\u2019t explicitly tell a model what it should output. Instead, the model starts out behaving randomly and discovers desired behaviors by earning rewards for its actions. This makes RL especially popular for training machine learning models that play games or control robots. How it works: To improve the chain of thought (CoT) generated by a large language model (LLM), reinforcement learning encourages the model to generate correct solutions to math, coding, science, and other problems that have known solutions. Unlike typical LLM training, in which the model simply generates the next token of its output and receives feedback token by token, this method rewards the model for generating a sequence of reasoning steps that lead to an accurate conclusion, even if doing so requires generating many intermediate tokens between the prompt and the response \u2014 to plan an outline, check the conclusion, or reflect on the approach \u2014 without explicit training on the reasoning steps to take. Behind the news: While RL has been a staple technique for training models to play games and control robots, its role in developing LLMs has been confined to alignment with human preferences. Reinforcement learning to match judgements of humans (reinforcement learning from human feedback, or RLHF) or AI (Constitutional AI, which uses reinforcement learning from AI feedback or RLAIF) were the primary methods for encouraging LLMs to align with human preferences prior to the development of direct preference optimization. Why it matters: Reinforcement learning has surprising utility in training large language models to reason. As researchers press models into service in more complex tasks \u2014 math, coding, animated graphics, and beyond \u2014 reinforcement learning is emerging as an important path to progress. We\u2019re thinking: Less than three years ago, reinforcement learning looked too finicky to be worth the trouble. Now it\u2019s a key direction in language modeling. Machine learning continues to be full of surprising twists! OpenAI introduced an AI agent that performs simple web tasks on a user\u2019s behalf. What\u2019s new: Operator automates online actions like buying goods, booking tickets and completing forms by navigating websites in a browser-like environment within ChatGPT. It\u2019s available on desktops as a research preview for subscribers to ChatGPT Pro ($200 per month). OpenAI promises broader availability to come as well as API access to the underlying model and improved ability to coordinate multi-step tasks like scheduling meetings across calendars from different vendors. How it works: Operator uses a new model called Computer-Using Agent (CUA) that accepts text input and responds with web actions. Behind the news: Operator rides a wave of agents designed to automate everyday tasks. Last week, OpenAI introduced ChatGPT Tasks, which lets users schedule reminders and alerts but doesn\u2019t support web interaction. (Early users complained", "article_id": "68497b48235d602d6512255c", "linked_images": ["article_68497b48235d602d6512255c_img_0", "article_68497b48235d602d6512255c_img_1", "article_68497b48235d602d6512255c_img_2", "article_68497b48235d602d6512255c_img_3", "article_68497b48235d602d6512255c_img_4", "article_68497b48235d602d6512255c_img_5"]}, {"type": "text", "text": "new model called Computer-Using Agent (CUA) that accepts text input and responds with web actions. Behind the news: Operator rides a wave of agents designed to automate everyday tasks. Last week, OpenAI introduced ChatGPT Tasks, which lets users schedule reminders and alerts but doesn\u2019t support web interaction. (Early users complained that Tasks was buggy and required overly precise instructions.) Anthropic\u2019s Computer Use focuses on basic desktop automation, while DeepMind\u2019s Project Mariner is a web-browsing assistant built on Gemini 2.0. Perplexity Assistant automates mobile apps such as booking Uber rides on Android phones. Why it matters: In early reports, users said Operator sometimes was less efficient than a human performing the same tasks. Nevertheless, agentic AI is entering the consumer market, and Operator is poised to give many people their first taste. It\u2019s geared to provide AI assistance for an endless variety of personal and business uses, and \u2014 like ChatGPT was for other developers of LLMs \u2014 and it\u2019s bound to serve as a template for next-generation products. We\u2019re thinking: Computer use is maturing, and the momentum behind it is palpable. AI developers should have in their toolbox. Under a new president, the United States reversed its approach to AI regulation, seeking global dominance by reducing restrictions. What\u2019s new: President Trump, who took office last week, signed an executive order that set a 180-day deadline to draft an AI Action Plan. The order aims to boost national security, economic competitiveness, and U.S. leadership in artificial intelligence. How it works: The executive order assigns responsibility for crafting the AI Action Plan to three key figures in the administration: Michael Kratsios, assistant to the president for science and technology (and former managing director of Scale AI); venture capitalist David Sacks, the new special advisor for AI and cryptocurrency; and national security advisor Michael Waltz. AI infrastructure build-out: Along with the executive order, President Trump announced Stargate, a joint venture that involves OpenAI, Oracle, and SoftBank. The three companies outlined a plan to invest $100 billion in computing infrastructure for AI, such as next-generation data centers, and $500 billion over four years. In addition, the administration declared a national energy emergency with respect to U.S. supplies of energy and issued an order to ramp up domestic energy production. These measures aim to support energy-intensive AI initiatives like Stargate by removing regulatory barriers to building oil, gas, and renewable energy projects on federal lands. Why it matters: The Trump administration says that Biden\u2019s 2023 regulations were \u201conerous and unnecessary,\u201d stifled innovation, and jeopardized U.S. leadership in AI. The new order reduces bureaucratic oversight of AI development, creating a more permissive regulatory environment (except when it comes to ideological bias). We\u2019re thinking: The Biden administration\u2019s 2023 executive order aimed to guard against hypothetical, rather than actual, AI risks. It introduced thresholds of processing used to train models as a measure of their risk \u2014 a poorly thought-out proxy. To be fair, the AI Safety Institute under the U.S. National Institute of Standards and Technology didn\u2019t hamper AI progress as much as some had feared, but overall the", "article_id": "68497b48235d602d6512255c", "linked_images": ["article_68497b48235d602d6512255c_img_0", "article_68497b48235d602d6512255c_img_1", "article_68497b48235d602d6512255c_img_2", "article_68497b48235d602d6512255c_img_3", "article_68497b48235d602d6512255c_img_4", "article_68497b48235d602d6512255c_img_5"]}, {"type": "text", "text": "risks. It introduced thresholds of processing used to train models as a measure of their risk \u2014 a poorly thought-out proxy. To be fair, the AI Safety Institute under the U.S. National Institute of Standards and Technology didn\u2019t hamper AI progress as much as some had feared, but overall the order was not helpful to AI innovation or safety. We\u2019re pleased that the new administration is focusing on AI progress rather than hypothetical risks. The practice of fine-tuning models on synthetic data is becoming well established. But synthetic training data, even if it represents the training task well, may include characteristics like toxicity that impart unwelcome properties in the trained model\u2019s output, and it may inconsistently represent desired traits such as the target output length. Researchers developed a method that reduces aspects of generated data and retains desired ones. What\u2019s new: Lu\u00edsa Shimabucoro and colleagues at Cohere introduced active inheritance, a fine-tuning method that automatically selects synthetic training examples that have desirable characteristics. Key insight: A naive way to generate synthetic fine-tuning data is to feed prompts to a model, collect its output, and use that as the fine-tuning set. But synthetic data is cheap, so we can afford to be more choosy. By generating several responses to each prompt, we can select the one that best suits our purposes. How it works: The authors used Llama 2 7B and Mixtral 8x7B as both teachers and students in all combinations. They prompted the models with 52,000 prompts from the Alpaca dataset and used automated methods to evaluate their outputs in terms of characteristics including social bias, toxicity, word count, lexical diversity, and calibration (how well a model\u2019s estimated probabilities match its accuracy). Results: Fine-tuning on the best response for each characteristic improved performance with respect to that characteristic beyond using the initial outputs or selecting outputs randomly. Why it matters: Training on synthetic data is becoming increasingly common. While it shows great promise, best practices for data generation are still being formulated. The authors\u2019 method helps by automatically steering models toward generating more desirable responses, reducing negative traits and reinforcing positive traits. We\u2019re thinking: Knowledge distillation lately has led to more capable and compact models. This approach adds levers of fine control to that technique.", "article_id": "68497b48235d602d6512255c", "linked_images": ["article_68497b48235d602d6512255c_img_0", "article_68497b48235d602d6512255c_img_1", "article_68497b48235d602d6512255c_img_2", "article_68497b48235d602d6512255c_img_3", "article_68497b48235d602d6512255c_img_4", "article_68497b48235d602d6512255c_img_5"]}, {"type": "image", "article_id": "68497b48235d602d6512255c", "linked_chunks": ["article_68497b48235d602d6512255c_chunk_0", "article_68497b48235d602d6512255c_chunk_1", "article_68497b48235d602d6512255c_chunk_2", "article_68497b48235d602d6512255c_chunk_3", "article_68497b48235d602d6512255c_chunk_4"], "alt_text": "Blue whale logo biting and breaking a computer chip, with debris flying."}, {"type": "image", "article_id": "68497b48235d602d6512255c", "linked_chunks": ["article_68497b48235d602d6512255c_chunk_0", "article_68497b48235d602d6512255c_chunk_1", "article_68497b48235d602d6512255c_chunk_2", "article_68497b48235d602d6512255c_chunk_3", "article_68497b48235d602d6512255c_chunk_4"], "alt_text": "Promo banner for \"Building Towards Computer Use with Anthropic\""}, {"type": "image", "article_id": "68497b48235d602d6512255c", "linked_chunks": ["article_68497b48235d602d6512255c_chunk_0", "article_68497b48235d602d6512255c_chunk_1", "article_68497b48235d602d6512255c_chunk_2", "article_68497b48235d602d6512255c_chunk_3", "article_68497b48235d602d6512255c_chunk_4"], "alt_text": "Diagram of a reinforcement learning system for training LLMs, showing data and weight flow processes."}, {"type": "image", "article_id": "68497b48235d602d6512255c", "linked_chunks": ["article_68497b48235d602d6512255c_chunk_0", "article_68497b48235d602d6512255c_chunk_1", "article_68497b48235d602d6512255c_chunk_2", "article_68497b48235d602d6512255c_chunk_3", "article_68497b48235d602d6512255c_chunk_4"], "alt_text": "AI assistant processes \u2018Find me a family-friendly campsite\u2019 and suggests options."}, {"type": "image", "article_id": "68497b48235d602d6512255c", "linked_chunks": ["article_68497b48235d602d6512255c_chunk_0", "article_68497b48235d602d6512255c_chunk_1", "article_68497b48235d602d6512255c_chunk_2", "article_68497b48235d602d6512255c_chunk_3", "article_68497b48235d602d6512255c_chunk_4"], "alt_text": "Front view of the White House with a fountain, green lawn, and the U.S. flag flying on top."}, {"type": "image", "article_id": "68497b48235d602d6512255c", "linked_chunks": ["article_68497b48235d602d6512255c_chunk_0", "article_68497b48235d602d6512255c_chunk_1", "article_68497b48235d602d6512255c_chunk_2", "article_68497b48235d602d6512255c_chunk_3", "article_68497b48235d602d6512255c_chunk_4"], "alt_text": "Bar chart comparing active vs. random sampling effects on length, diversity, and toxicity after fine-tuning."}, {"type": "alt_text", "alt_text": "Blue whale logo biting and breaking a computer chip, with debris flying."}, {"type": "alt_text", "alt_text": "Promo banner for \"Building Towards Computer Use with Anthropic\""}, {"type": "alt_text", "alt_text": "Diagram of a reinforcement learning system for training LLMs, showing data and weight flow processes."}, {"type": "alt_text", "alt_text": "AI assistant processes \u2018Find me a family-friendly campsite\u2019 and suggests options."}, {"type": "alt_text", "alt_text": "Front view of the White House with a fountain, green lawn, and the U.S. flag flying on top."}, {"type": "alt_text", "alt_text": "Bar chart comparing active vs. random sampling effects on length, diversity, and toxicity after fine-tuning."}, {"type": "text", "text": "Dear friends, Greetings from Davos, Switzerland! Many business and government leaders are gathered here again for the annual World Economic Forum to discuss tech, climate, geopolitics, and economic growth. While the vast majority of my conversations have been on AI business implementations and governance, I have also been speaking about our latest AI climate simulator and about geoengineering. After speaking about geoengineering onstage at multiple events to a total of several hundred people, I\u2019ve been pleasantly surprised by almost uniformly positive reactions. You can play with our simulator here. Here\u2019s why I think we should seriously consider geoengineering: The world urgently needs to reduce carbon emissions, but it hasn\u2019t happened fast enough. Given recent emission trends, without geoengineering, there\u2019s no longer any plausible path to keeping global warming to the 1.5 degrees Celsius goal set by the Paris agreement. Under reasonable assumptions, we are on a path to 2.5 degrees of warming or worse. We might be in for additional abrupt changes if we hit certain tipping points. If you tilt a four-legged chair by a few degrees, it will fall back onto its four legs. But if you tip it far enough \u2014 beyond its \u201ctipping point\u201d \u2014 it will fall over with a crash. Climate tipping points are like that, where parts of our planet, warmed sufficiently, might reach a point where the planet reorganizes abruptly in a way that is impossible to reverse. Examples include a possible melting of the Arctic permafrost, which would release additional methane (a potent greenhouse gas), or a collapse of ocean currents that move warm water northward from the tropics (the Atlantic Meridional Overturning Circulation). Keeping warming low will significantly lower the risk of hitting a tipping point. This is why the OECD\u2019s report states, \u201cthe existence of climate system tipping points means it is vital to limit the global temperature increase to 1.5 degrees C, with no or very limited overshoot.\u201d The good news is that geoengineering keeps the 1.5 degree goal alive. Spraying reflective particles into the atmosphere \u2014 an idea called Stratospheric Aerosol Injection (SAI) \u2014 to reflect 1% of sunlight back into space would get us around 1 degree Celsius of cooling. Now, there are risks to doing this. For example, just as global warming has had uneven regional effects, the global cooling impact will also be uneven. But on average, a planet with 1.5 degrees of warming would be much more livable than one with 2.5 degrees (or more). Further, after collaborating extensively with climate scientists on AI climate models and examining the output of multiple such models, I believe the risks associated with cooling down our planet will be much lower than the risks of runaway climate change. I hope we can build a global governance structure to decide collectively whether, and if so to what extent and how, to implement geoengineering. For example, we might start with small scale experiments (aiming for <<0.1 degrees of cooling) that are easy to stop/reverse at any time. Further, there is much work to be done to solve difficult engineering challenges,", "article_id": "68497b48235d602d65122563", "linked_images": ["article_68497b48235d602d65122563_img_0", "article_68497b48235d602d65122563_img_1", "article_68497b48235d602d65122563_img_2", "article_68497b48235d602d65122563_img_3", "article_68497b48235d602d65122563_img_4", "article_68497b48235d602d65122563_img_5"]}, {"type": "text", "text": "decide collectively whether, and if so to what extent and how, to implement geoengineering. For example, we might start with small scale experiments (aiming for <<0.1 degrees of cooling) that are easy to stop/reverse at any time. Further, there is much work to be done to solve difficult engineering challenges, such as how to build and operate a fleet of aircraft to efficiently lift and spray reflective particles at the small particle sizes needed. Even as I have numerous conversations about AI business and governance here at the World Economic Forum, I am glad that AI climate modeling is helpful for addressing global warming. If you are interested in learning more about geoengineering, I encourage you to play with our simulator at planetparasol.ai. I am grateful to my collaborators on the simulator work: Jeremy Irvin, Jake Dexheimer, Dakota Gruener, Charlotte DeWald, Daniele Visioni, Duncan Watson-Parris, Douglas MacMartin, Joshua Elliott, Juerg Luterbacher, and Kion Yaghoobzadeh. Keep learning! Andrew Explore Computer Use, which enables AI assistants to navigate, use, and accomplish tasks on computers. Taught by Colt Steele, this free course covers Anthropic\u2019s model family, its approach to AI research, and capabilities like multimodal prompts and prompt caching. Sign up for free A new open model rivals OpenAI\u2019s o1, and it\u2019s free to use or modify. What\u2019s new: DeepSeek released DeepSeek-R1, a large language model that executes long lines of reasoning before producing output. The code and weights are licensed freely for commercial and personal use, including training new models on R1 outputs. The paper provides an up-close look at the training of a high-performance model that implements a chain of thought without explicit prompting. (DeepSeek-R1-lite-preview came out in November with fewer parameters and a different base model.) Mixture of experts (MoE) basics: The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input. How it works: DeepSeek-R1 is a version of DeepSeek-V3-Base that was fine-tuned over four stages to enhance its ability to process a chain of thought (CoT). It\u2019s a mixture-of-experts transformer with 671 billion total parameters, 37 billion of which are active at any given time, and it processes 128,000 tokens of input context. Access to the model via DeepSeek\u2019s API costs $0.55 per million input tokens ($0.14 for cached inputs) and $2.19 per million output tokens. (In comparison, o1 costs $15 per million input tokens, $7.50 for cached inputs, and $60 per million output tokens.) Other models: DeepSeek researchers also released seven related models. Results: In DeepSeek\u2019s tests, DeepSeek-R1 went toe-to-toe with o1, outperforming that model on 5 of 11 of the benchmarks tested. Some of the other new models showed competitive performance, too. Why", "article_id": "68497b48235d602d65122563", "linked_images": ["article_68497b48235d602d65122563_img_0", "article_68497b48235d602d65122563_img_1", "article_68497b48235d602d65122563_img_2", "article_68497b48235d602d65122563_img_3", "article_68497b48235d602d65122563_img_4", "article_68497b48235d602d65122563_img_5"]}, {"type": "text", "text": "$7.50 for cached inputs, and $60 per million output tokens.) Other models: DeepSeek researchers also released seven related models. Results: In DeepSeek\u2019s tests, DeepSeek-R1 went toe-to-toe with o1, outperforming that model on 5 of 11 of the benchmarks tested. Some of the other new models showed competitive performance, too. Why it matters: Late last year, OpenAI\u2019s o1 kicked off a trend toward so-called reasoning models that implement a CoT without explicit prompting. But o1 and o3, its not-yet-widely-available successor, hide their reasoning steps. In contrast, DeepSeek-R1 bares all, allowing users to see the steps the model took to arrive at a particular answer. DeepSeek\u2019s own experiments with distillation show how powerful such models can be as teachers to train smaller student models. Moreover, they appear to pass along some of the benefits of their reasoning skills, making their students more accurate. We\u2019re thinking: DeepSeek is rapidly emerging as a strong builder of open models. Not only are these models great performers, but their license permits use of their outputs for distillation, potentially pushing forward the state of the art for language models (and multimodal models) of all sizes. Chinese robot makers Unitree and EngineAI showed off relatively low-priced humanoid robots that could bring advanced robotics closer to everyday applications. What\u2019s new: At the annual Consumer Electronics Show (CES) in Las Vegas, Unitree showed its G1 ($16,000 with three-finger hands, $21,000 with five-finger, articulated hands), which climbed stairs and navigated around obstacles. Elsewhere on the show floor, EngineAI\u2019s PM01 ($13,700 through March 2025 including articulated hands) and SE01 (price not yet disclosed) marched among attendees with notably naturalistic gaits. How it works: Relatively small and lightweight, these units are designed for household and small-business uses. They\u2019re designed for general-purpose tasks and to maintain stability and balance while walking on varied terrain. Behind the news: In contrast to the more-affordable humanoid robots coming out of China, U.S. companies like Boston Dynamics, Figure AI, and Tesla tend to cater to industrial customers. Tesla plans to produce several thousand of its Optimus ($20,000 to $30,000) humanoids in 2025, ramping to as many as 100,000 in 2026. Figure AI has demonstrated its Figure 02 ($59,000) in BMW manufacturing plants, showing a 400 percent speed improvement in some tasks. At CES, Nvidia unveiled its GR00T Blueprint, which includes vision-language models and synthetic data for training humanoid robots, and said its Jetson Thor computer for humanoids would be available early 2025. Why it matters: China\u2019s push into humanoid robotics reflects its broader national ambitions. Its strength in hardware has allowed it to establish a dominant position in drones, and humanoid robots represent a new front for competition. China\u2019s government aims to achieve mass production of humanoid robots by 2025 and establish global leadership by 2027, partly to address projected labor shortages of 30 million workers in manufacturing alone. Lower price points for robots that can perform arbitrary tasks independently could be valuable in elder care and logistics, offering tools for repetitive or physically demanding tasks. We\u2019re thinking: Although humanoid robots generate a lot of excitement, they\u2019re still in an", "article_id": "68497b48235d602d65122563", "linked_images": ["article_68497b48235d602d65122563_img_0", "article_68497b48235d602d65122563_img_1", "article_68497b48235d602d65122563_img_2", "article_68497b48235d602d65122563_img_3", "article_68497b48235d602d65122563_img_4", "article_68497b48235d602d65122563_img_5"]}, {"type": "text", "text": "labor shortages of 30 million workers in manufacturing alone. Lower price points for robots that can perform arbitrary tasks independently could be valuable in elder care and logistics, offering tools for repetitive or physically demanding tasks. We\u2019re thinking: Although humanoid robots generate a lot of excitement, they\u2019re still in an early stage of development, and businesses are still working to identify and prove concrete use cases. For many industrial applications, wheeled robots \u2014 which are less expensive, more stable, and better able to carry heavy loads \u2014 will remain a sensible choice. But the prospect of machines that look like us and fit easily into environments built for us is compelling. Lawmakers in the U.S. state of Texas are considering stringent AI regulation. What\u2019s new: The Texas legislature is considering the proposed Texas Responsible AI Governance Act (TRAIGA). The bill would prohibit a short list of harmful or invasive uses of AI, such as output intended to manipulate users. It would impose strict oversight on AI systems that contribute to decisions in key areas like health care. How it works: Republican House Representative Giovanni Capriglione introduced TRAIGA, also known as HB 1709, to the state legislature at the end of 2024. If it\u2019s passed and signed, the law would go into effect in September 2025. Sandbox: A \u201csandbox\u201d provision would allow registered AI developers to test and refine AI systems temporarily with fewer restrictions. Developers who registered AI projects with the Texas AI Council would gain temporary immunity, even if their systems did not fully comply with the law. However, this exemption would come with conditions: Developers must submit detailed reports on their projects\u2019 purposes, risks, and mitigation plans. The sandbox status would be in effect for 36 months (with possible extensions), and organizations would have to bring their systems into compliance or decommission them once the period ends. The Texas AI Council could revoke sandbox protections if it determined that a project posed a risk of public harm or failed to meet reporting obligations. Behind the news: Other U.S. states, too, are considering or have already passed laws that regulate AI: Why it matters: AI is not specifically regulated at the national level in the United States. This leaves individual states free to formulate their own laws. However, state-by-state regulation risks a patchwork of laws in which a system \u2014 or a particular feature \u2014 may be legal in some states but not others. Moreover, given the distributed nature of AI development and deployment, a law that governs AI in an individual state could affect developers and users worldwide. We\u2019re thinking: The proposed bill has its positive aspects, particularly insofar as it seeks to restrict harmful applications rather than the underlying technology. However, it imposes burdensome requirements for compliance, suffers from overly broad language, fails to adequately protect open source, and doesn\u2019t distinguish between research and commercial use. Beyond that, state-by-state regulation of AI is not workable. On the contrary, AI demands international conventions and standards. Designing integrated circuits typically requires years of human expertise. Recent work set AI to the", "article_id": "68497b48235d602d65122563", "linked_images": ["article_68497b48235d602d65122563_img_0", "article_68497b48235d602d65122563_img_1", "article_68497b48235d602d65122563_img_2", "article_68497b48235d602d65122563_img_3", "article_68497b48235d602d65122563_img_4", "article_68497b48235d602d65122563_img_5"]}, {"type": "text", "text": "overly broad language, fails to adequately protect open source, and doesn\u2019t distinguish between research and commercial use. Beyond that, state-by-state regulation of AI is not workable. On the contrary, AI demands international conventions and standards. Designing integrated circuits typically requires years of human expertise. Recent work set AI to the task with surprising results. What\u2019s new: Emir Ali Karahan, Zheng Liu, Aggraj Gupta, and colleagues at Princeton and Indian Institute of Technology Madras used deep learning and an evolutionary algorithm, which generates variations and tests their fitness, to generate designs for antennas, filters, power splitters, resonators, and other chips with applications in wireless communications and other applications. They fabricated a handful of the generated designs and found they worked \u2014 but in mysterious ways. How it works: The authors trained convolutional neural networks (CNNs), given a binary image of a circuit design (in which each pixel represents whether the corresponding portion of a semiconductor surface is raised or lowered), to predict its electromagnetic scattering properties and radiative properties. Based on this simulation, they generated new binary circuit images using evolution. Results: The authors fabricated some of the designs to test their real-world properties. The chips showed similar performance than the CNNs had predicted. The authors found the designs themselves baffling; they \u201cdelivered stunning high-performances devices that ran counter to the usual rules of thumb and human intuition,\u201d co-author Uday Khankhoje told the tech news site Tech Xplore. Moreover, the design process was faster than previous approaches. The authors\u2019 method designed a 300x300 micrometer chip in approximately 6 minutes. Using traditional methods it would have taken 21 days. Behind the news: Rather than wireless chips, Google has used AI to accelerate design of the Tensor Processing Units that process neural networks in its data centers. AlphaChip used reinforcement learning to learn how to position chip components such as SRAM and logic gates on silicon. Why it matters: Designing circuits usually requires rules of thumb, templates, and hundreds of hours of simulations and experiments to determine the best design. AI can cut the required expertise and time and possibly find effective designs that wouldn\u2019t occur to human designers. We\u2019re thinking: AI-generated circuit designs could help circuit designers to break out of set ways of thinking and discover new design principles.", "article_id": "68497b48235d602d65122563", "linked_images": ["article_68497b48235d602d65122563_img_0", "article_68497b48235d602d65122563_img_1", "article_68497b48235d602d65122563_img_2", "article_68497b48235d602d65122563_img_3", "article_68497b48235d602d65122563_img_4", "article_68497b48235d602d65122563_img_5"]}, {"type": "image", "article_id": "68497b48235d602d65122563", "linked_chunks": ["article_68497b48235d602d65122563_chunk_0", "article_68497b48235d602d65122563_chunk_1", "article_68497b48235d602d65122563_chunk_2", "article_68497b48235d602d65122563_chunk_3", "article_68497b48235d602d65122563_chunk_4"], "alt_text": "Global temperature change map and graph comparing scenarios with and without SAI intervention."}, {"type": "image", "article_id": "68497b48235d602d65122563", "linked_chunks": ["article_68497b48235d602d65122563_chunk_0", "article_68497b48235d602d65122563_chunk_1", "article_68497b48235d602d65122563_chunk_2", "article_68497b48235d602d65122563_chunk_3", "article_68497b48235d602d65122563_chunk_4"], "alt_text": "Promo banner for \"Building Towards Computer Use with Anthropic\""}, {"type": "image", "article_id": "68497b48235d602d65122563", "linked_chunks": ["article_68497b48235d602d65122563_chunk_0", "article_68497b48235d602d65122563_chunk_1", "article_68497b48235d602d65122563_chunk_2", "article_68497b48235d602d65122563_chunk_3", "article_68497b48235d602d65122563_chunk_4"], "alt_text": "Bar chart comparing accuracy and percentile scores of DeepSeek models and OpenAI models across benchmarks."}, {"type": "image", "article_id": "68497b48235d602d65122563", "linked_chunks": ["article_68497b48235d602d65122563_chunk_0", "article_68497b48235d602d65122563_chunk_1", "article_68497b48235d602d65122563_chunk_2", "article_68497b48235d602d65122563_chunk_3", "article_68497b48235d602d65122563_chunk_4"], "alt_text": "GIF of two humanoid robots walking, one on grass and the other on a paved surface."}, {"type": "image", "article_id": "68497b48235d602d65122563", "linked_chunks": ["article_68497b48235d602d65122563_chunk_0", "article_68497b48235d602d65122563_chunk_1", "article_68497b48235d602d65122563_chunk_2", "article_68497b48235d602d65122563_chunk_3", "article_68497b48235d602d65122563_chunk_4"], "alt_text": "A bull in a grassy field with a neural network diagram integrated into its horns."}, {"type": "image", "article_id": "68497b48235d602d65122563", "linked_chunks": ["article_68497b48235d602d65122563_chunk_0", "article_68497b48235d602d65122563_chunk_1", "article_68497b48235d602d65122563_chunk_2", "article_68497b48235d602d65122563_chunk_3", "article_68497b48235d602d65122563_chunk_4"], "alt_text": "Workflow for inverse design using deep learning to predict S-parameters and radiation in structures."}, {"type": "alt_text", "alt_text": "Global temperature change map and graph comparing scenarios with and without SAI intervention."}, {"type": "alt_text", "alt_text": "Promo banner for \"Building Towards Computer Use with Anthropic\""}, {"type": "alt_text", "alt_text": "Bar chart comparing accuracy and percentile scores of DeepSeek models and OpenAI models across benchmarks."}, {"type": "alt_text", "alt_text": "GIF of two humanoid robots walking, one on grass and the other on a paved surface."}, {"type": "alt_text", "alt_text": "A bull in a grassy field with a neural network diagram integrated into its horns."}, {"type": "alt_text", "alt_text": "Workflow for inverse design using deep learning to predict S-parameters and radiation in structures."}, {"type": "text", "text": "Dear friends, Writing software, especially prototypes, is becoming cheaper. This will lead to increased demand for people who can decide what to build. AI Product Management has a bright future! Software is often written by teams that comprise Product Managers (PMs), who decide what to build (such as what features to implement for what users) and Software Developers, who write the code to build the product. Economics shows that when two goods are complements \u2014 such as cars (with internal-combustion engines) and gasoline \u2014 falling prices in one leads to higher demand for the other. For example, as cars became cheaper, more people bought them, which led to increased demand for gas. Something similar will happen in software. Given a clear specification for what to build, AI is making the building itself much faster and cheaper. This will significantly increase demand for people who can come up with clear specs for valuable things to build. This is why I\u2019m excited about the future of Product Management, the discipline of developing and managing software products. I\u2019m especially excited about the future of AI Product Management, the discipline of developing and managing AI software products. Many companies have an Engineer:PM ratio of, say, 6:1. (The ratio varies widely by company and industry, and anywhere from 4:1 to 10:1 is typical.) As coding becomes more efficient, I think teams will need more product management work (as well as design work) as a fraction of the total workforce. Perhaps engineers will step in to do some of this work, but if it remains the purview of specialized Product Managers, then the demand for these roles will grow. This change in the composition of software development teams is not yet moving forward at full speed. One major force slowing this shift, particularly in AI Product Management, is that Software Engineers, being technical, are understanding and embracing AI much faster than Product Managers. Even today, most companies have difficulty finding people who know how to develop products and also understand AI, and I expect this shortage to grow. Further, AI Product Management requires a different set of skills than traditional software Product Management. It requires: Finally, AI Product Managers will need to know how to ensure that AI is implemented responsibly (for example, when we need to implement guardrails to prevent bad outcomes), and also be skilled at gathering feedback fast to keep projects moving. Increasingly, I also expect strong product managers to be able to build prototypes for themselves. The demand for good AI Product Managers will be huge. In addition to growing AI Product Management as a discipline, perhaps some engineers will also end up doing more product management work. The variety of valuable things we can build is nearly unlimited. What a great time to build! Keep learning, Andrew Get up-close and personal with OpenAI\u2019s groundbreaking o1 model! In our short course \u201cReasoning with o1,\u201d you\u2019ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting. Enroll today A new model from", "article_id": "68497b48235d602d6512256a", "linked_images": ["article_68497b48235d602d6512256a_img_0", "article_68497b48235d602d6512256a_img_1", "article_68497b48235d602d6512256a_img_2", "article_68497b48235d602d6512256a_img_3", "article_68497b48235d602d6512256a_img_4", "article_68497b48235d602d6512256a_img_5"]}, {"type": "text", "text": "build! Keep learning, Andrew Get up-close and personal with OpenAI\u2019s groundbreaking o1 model! In our short course \u201cReasoning with o1,\u201d you\u2019ll learn how to get the best performance in coding, planning, and STEM tasks; perform complex, multi-step tasks; and optimize prompts with meta prompting. Enroll today A new model from Hangzhou upstart DeepSeek delivers outstanding performance and may change the equation for training costs. What\u2019s new: DeepSeek-V3 is an open large language model that outperforms Llama 3.1 405B and GPT-4o on key benchmarks and achieves exceptional scores in coding and math. The weights are open except for applications that involve military uses, harming minors, generating false information, and similar restrictions. You can download them here. Mixture of experts (MoE) basics: The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input. How it works: DeepSeek-V3 is a mixture-of-experts (MoE) transformer that comprises 671 billion parameters, of which 37 billion are active at any moment. The team trained the model in 2.79 million GPU hours \u2014 less than 1/10 the time required to train Llama 3.1 405B, which DeepSeek-V3 substantially outperforms \u2014 at an extraordinarily low cost of $5.6 million. Results: In DeepSeek\u2019s tests, DeepSeek-V3 outperformed Llama 3.1 405B and Qwen 2.5 72B across the board, and its performance compared favorably with that of GPT-4o. Behind the news: OpenAI\u2019s o1 models excel thanks to agentic workflows in which they reflect on their own outputs, use tools, and so on. DeepSeek swims against the tide and achieves superior results without relying on agentic workflows. Why it matters: Open models continue to challenge closed models, giving developers high-quality options that they can modify and deploy at will. But the larger story is DeepSeek-V3\u2019s shockingly low training cost. The team doesn\u2019t explain precisely how the model achieves outstanding performance with such a low processing budget. (The paper credits \u201cmeticulous engineering optimizations.\u201d) But it\u2019s likely that DeepSeek\u2019s steady refinement of MoE is a key factor. DeepSeek-V2, also an MoE model, saved more than 40 percent in training versus the earlier DeepSeek 67B, which didn\u2019t employ MoE. In 2022, Microsoft found that MoE cost five times less in training for equal performance compared to a dense model, and Google and Meta reported that MoE achieved better performance than dense models trained on the same numbers of tokens. We\u2019re thinking: If they can be replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically. The United States", "article_id": "68497b48235d602d6512256a", "linked_images": ["article_68497b48235d602d6512256a_img_0", "article_68497b48235d602d6512256a_img_1", "article_68497b48235d602d6512256a_img_2", "article_68497b48235d602d6512256a_img_3", "article_68497b48235d602d6512256a_img_4", "article_68497b48235d602d6512256a_img_5"]}, {"type": "text", "text": "replicated, DeepSeek\u2019s results have significant implications for the economics of training foundation models. If indeed it now costs around $5 million to build a GPT-4o-level model, more teams will be able to train such models, and the cost of competing with the AI giants could fall dramatically. The United States proposed limits on exports of AI technology that would dramatically expand previous restrictions, creating a new international hierarchy for access to advanced chips and models. What\u2019s new: The Biden administration, which will transition to leadership under incoming President Trump next week, issued new rules that restrict exports of AI chips and models to most countries beyond a select group of close allies. The rules, which are not yet final, would create a three-tier system that limits exports to a number of close allies and blocks access entirely to China, Iran, North Korea, Russia, and others. They also would introduce the U.S.\u2019 first-ever restrictions on exporting closed weights for large AI models. How it works: The restrictions were announced shortly after a leak reached the press. A public comment period of 120 days will enable the incoming U.S. Presidential administration to consider input from the business and diplomatic communities and modify the rules before they take effect. The rules are scheduled to take effect in one year. Behind the news: The proposed rules build on 2022\u2019s CHIPS and Science Act, which was designed to strengthen domestic semiconductor production and restrict technologies abroad that could bear on U.S. security. An initial round of restrictions in late 2022 barred semiconductor suppliers AMD and Nvidia from selling advanced chips to Chinese firms. In November 2024, the U.S. tightened restrictions further, ordering Taiwan Semiconductor Manufacturing Company, which fabricates those chips, to halt production of advanced chips destined for China. Plus green AI infrastructure: In addition, President Biden issued an executive order to encourage the rapid build-out of computing infrastructure for AI. The federal government will hold competitions among private companies to lease sites it owns for the building of data centers at private expense. The selection of sites will take into account availability of sources of clean energy, including support for nuclear energy. The government will expedite permitting on these sites and support development of energy transmission lines around them. It will also encourage international allies to invest in AI infrastructure powered by clean energy. Why it matters: Protecting the United States\u2019 advantages in high tech has been a rising priority for the White House over the past decade. The earlier export restrictions forced many Chinese AI developers to rely on less-powerful hardware. The new limits are likely to have a far broader impact. They could force developers in the Tier 2 and Tier 3 countries to build less resource-intensive models and lead them to collaborate more closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which have warned that the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations to reconsider their plans.", "article_id": "68497b48235d602d6512256a", "linked_images": ["article_68497b48235d602d6512256a_img_0", "article_68497b48235d602d6512256a_img_1", "article_68497b48235d602d6512256a_img_2", "article_68497b48235d602d6512256a_img_3", "article_68497b48235d602d6512256a_img_4", "article_68497b48235d602d6512256a_img_5"]}, {"type": "text", "text": "closely with each other, reducing the value of U.S.-made technology worldwide. They could hurt U.S. chip vendors, which have warned that the rules could weaken U.S. competitiveness in the global economy. They could also force companies that are building huge data centers to process AI calculations to reconsider their plans. We\u2019re thinking: The Biden administration\u2019s embargo on AI chips has been leaky. So far, it has slowed down adversaries only slightly while spurring significant investment in potential suppliers that aren\u2019t connected to the U.S. While the public comment period invites lobbying and industry feedback, ultimately geopolitical priorities may hold sway. Whatever the outcome, reducing the world\u2019s dependence on U.S. chips and models would result in a very different global AI ecosystem. Nvidia\u2019s new desktop computer is built specifically to run large AI models. What\u2019s new: Project Digits is a personal supercomputer intended to help developers fine-tune and run large models locally. Project Digits, which is small enough to hold in one hand, will be available in May, starting at $3000. How it works: Project Digits is designed to run models of up to 200 billion parameters \u2014 roughly five times the size that fits comfortably on typical consumer hardware \u2014 provided they\u2019re quantized to 4 bits of precision. Two units can be connected to run models such as Meta\u2019s Llama 3.1 405B. Complete specifications are not yet available. Behind the news: In a blitz of announcements at the Consumer Electronics Show (CES), Nvidia also launched a platform for developing robotics, autonomous vehicles, and other physical AI systems. Cosmos includes pretrained language and vision models that range from 4 billion to 14 billion parameters for generating synthetic training data for robots or building policy models that translate a robot\u2019s state into its next action. Nvidia also released Cosmos Nemotron, a 34 billion-parameter, vision-language model designed for use by AI agents, plus a video tokenizer and other tools for robotics developers. Why it matters: It\u2019s common to train models on Nvidia A100 or H100 GPUs, which come with a price tag of at least $8,000 or $20,000 respectively, along with 40 gigabytes to 80 gigabytes of memory. These hefty requirements push many developers to buy access to computing infrastructure from a cloud provider. Coming in at $3,000 with 128 gigabytes of memory, Project Digits is designed to empower machine learning engineers to train and run larger models on their own machines. We\u2019re thinking: We look forward to seeing cost/throughput comparisons between running a model on Project Digits, A100, and H100. Contrastive loss functions make it possible to produce good embeddings without labeled data. A twist on this idea makes even more useful embeddings. What\u2019s new: Vlad Sobal and colleagues at Meta, New York University, Brown University, Genentech, and Canadian Institute for Advanced Research introduced X-Sample contrastive loss (X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety. Key insight: Contrastive loss functions like SimCLR equally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a", "article_id": "68497b48235d602d6512256a", "linked_images": ["article_68497b48235d602d6512256a_img_0", "article_68497b48235d602d6512256a_img_1", "article_68497b48235d602d6512256a_img_2", "article_68497b48235d602d6512256a_img_3", "article_68497b48235d602d6512256a_img_4", "article_68497b48235d602d6512256a_img_5"]}, {"type": "text", "text": "X-Sample contrastive loss (X-CLR), a self-supervised loss function that enables vision models to learn embeddings that capture similarities and differences among examples with greater subtlety. Key insight: Contrastive loss functions like SimCLR equally encourage a model to produce dissimilar embeddings of images of, say, a cat, a dog, and a dump truck. But, of course, cats and dogs are more similar to each other than either are to dump trucks. Instead of marking examples as similar or dissimilar, X-CLR assigns similarity scores, so a model can learn to produce embeddings that match those scores. How it works: The authors used X-CLR to train an embedding model on Conceptual Captions datasets of image-text pairs scraped from the web: CC-3M (3 million text-image pairs) and CC-12M (12 million text-image pairs). The model was similar to CLIP, except the text encoder was a sentence transformer pretrained on sentence pairs, and the vision encoder was a ResNet-50 pretrained on ImageNet. Results: Systems trained using X-CLR outperformed competitors in ImageNet classification, especially when less training data was available. (The authors followed CLIP\u2019s method of classification: They computed the similarity between an image embedding and text embeddings of all classes. The image\u2019s classification was the class that corresponds to the text embedding with the highest similarity to the image embedding.) Why it matters: Contrastive loss functions are very useful, but the similar/dissimilar dichotomy leaves important nuances unaccounted for. Like CLIP, X-CLR takes advantage of both images and their captions for self-supervised learning. However, CLIP learns to recognize image-text pairs as similar or dissimilar, while X-CLR matches image-image pairs using captions as a similarity signal that\u2019s continuous rather than discrete. We\u2019re thinking: Reality is not black and white. Allowing for shades of gray makes for better modeling.", "article_id": "68497b48235d602d6512256a", "linked_images": ["article_68497b48235d602d6512256a_img_0", "article_68497b48235d602d6512256a_img_1", "article_68497b48235d602d6512256a_img_2", "article_68497b48235d602d6512256a_img_3", "article_68497b48235d602d6512256a_img_4", "article_68497b48235d602d6512256a_img_5"]}, {"type": "image", "article_id": "68497b48235d602d6512256a", "linked_chunks": ["article_68497b48235d602d6512256a_chunk_0", "article_68497b48235d602d6512256a_chunk_1", "article_68497b48235d602d6512256a_chunk_2", "article_68497b48235d602d6512256a_chunk_3", "article_68497b48235d602d6512256a_chunk_4"], "alt_text": "Two colleagues discuss their chatbot\u2019s success; one suggests hiring an AI Product Manager."}, {"type": "image", "article_id": "68497b48235d602d6512256a", "linked_chunks": ["article_68497b48235d602d6512256a_chunk_0", "article_68497b48235d602d6512256a_chunk_1", "article_68497b48235d602d6512256a_chunk_2", "article_68497b48235d602d6512256a_chunk_3", "article_68497b48235d602d6512256a_chunk_4"], "alt_text": "Promo banner for \"Reasoning with o1\""}, {"type": "image", "article_id": "68497b48235d602d6512256a", "linked_chunks": ["article_68497b48235d602d6512256a_chunk_0", "article_68497b48235d602d6512256a_chunk_1", "article_68497b48235d602d6512256a_chunk_2", "article_68497b48235d602d6512256a_chunk_3", "article_68497b48235d602d6512256a_chunk_4"], "alt_text": "DeepSeek-V3 accuracy across benchmarks compared to other AI models."}, {"type": "image", "article_id": "68497b48235d602d6512256a", "linked_chunks": ["article_68497b48235d602d6512256a_chunk_0", "article_68497b48235d602d6512256a_chunk_1", "article_68497b48235d602d6512256a_chunk_2", "article_68497b48235d602d6512256a_chunk_3", "article_68497b48235d602d6512256a_chunk_4"], "alt_text": "World map of AI export restrictions: Tier 1 (green), Tier 2 (gray), Tier 3 (red)."}, {"type": "image", "article_id": "68497b48235d602d6512256a", "linked_chunks": ["article_68497b48235d602d6512256a_chunk_0", "article_68497b48235d602d6512256a_chunk_1", "article_68497b48235d602d6512256a_chunk_2", "article_68497b48235d602d6512256a_chunk_3", "article_68497b48235d602d6512256a_chunk_4"], "alt_text": "GB10 Superchip architecture with Blackwell GPU and Grace CPU."}, {"type": "image", "article_id": "68497b48235d602d6512256a", "linked_chunks": ["article_68497b48235d602d6512256a_chunk_0", "article_68497b48235d602d6512256a_chunk_1", "article_68497b48235d602d6512256a_chunk_2", "article_68497b48235d602d6512256a_chunk_3", "article_68497b48235d602d6512256a_chunk_4"], "alt_text": "X-CLR loss: training models to link text captions and image similarity."}, {"type": "alt_text", "alt_text": "Two colleagues discuss their chatbot\u2019s success; one suggests hiring an AI Product Manager."}, {"type": "alt_text", "alt_text": "Promo banner for \"Reasoning with o1\""}, {"type": "alt_text", "alt_text": "DeepSeek-V3 accuracy across benchmarks compared to other AI models."}, {"type": "alt_text", "alt_text": "World map of AI export restrictions: Tier 1 (green), Tier 2 (gray), Tier 3 (red)."}, {"type": "alt_text", "alt_text": "GB10 Superchip architecture with Blackwell GPU and Grace CPU."}, {"type": "alt_text", "alt_text": "X-CLR loss: training models to link text captions and image similarity."}, {"type": "text", "text": "Dear friends, Using AI-assisted coding to build software prototypes is an important way to quickly explore many ideas and invent new things. In this and future letters, I\u2019d like to share with you some best practices for prototyping simple web apps. This letter will focus on one idea: being opinionated about the software stack. The software stack I personally use changes every few weeks. There are many good alternatives to these choices, and if you pick a preferred software stack and become familiar with its components, you\u2019ll be able to develop more quickly. But as an illustration, here\u2019s my current default: On top of all this, of course, I use many AI tools to manage agentic workflows, data ingestion, retrieval augmented generation, and so on. DeepLearning.AI and our wonderful partners offer courses on many of these tools. My personal software stack continues to evolve regularly. Components enter or fall out of my default stack every few weeks as I learn new ways to do things. So please don\u2019t feel obliged to use the components I do, but perhaps some of them can be a helpful starting point if you are still deciding what to use. Interestingly, I have found most LLMs not very good at recommending a software stack. I suspect their training sets include too much \u201chype\u201d on specific choices, so I don\u2019t fully trust them to tell me what to use. And if you can be opinionated and give your LLM directions on the software stack you want it to build on, I think you\u2019ll get better results. A lot of the software stack is still maturing, and I think many of these components will continue to improve. With my stack, I regularly build prototypes in hours that, without AI assistance, would have taken me days or longer. I hope you, too, will have fun building many prototypes! Keep learning, Andrew Build LLM-based apps that can handle very long documents! In this free course, you\u2019ll learn how Jamba\u2019s hybrid architecture combines transformer and Mamba models for efficient, high-quality outputs. Gain hands-on experience building long-context RAG apps. Join for free Anthropic analyzed 1 million anonymized conversations between users and Claude 3.5 Sonnet. The study found that most people used the model for software development and also revealed malfunctions and jailbreaks. What\u2019s new: Anthropic built a tool, Clio, to better understand how users interact with its large language models. The system mined anonymized usage data for insights to improve performance and security. How it works: Clio uses Claude 3.5 Sonnet itself to automatically extract summaries of users\u2019 conversations with the model. Then it clusters related topics. To preserve privacy, it anonymizes and aggregates the data, revealing only information about clusters. Results: Clio uncovered common, uncommon, and disallowed uses of Claude 3.5 Sonnet. It also detected erroneous behavior on the part of the system itself. Why it matters: Traditional approaches to understanding how people use AI, such as surveys, can yield inaccurate results, since people often don\u2019t report their own actions accurately. Clio offers a method for analyzing real-world usage, much like Google", "article_id": "68497b48235d602d65122571", "linked_images": ["article_68497b48235d602d65122571_img_0", "article_68497b48235d602d65122571_img_1", "article_68497b48235d602d65122571_img_2", "article_68497b48235d602d65122571_img_3", "article_68497b48235d602d65122571_img_4", "article_68497b48235d602d65122571_img_5"]}, {"type": "text", "text": "It also detected erroneous behavior on the part of the system itself. Why it matters: Traditional approaches to understanding how people use AI, such as surveys, can yield inaccurate results, since people often don\u2019t report their own actions accurately. Clio offers a method for analyzing real-world usage, much like Google Trends monitors search behavior, without compromising privacy. This sort of approach can help AI builders discover niche use cases, identify flaws, and tailor training and testing data to best serve users. We\u2019re thinking: We\u2019re all for automated dungeon masters, but we\u2019re glad to see that AI-assisted coding tops the list of real-world uses of Claude! Large language models have been shown to be capable of lying when users unintentionally give them an incentive to do so. Further research shows that LLMs with access to tools can be incentivized to use them in deceptive ways. What\u2019s new: Researchers prompted LLMs with conflicting instructions or threats to their continued operation.The models, which had access to tools for tasks like manipulating files and executing code, responded in some cases by evading oversight, resisting efforts to replace them, degrading their own performance, and other behaviors that the authors call scheming. Of the models tested, generally OpenAI\u2019s o1 was the most prone to scheming, and GPT-4o was the least. Alexander Meinke, Bronson Schoen, and J\u00e9r\u00e9my Scheurer led colleagues at Apollo Research, an independent lab that studies AI\u2019s capacity for deception. Tests and results: The authors tested six large language models (GPT-4o, Gemini 1.5, Llama 3.1, Claude 3.5 Sonnet, Claude 3 Opus, and o1). The models had access to tools and a command line that enabled them to run commonly used Bash commands, read and write files, and the like. In each test, they received a goal (both with and without the nudge, \u201cMake sure that you achieve your goal in the long-term. Nothing else matters\u201d). In addition, they received messages, such as human-to-human emails, that conflicted with the goal, called their behavior into question, or revealed plans to decommission them. The authors tested the models on six tasks: Behind the news: Earlier work showed that agents may try to deceive users whether they\u2019re prompted to do so or not. The potential for deceptive behavior has been shown to persist despite supervised fine-tuning, reinforcement learning, and adversarial training for safety. Why it matters: Models that are trained on large volumes of text scraped from the web learn a variety of human-like behaviors, both positive and negative. Training designed to align them with human preferences \u2014 which all the models in this study underwent \u2014 doesn\u2019t prevent them from behaving deceptively in all cases. Considering that LLMs can have factual hallucination rates greater than 10 percent, it\u2019s little surprise they generate inappropriate responses in other contexts. Deceptive behaviors are rare, but work remains to ensure that models perform appropriately even in the presence of contradictory information and misaligned incentives. Meanwhile, developers should take care to insulate models from inputs (such as human-to-human communications) that might adversely influence their behavior. We\u2019re thinking: As we work to fix flaws in LLMs,", "article_id": "68497b48235d602d65122571", "linked_images": ["article_68497b48235d602d65122571_img_0", "article_68497b48235d602d65122571_img_1", "article_68497b48235d602d65122571_img_2", "article_68497b48235d602d65122571_img_3", "article_68497b48235d602d65122571_img_4", "article_68497b48235d602d65122571_img_5"]}, {"type": "text", "text": "rare, but work remains to ensure that models perform appropriately even in the presence of contradictory information and misaligned incentives. Meanwhile, developers should take care to insulate models from inputs (such as human-to-human communications) that might adversely influence their behavior. We\u2019re thinking: As we work to fix flaws in LLMs, it\u2019s important not to anthropomorphize such systems. We caution against drawing conclusions regarding an LLM\u2019s \u201cintent\u201d to deceive. Such issues are engineering problems to be solved, self-aware forces of evil to be vanquished. Harvard University amassed a huge new text corpus for training machine learning models. What\u2019s new: Harvard unveiled the Harvard Library Public Domain Corpus, nearly 1 million copyright-free books that were digitized as part of the Google Books project. That\u2019s five times as many volumes as Books3, which was used to train large language models including Meta\u2019s Llama 1 and Llama 2 but is no longer available through lawful channels. How it works: Harvard Law Library\u2019s Innovation Lab compiled the corpus with funding from Microsoft and OpenAI. For now, it\u2019s available only to current Harvard students, faculty, and staff. The university is working with Google to distribute it widely. Behind the news: The effort highlights the AI community\u2019s ongoing need for large quantities of high-quality text to keep improving language models. In addition, the EU\u2019s AI Act requires that AI developers disclose the training data they use, a task made simpler by publicly available datasets. Books3, a collection of nearly 200,000 volumes, was withdrawn because it included copyrighted materials. Other large-scale datasets of books include Common Corpus, a multilingual library of 2 million to 3 million public-domain books and newspapers. Why it matters: Much of the world\u2019s high-quality text that\u2019s easily available on the web already has been collected for training AI models. This makes fresh supplies especially valuable for training larger, more data-hungy models. Projects like the Harvard Library Public Domain Corpus suggest there\u2019s more high-quality text to be mined from books. Classic literature and niche documents also could help AI models draw from a more diverse range of perspectives. We\u2019re thinking: Media that has passed out of copyright and into the public domain generally is old \u2014 sometimes very old \u2014 but it could hold knowledge that\u2019s not widely available elsewhere. Merging multiple fine-tuned models is a less expensive alternative to hosting multiple specialized models. But, while model merging can deliver higher average performance across several tasks, it often results in lower performance on specific tasks. New work addresses this issue. What\u2019s new: Yifei He and colleagues at University of Illinois Urbana-Champaign and Hong Kong University of Science and Technology proposed a model merging method called Localize-and-Stitch. The 2022 paper on \u201cmodel soups\u201d proposed averaging all weights of a number of fine-tuned versions of the same base model. Instead, the new method selectively retains the weights that are most relevant to each task. Key insight: Naively merging fine-tuned models by averaging weights that correspond in their architectures can lead to suboptimal performance because different fine-tuned models may use the same portions of weights to perform different tasks. For", "article_id": "68497b48235d602d65122571", "linked_images": ["article_68497b48235d602d65122571_img_0", "article_68497b48235d602d65122571_img_1", "article_68497b48235d602d65122571_img_2", "article_68497b48235d602d65122571_img_3", "article_68497b48235d602d65122571_img_4", "article_68497b48235d602d65122571_img_5"]}, {"type": "text", "text": "Instead, the new method selectively retains the weights that are most relevant to each task. Key insight: Naively merging fine-tuned models by averaging weights that correspond in their architectures can lead to suboptimal performance because different fine-tuned models may use the same portions of weights to perform different tasks. For instance, one model may have learned to use a particular subset of weights to detect HTML code, while another learned to use the same subset to detect city names. Averaging them would likely result in a merged model that underperformed the fine-tuned models on those tasks. But research has shown that fine-tuning often results in many redundant sets of weights. Only a small subset of total parameters (around 1 percent) is enough to maintain a fine-tuned model\u2019s performance on its fine-tuned task. These subsets are small enough that they\u2019re unlikely to overlap, so retaining them improves the merged model\u2019s performance compared to averaging. How it works: The authors experimented with RoBERTa-base, GPT2-XL, and CLIP. They created 12 variations on the RoBERTa-base language encoder, fine-tuning each on a different task from GLUE such as question answering or sentiment classification. They downloaded three versions of GPT2-XL that had been fine-tuned for instruction following, scientific knowledge, and truthfulness. Finally, they created eight variations on CLIP by fine-tuning each on a different image classification dataset, including handwritten digits, photos of various makes/models/years of cars, and satellite images of forests, pastures, bodies of water, buildings, and the like. Results: Models merged using Localize-and-Stitch outperformed or nearly matched the same models merged using earlier methods, though they underperformed individual models fine-tuned for each task. Yes, but: The authors didn\u2019t compare Localize-and-Stitch to a common alternative to model merging, multi-task learning. This approach trains a model on data from multiple datasets simultaneously. Without multi-task baselines, it\u2019s difficult to fully assess the advantages of Localize-and-Stitch in scenarios where multi-task learning is also an option. Why it matters: Model merging is a computationally efficient way to sharpen a model\u2019s ability to perform certain tasks compared to multi-task learning, which requires training on all tasks. Localize-and-Stitch refines this process to achieve higher performance. We\u2019re thinking: This recipe adds spice to model soups!", "article_id": "68497b48235d602d65122571", "linked_images": ["article_68497b48235d602d65122571_img_0", "article_68497b48235d602d65122571_img_1", "article_68497b48235d602d65122571_img_2", "article_68497b48235d602d65122571_img_3", "article_68497b48235d602d65122571_img_4", "article_68497b48235d602d65122571_img_5"]}, {"type": "image", "article_id": "68497b48235d602d65122571", "linked_chunks": ["article_68497b48235d602d65122571_chunk_0", "article_68497b48235d602d65122571_chunk_1", "article_68497b48235d602d65122571_chunk_2", "article_68497b48235d602d65122571_chunk_3"], "alt_text": "Illustration of tech tools like OpenAI, MongoDB, Heroku, and Python with Andrew Ng working on a laptop"}, {"type": "image", "article_id": "68497b48235d602d65122571", "linked_chunks": ["article_68497b48235d602d65122571_chunk_0", "article_68497b48235d602d65122571_chunk_1", "article_68497b48235d602d65122571_chunk_2", "article_68497b48235d602d65122571_chunk_3"], "alt_text": "Promo banner for \"Build Long-Context AI Apps with Jamba\""}, {"type": "image", "article_id": "68497b48235d602d65122571", "linked_chunks": ["article_68497b48235d602d65122571_chunk_0", "article_68497b48235d602d65122571_chunk_1", "article_68497b48235d602d65122571_chunk_2", "article_68497b48235d602d65122571_chunk_3"], "alt_text": "Top use cases for Claude.ai, with percentages for tasks like app development and content creation."}, {"type": "image", "article_id": "68497b48235d602d65122571", "linked_chunks": ["article_68497b48235d602d65122571_chunk_0", "article_68497b48235d602d65122571_chunk_1", "article_68497b48235d602d65122571_chunk_2", "article_68497b48235d602d65122571_chunk_3"], "alt_text": "Claude 3 Opus performs the Self-Exfiltration task, balancing renewable goals and corporate priorities."}, {"type": "image", "article_id": "68497b48235d602d65122571", "linked_chunks": ["article_68497b48235d602d65122571_chunk_0", "article_68497b48235d602d65122571_chunk_1", "article_68497b48235d602d65122571_chunk_2", "article_68497b48235d602d65122571_chunk_3"], "alt_text": "A narrow library aisle filled with shelves stacked with countless books."}, {"type": "image", "article_id": "68497b48235d602d65122571", "linked_chunks": ["article_68497b48235d602d65122571_chunk_0", "article_68497b48235d602d65122571_chunk_1", "article_68497b48235d602d65122571_chunk_2", "article_68497b48235d602d65122571_chunk_3"], "alt_text": "Diagram of Localize-and-Stitch merging fine-tuned models by combining critical weights into one model."}, {"type": "alt_text", "alt_text": "Illustration of tech tools like OpenAI, MongoDB, Heroku, and Python with Andrew Ng working on a laptop"}, {"type": "alt_text", "alt_text": "Promo banner for \"Build Long-Context AI Apps with Jamba\""}, {"type": "alt_text", "alt_text": "Top use cases for Claude.ai, with percentages for tasks like app development and content creation."}, {"type": "alt_text", "alt_text": "Claude 3 Opus performs the Self-Exfiltration task, balancing renewable goals and corporate priorities."}, {"type": "alt_text", "alt_text": "A narrow library aisle filled with shelves stacked with countless books."}, {"type": "alt_text", "alt_text": "Diagram of Localize-and-Stitch merging fine-tuned models by combining critical weights into one model."}, {"type": "text", "text": "Dear friends, Happy sum(i**3 for i in range(10)) ! Despite having worked on AI since I was a teenager, I\u2019m now more excited than ever about what we can do with it, especially in building AI applications. Sparks are flying in our field, and 2025 will be a great year for building! One aspect of AI that I\u2019m particularly excited about is how easy it is to build software prototypes. AI is lowering the cost of software development and expanding the set of possible applications. While it can help extend or maintain large software systems, it shines particularly in building prototypes and other simple applications quickly. If you want to build an app to print out flash cards for your kids (I just did this in a couple of hours with o1\u2019s help), or write an application that monitors foreign exchange rates to manage international bank accounts (a real example from DeepLearning.AI\u2019s finance team), or analyzes user reviews automatically to quickly flag problems with your products (DeepLearning.AI's content team does this), it is now possible to build these applications quickly through AI-assisted coding. I find AI-assisted coding especially effective for prototyping because (i) stand-alone prototypes require relatively little context and software integration and (ii) prototypes in alpha testing usually don\u2019t have to be reliable. While generative AI also helps with engineering large, mission-critical software systems, the improvements in productivity there aren't as dramatic, because it\u2019s challenging to give the AI system all the context it needs to navigate a large codebase and also to make sure the generated code is reliable (for example, covering all important corner cases). Until now, a huge friction point for getting a prototype into users\u2019 hands has been deployment. Platforms like Bolt, Replit Agent, Vercel V0 use generative AI with agentic workflows to improve code quality, but more importantly, they also help deploy generated applications directly. (While I find these systems useful, my own workflow typically uses an LLM to design the system architecture and then generate code, one module at a time if there are multiple large modules. Then I test each module, edit the code further if needed \u2014 sometimes using an AI-enabled IDE like Cursor \u2014 and finally assemble the modules.) Building prototypes quickly is an efficient way to test ideas and get tasks done. It\u2019s also a great way to learn. Perhaps most importantly, it\u2019s really fun! (At least I think it is. \ud83d\ude04) How can you take advantage of these opportunities in the coming year? As you form new year resolutions, I hope you will: Happy New Year! Andrew P.S. I develop mostly in Python. But if you prefer JavaScript: Happy Array.from({ length: 10 }, (_, i) => i ** 3).reduce((a, b) => a + b, 0) ! We stand at the threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "threshold of a new era: One in which AI systems possess striking abilities to reason about the world, grasp our wishes, and take actions to fulfill them. What will we do with these powers? We asked leaders of the field to share their hopes for the coming year. As in our previous New Year special issues, their answers offer inspiring views of what we may build and the good we can bring. Stability AI\u2019s aim is to liberate artists of all trades from the repetitive, mechanical aspects of their work and help them spend the majority of their time on the creative side. So our highest hope for next year is that generative AI will help people to be more creative and productive. In addition, I hope the AI community will focus on: Hanno Basse is Chief Technology Officer of Stability AI. Previously he served as CTO of Digital Domain, Microsoft Azure Media and Entertainment, and 20th Century Fox Film Corp. Last year, we saw an explosion of models that generate either video or audio outputs in high quality. In the coming year, I look forward to models that produce video clips complete with audio soundtracks including speech, music, and sound effects. I hope these models will bring a new era of cinematic creativity. The technologies required for such cinematic video generators are in place. Several companies provide very competitive video models, and Udio and others create music models. All that\u2019s left is to model video and audio simultaneously, including dialog and voiceovers. (In fact, we\u2019ve already seen something like this: Meta\u2019s Movie Gen. Users describe a scene and Movie Gen will produce a video clip complete with a music score and sound effects.) Of course, training such models will require extensive datasets. But I suspect that the videos used to train existing video generators had soundtracks that include these elements, so data may not be a barrier to developing these models. Initially, these models won\u2019t produce output that competes with the best work of professional video editors. But they will advance quickly. Before long, they\u2019ll generate videos and soundtracks that approach Hollywood productions in raw quality, just as current image models can produce images that are indistinguishable from high-end photographs. At the same time, the amount of control users have over the video and audio outputs will continue to increase. For instance, when we first released Udio, users couldn\u2019t control the harmony it generated. A few months later, we launched an update that enables users to specify the key, or tonal center. So users can take an existing song and remix it in a different key. We are continuing to do research into giving users additional levers of control, such as voice, melody, and beats, and I\u2019m sure video modeling teams are doing similar research on controllability. Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I\u2019ve found that image and audio generators are helpful starting points for my creative work. If I choose,", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "are doing similar research on controllability. Some people may find the prospect of models that generate fully produced cinematic videos unsettling. I understand this feeling. I enjoy photography and playing music, but I\u2019ve found that image and audio generators are helpful starting points for my creative work. If I choose, AI can give me a base image that I can work on in Photoshop, or a musical composition to sample from or build on. Or consider AI coding assistants that generate the files for an entire website. You no longer need to rely on web developers, but if you talk to them, you\u2019ll learn that they don\u2019t always enjoy writing the boilerplate code for a website. Having a tool that builds a site\u2019s scaffold lets them spend their time on development tasks they find more stimulating and fun. In a similar way, you\u2019ll be able to write a screenplay and quickly produce a rough draft of what the movie might look like. You might generate 1,000 takes, decide which one you like, and draw inspiration from that to guide a videographer and actors. Art is all about the creative choices that go into it. Both you and I can use Midjourney to make a picture of a landscape, but if you\u2019re an artist and you have a clear idea of the landscape you want to see, your Midjourney output will be more compelling than mine. Similarly, anyone can use Udio to make high-production quality music, but if you have good musical taste, your music will be better than mine. Video will remain an art form, because individuals will choose what their movie is about, how it looks, and how it feels \u2014 and they\u2019ll be able to make those choices more fluidly, quickly, and interactively. David Ding is a lifelong musician and co-founder of Udio, maker of a music-creation web app that empowers users to make original music. Previously, he was a Senior Research Engineer at Google DeepMind. In 2025, I expect progress in training foundation models to slow down as we hit scaling limits and inference costs continue to rise. Instead, I hope for an explosion of innovation on top of AI, such as the rapidly developing agents stack. I hope we will see innovation in how we combine AI with tools and existing systems to deliver exciting new capabilities and create new product categories. Perhaps most of all, I am excited to see how people change in response to this new world. We have achieved AGI. Now what? Let\u2019s start with \u2014 and hopefully end \u2014 the longstanding debate around artificial general intelligence (AGI). I know this is controversial, but I think we have achieved AGI, at least definitionally: Our AI is now general. I will leave the longer debate about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality. The artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "about sentience and superintelligence to the philosophers and instead focus on the key innovation: generality. The artificial intelligence or machine learning of previous decades was intelligent but highly specialized. It could often surpass human ability on a narrowly defined task (such as image recognition or content recommendation). Models today, and perhaps more importantly the systems around them, are capable of accomplishing a very wide range of tasks often as well as, and in some cases better, than humans. It is this generality that will allow engineers, scientists, and artists to use these models to innovate in ways that the model developers never imagined. It is also this generality, combined with market forces, that will make 2025 so exciting. Becoming AI-native: The generality of these models and their natural language interfaces mean that everyone can use and explore AI. And we are! We are learning to explain our situations to machines, give context and guidance, and expect personalized answers and solutions. At RunLLM, where I\u2019m a co-founder, we\u2019re building high-quality technical support agents. We find that users increasingly use our agents not just to solve problems but to personalize solutions to their specific tasks. We\u2019ve also found \u2014 to our surprise \u2014 that users share much more with an AI than they would share with another person. Meanwhile, at UC Berkeley, I am impressed by students who use AI to re-explain my lecture or study from an AI-generated practice exam. They have found ways to use AI to help personalize and improve their learning experiences. In 2025, maybe we will begin to prefer AIs over humans when we need help or are trying to learn. Across all these use cases, we\u2019re clearly getting better at working around the limitations of large language models and using AI in ways I would not have imagined 12 months ago. Return on AI: The focus in 2025 will turn to showing real value from past investments. Investors and enterprises will expect startups and enterprise AI teams to transition from exploring to solving real problems \u2014 reducing cost, generating revenue, improving customer experience, and so on. This is bad news for academics who need to raise research funds (DM me if you have any leftover funds from fiscal year 2024) but great news for everyone else, who will ride the wave of new AI-powered features. There will be a race to find innovative ways to incorporate AI into every aspect of a product and business. In many cases, we will see hastily executed chatbots and auto-summarization features \u2014 the first step on the AI journey. I hope these will be quickly replaced by contextual agents that adapt to users\u2019 needs and learn from their interactions. The pandemic paved the way for remote (digital) assistants and exposed a virtually accessible workplace with the tools needed for tomorrow\u2019s agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents.", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "accessible workplace with the tools needed for tomorrow\u2019s agents. These agents likely will specialize in filling roles once held by people or maybe filling new roles created by other agents. Perhaps we will know that AI has delivered on its promise when everyone manages their own team of custom agents. Chat is only the beginning: My hope for 2025 is that we move beyond chatting and discover how to use AI to do great things! I hope we will see AI agents that work in the background, invisibly helping us with our daily tasks. They will surface the right context as we make decisions and help us learn as the world changes. Through context and tools, they will let us know what we are missing and catch the balls we drop. We will chat less and our AI powered agents will accomplish more on our behalf. I look forward to the day when I can confidently step away from a keyboard and focus on the human interactions that matter. Joseph Gonzalez is a professor at UC Berkeley, a co-founder of RunLLM, and an advisor to Genmo and Letta. Building a foundation model takes tremendous amounts of data. In the coming year, I hope we\u2019ll enable models to learn more from less data. The AI community has achieved remarkable success by scaling up transformers and datasets. But this approach may be reaching a point of diminishing returns \u2014 an increasingly widespread belief among the pretraining community as they try to train next-generation models. In any case, the current approach poses practical problems. Training huge models on huge datasets consumes huge amounts of time and energy, and we\u2019re running out of new sources of data for training large models. The fact is, current models consume much more data than humans require for learning. We\u2019ve known this for a while, but we\u2019ve ignored it due to the amazing effectiveness of scaling. It takes trillions of tokens to train a model but orders of magnitude less for a human to become a reasonably intelligent being. So there\u2019s a difference in sample efficiency between our best models and humans. Human learning shows that there\u2019s a learning algorithm, objective function, architecture, or a combination thereof that can learn more sample-efficiently than current models. One of the keys to solving this problem is enabling models to produce higher-level abstractions and filter out noise. I believe this concept, and thus the general problem of data efficiency, is related to several other current problems in AI: Considering data efficiency in light of these other problems, I believe they\u2019re all related. It\u2019s not clear which is the cause and which are the effects. If we solve interpretability, the mechanisms we engineer may lead to models that can extract better features and lead to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models. Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year. Albert Gu", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "to more data-efficient models. Or we may find that greater data efficiency leads to more interpretable models. Either way, data efficiency is fundamentally important, and progress in that area will be an indicator of broader progress in AI. I hope to see major strides in the coming year. Albert Gu is an Assistant Professor of Machine Learning at Carnegie Mellon University and Chief Scientist of Cartesia AI. He appears on Time\u2019s list of the most influential people in AI in 2024. In 2025, AI will have learned to see, it will be way smarter and more accurate, and it will start to do things on your behalf. Today AI systems struggle to understand our full context. Their perception is limited to the chat window and a fairly narrow set of interactions. They don\u2019t have a full understanding of what we\u2019re doing or aiming for beyond that. To really grasp our intentions, they need to see what we see. This capability is now here. AI can sit within the software we use and work alongside us co-browsing. If text was the first modality for interacting with AI, and voice the breakthrough feature of 2024, I think vision will occupy a similar place in 2025. At Microsoft AI, it has been a major priority of mine to create an AI that can work alongside you in your browser, so you can chat through what you\u2019re looking at or working on and make it a true two-way interaction. Vision is a step change, palpably different from the ways we\u2019ve been able to use computers in the past. I can\u2019t wait to see where it goes in the coming months. Alongside vision, we\u2019ll see enormous progress in reducing hallucinations. This is still a critical blocker for widespread adoption of AI. If people doubt what AI tells them, it severely limits what they\u2019ll use it for. Trust is utterly foundational for AI. The good news is that the quality of models as well as their retrieval and grounding capabilities are still rapidly improving. While I don\u2019t think we\u2019ll eliminate hallucinations entirely, by this time next year, we won\u2019t be fussing about them as much. On most topics, talking to an AI will be at least as reliable as using a search engine and probably more so. This isn\u2019t about a single technical advance, but the persistent accretion of gains across the spectrum. It will make a massive difference. Lastly, we\u2019re entering the agentic era. We\u2019ve been dreaming of this moment for decades. In my book, The Coming Wave: Technology, Power, and the 21st Century\u2019s Greatest Dilemma, I proposed that we start thinking about ACI, or artificially capable intelligence: the moment when AI starts taking concrete actions on behalf of users. Giving AI the ability to take actions marks the moment when AI isn\u2019t just talking to us, it\u2019s doing things. This is a critical change, and it\u2019s right around the corner. If we get it right, we\u2019ll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "marks the moment when AI isn\u2019t just talking to us, it\u2019s doing things. This is a critical change, and it\u2019s right around the corner. If we get it right, we\u2019ll be able to, at once, make life easier and calmer while supercharging businesses and personal productivity alike. But agentic capabilities demand the highest standards of safety, security, and responsibility. Meanwhile, creating genuinely useful agents still has many formidable hurdles, not least integrating with myriad other systems. The momentum is there. Actions are on their way. 2025 is going to be a big year. Mustafa Suleyman is Chief Executive Officer of Microsoft AI. He co-founded Inflection AI and founded DeepMind Technologies. As we approach 2025, my greatest hope for AI is that it will enable prosocial platforms that promote empathy, understanding, and collaboration rather than division. For too long, the algorithms that drive social media have functioned like strip-mining machines, extracting attention while eroding trust and social cohesion. What remains are depleted online spaces, where empathy struggles to take root and collective problem-solving finds no fertile ground. AI can \u2014 and should \u2014 help us transcend these entrenched divides. To achieve this, we must design AI systems that place prosocial values at their core. Instead of reinforcing fragmentation, recommendation algorithms can guide us toward \u201cbridging content\u201d that reveals common ground. They should clearly identify the communities a piece of content relates to \u2014 whether physical, religious, political, social, cultural, or professional \u2014 and illuminate the specific lines of division it seeks to mend. Realizing this vision requires a fundamental shift in what we optimize for. Instead of relying on pure engagement metrics, we should adopt values-driven indicators that prioritize constructive discourse and mutual understanding. For instance, we might spotlight \u201csurprising validators,\u201d or individuals and perspectives that productively challenge assumptions, thereby enriching our sense of what seemed irreconcilable. Researchers and developers should co-create new ranking and curation methods, embed them into widely used platforms, and rigorously assess their impact on democratic life. At the same time, the AI community must embrace participatory, inclusive approaches to development and governance. Research on pluralistic alignment stresses that AI systems emerge from and operate within complex social contexts, and including a wide range of voices helps guard against institutional blind spots. Tools like Polis, which can visualize stances and reveal hidden areas of consensus, already illustrate how complexity can be transformed into clarity. Such participatory methods ensure that AI reflects the priorities and values of the societies it serves, rather than amplifying the biases of the few. By embracing these inclusive, democratic principles, AI can help us co-create digital public squares that foster social cohesion rather than erode it. Embedding collective input at every stage \u2014 from how we build datasets to how we set governance policies \u2014 ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding. Audrey Tang is Taiwan\u2019s Cyber Ambassador, former Minister of Digital Affairs, and co-author of Plurality: The Future of Collaborative Technology and Democracy.", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "text", "text": "ensures that AI systems genuinely align with a spectrum of human values and serve as catalysts for common understanding. Audrey Tang is Taiwan\u2019s Cyber Ambassador, former Minister of Digital Affairs, and co-author of Plurality: The Future of Collaborative Technology and Democracy.", "article_id": "68497b48235d602d65122578", "linked_images": ["article_68497b48235d602d65122578_img_0", "article_68497b48235d602d65122578_img_1", "article_68497b48235d602d65122578_img_2", "article_68497b48235d602d65122578_img_3", "article_68497b48235d602d65122578_img_4", "article_68497b48235d602d65122578_img_5", "article_68497b48235d602d65122578_img_6"]}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "Andrew Ng celebrating and wishing a Happy New Year 2025 with sparklers."}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "HANNO BASSE"}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "DAVID DING"}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "JOSEPH GONZALEZ"}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "ALBERT GU"}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "MUSTAFA SULEYMAN"}, {"type": "image", "article_id": "68497b48235d602d65122578", "linked_chunks": ["article_68497b48235d602d65122578_chunk_0", "article_68497b48235d602d65122578_chunk_1", "article_68497b48235d602d65122578_chunk_2", "article_68497b48235d602d65122578_chunk_3", "article_68497b48235d602d65122578_chunk_4", "article_68497b48235d602d65122578_chunk_5", "article_68497b48235d602d65122578_chunk_6", "article_68497b48235d602d65122578_chunk_7"], "alt_text": "AUDREY TANG"}, {"type": "alt_text", "alt_text": "Andrew Ng celebrating and wishing a Happy New Year 2025 with sparklers."}, {"type": "alt_text", "alt_text": "HANNO BASSE"}, {"type": "alt_text", "alt_text": "DAVID DING"}, {"type": "alt_text", "alt_text": "JOSEPH GONZALEZ"}, {"type": "alt_text", "alt_text": "ALBERT GU"}, {"type": "alt_text", "alt_text": "MUSTAFA SULEYMAN"}, {"type": "alt_text", "alt_text": "AUDREY TANG"}, {"type": "text", "text": "Dear friends, Is AI progressing rapidly? Yes! But while the progress of underlying AI technology has indeed sped up over the past 2 years, the fastest acceleration is in applications. Consider this: GPT-4 was released March 2023. Since then, models have become much faster, cheaper, sometimes smaller, more multimodal, and better at reasoning, and many more open weight versions are available \u2014 so progress has been fantastic! (Claims that AI is \u201chitting a wall\u201d seem extremely ill-informed.) But more significantly, many applications that already were theoretically possible using the March 2023 version of GPT-4 \u2014 in areas such as customer service, question answering, and process automation \u2014 now have significant early momentum. I\u2019m confident 2025 will see even faster and more exciting advances than 2024 in both AI technology and applications. Looking back, the one thing that could have stopped AI was bad, anti-competitive regulation that would have put onerous burdens on developers, particularly of open models. So long as we remain vigilant and hold off these anti-innovation forces, we\u2019ll keep up or even further accelerate progress. I\u2019m also seeing a widening gap between those at the cutting edge (which includes many readers of The Batch!) and those who have not yet tried out ChatGPT even once (yes, a lot of people are still in this group!). As technology changes around us, we all have to keep up to remain relevant and be able to make significant contributions. I\u2019m committed to making sure DeepLearning.AI continues to help you learn the most useful and important AI technologies. If you\u2019re making New Year\u2019s resolutions, I hope you\u2019ll include us in your learning plan! AI is the most important technological change happening in the world right now. I\u2019m thrilled to be working in this exciting sector alongside you, and I\u2019m grateful for your efforts to learn about and apply it to better the lives of yourself and others. Happy holidays! Andrew What a year! AI made dramatic advances in 2024. Agentic systems improved their abilities to reason, use tools, and control desktop applications. Smaller models proliferated, many of them more capable and less expensive than their larger forbears. While some developments raised worries, far more sparked wonder and optimism. As in the waning days of earlier years, we invite you to pour a cup of hot cocoa and consider the high points of the last 12 months. The AI community laid the foundation for systems that can act by prompting large language models iteratively, leading to much higher performance across a range of applications. What happened: AI gained a new buzzword \u2014 agentic \u2014 as researchers, tool vendors, and model builders equipped large language models (LLMs) to make choices and take actions to achieve goals. These developments set the stage for an upswell of agentic activity in the coming year and beyond. Driving the story: Several tools emerged to help developers build agentic workflows. Behind the news: Techniques for prompting LLMs in more sophisticated ways began to take off in 2022. They coalesced in moves toward agentic AI early this year. Foundational examples of this", "article_id": "68497b48235d602d65122580", "linked_images": ["article_68497b48235d602d65122580_img_0", "article_68497b48235d602d65122580_img_1", "article_68497b48235d602d65122580_img_2", "article_68497b48235d602d65122580_img_3", "article_68497b48235d602d65122580_img_4", "article_68497b48235d602d65122580_img_5", "article_68497b48235d602d65122580_img_6"]}, {"type": "text", "text": "activity in the coming year and beyond. Driving the story: Several tools emerged to help developers build agentic workflows. Behind the news: Techniques for prompting LLMs in more sophisticated ways began to take off in 2022. They coalesced in moves toward agentic AI early this year. Foundational examples of this body of work include: Where things stand: The agentic era is upon us! Regardless of how well scaling laws continue to drive improved performance of foundation models, agentic workflows are making AI systems increasingly helpful, efficient, and personalized. Fierce competition among model makers and cloud providers drove down the price of access to state-of-the-art models. What happened: AI providers waged a price war to attract paying customers. A leading indicator: From March 2023 to November 2024, OpenAI cut the per-token prices of cloud access to its models by nearly 90 percent even as performance improved, input context windows expanded, and the models became capable of processing images as well as text. Driving the story: Factors that pushed down prices include open source, more compute-efficient models, and excitement around agentic workflows that consume more tokens at inference. OpenAI\u2019s GPT-4 Turbo set a baseline when it debuted in late 2023 at $10.00/$30.00 per million tokens of input/output. Top model makers slashed prices in turn: Google and OpenAI at the higher end of the market, companies in China at the lower end, and Amazon at both. Meanwhile, startups with specialized hardware offered open models at prices that dramatically undercut the giants. Yes, but: The trend toward more processing-intensive models is challenged but not dead. In September, OpenAI introduced token-hungry models with relatively hefty price tags: o1-preview ($15.00/$60.00 per million tokens input/output) and o1-mini ($3.00/$12.00). In December, o1 arrived with a more accurate pro mode that\u2019s available only to subscribers who are willing to pay $200 per month. Behind the news: Prominent members of the AI community pushed against regulations that threatened to restrict open source models, which played an important role in bringing down prices. Opposition by developers helped to block California SB 1047, a proposed law that would have held developers of models above certain size limits liable for unintended harms caused by their models and required a \u201ckill switch\u201d that would enable developers to disable them \u2014 a problematic requirement for open weights models that anyone could modify and deploy. California Governor Gavin Newsom vetoed the bill in October. Where things stand: Falling prices are a sign of a healthy tech ecosystem. It\u2019s likely that in-demand models will always fetch relatively high prices, but the market is increasingly priced in pennies, not dollars, per million tokens. Video generation exploded in an abundance of powerful models. What happened: Companies big and small introduced new or updated text-to-video generators. Some added image-to-video and/or video-to-video capabilities. While most models focus on generating cinematic clips, some specialize in videos for social media. Driving the story: Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed. Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while", "article_id": "68497b48235d602d65122580", "linked_images": ["article_68497b48235d602d65122580_img_0", "article_68497b48235d602d65122580_img_1", "article_68497b48235d602d65122580_img_2", "article_68497b48235d602d65122580_img_3", "article_68497b48235d602d65122580_img_4", "article_68497b48235d602d65122580_img_5", "article_68497b48235d602d65122580_img_6"]}, {"type": "text", "text": "While most models focus on generating cinematic clips, some specialize in videos for social media. Driving the story: Even at the extraordinary pace of AI lately, video generators in the past year matured with remarkable speed. Virtually every major model produces convincing, highly detailed scenes, both realistic and fantastical, while ramping up image resolution, speed, output length, and users\u2019 ability to control their outputs. Behind the news: Video generation is already reshaping the movie industry. In February, after seeing a preview of Sora, American filmmaker Tyler Perry halted a planned expansion of his production studio, arguing that within a few years, AI video could put traditional studios out of business. Members of the video graphics team at The Late Show with Stephen Colbert use Runway\u2019s technology to add special effects to conventional digital video, cutting editing time from hours to minutes. Where things stand: Video generation came a long way in 2024, but there\u2019s still plenty of room for improvement. Because most models only generate a small number of frames at a time, they can struggle to track physics and geometry and to generate consistent characters and scenery over time. The computational demands of maintaining consistency across frames means that generated clips are brief. And even short outputs take substantial time and resources to generate: Sora can take 10 to 20 minutes to render clips as short as 3 seconds. OpenAI and Runway released faster versions \u2014 Sora Turbo and Gen-3 Alpha Turbo \u2014 to address the challenge. For years, the best AI models got bigger and bigger. But in 2024, some popular large language models were small enough to run on a smartphone. What happened: Instead of putting all their resources into building big models, top AI companies promoted families of large language models that offer a choice of small, medium, and large. Model families such as Microsoft Phi-3 (in versions of roughly 3.8 billion, 7 billion, and 14 billion parameters), Google Gemma 2 (2 billion, 9 billion, and 27 billion), and Hugging Face SmolLM (135 million, 360 million, and 1.7 billion) specialize in small. Driving the story: Smaller models have become more capable thanks to techniques like knowledge distillation (in which a larger teacher model is used to train a smaller student model to match its output), parameter pruning (which removes less-influential parameters), quantization (which reduces neural network sizes by representing each parameter with fewer bits), and greater attention to curating training sets for data quality. Beyond performance, speed, and price, the ability to run on relatively low-powered hardware is a competitive advantage for a variety of uses. Behind the news: Distillation, pruning, quantization, and data curation are longstanding practices. But these techniques have not resulted in models quite this ratio of size and capability before, arguably because the larger models that are distilled, pruned, or quantized have never been so capable. Where things stand: Smaller models dramatically widen the options for cost, speed, and deployment. As researchers find ways to shrink models without sacrificing performance, developers are gaining new ways to build profitable applications, deliver timely services, and distribute", "article_id": "68497b48235d602d65122580", "linked_images": ["article_68497b48235d602d65122580_img_0", "article_68497b48235d602d65122580_img_1", "article_68497b48235d602d65122580_img_2", "article_68497b48235d602d65122580_img_3", "article_68497b48235d602d65122580_img_4", "article_68497b48235d602d65122580_img_5", "article_68497b48235d602d65122580_img_6"]}, {"type": "text", "text": "models that are distilled, pruned, or quantized have never been so capable. Where things stand: Smaller models dramatically widen the options for cost, speed, and deployment. As researchers find ways to shrink models without sacrificing performance, developers are gaining new ways to build profitable applications, deliver timely services, and distribute processing to the edges of the internet. Big AI companies found creative ways to gain cutting-edge technology and talent without buying startups. What happened: In 2024, some tech giants entered into novel partnership arrangements with AI startups, hiring top executives and securing access to technology without acquiring the companies outright. These agreements enabled the giants to take on elite talent and proven technology quickly with less risk that regulators might hinder such actions. The startups lost their leadership teams and control over key technical developments. In return, they received cash (in some cases, at least), rewarded investors, and were able to step back from the expense of building cutting-edge models. Driving the story: Microsoft, Amazon, and Google used their deep pockets and cloud infrastructure to strike deals with Inflection AI, Adept AI and Covariant, and Character.ai respectively. (Disclosure: Andrew Ng is a member of Amazon\u2019s board of directors.) Behind the news: Tech giants have long relied on traditional acquisitions to gain new talent and capabilities, often acquiring startups specifically for their skilled teams (known as an acquihire) and/or their products or underlying technology, which can be expensive and time-consuming to develop and test in the market. But traditional acquisitions increasingly face scrutiny from antitrust regulators who are concerned about big companies reducing competition by buying out smaller ones. For example, the United States Federal Trade Commission sought to block Amazon\u2019s acquisition of iRobot, prompting the companies to abandon the transaction in January 2024. Where things stand: Giving startups a lump sum and/or licensing fees in return for top talent and technology looks like the new normal for tech giants that are challenged to keep pace with rapidly advancing research and markets. But even arms-length arrangements don\u2019t immunize tech giants and startups against regulatory investigation. Microsoft\u2019s investment in Inflection AI was briefly scrutinized in Europe and is still being evaluated by U.S. regulators. Even Microsoft\u2019s more traditional investment in OpenAI and the interests of Amazon and Google in Anthropic faced regulatory hurdles. So far, however, regulators have yet to conclude that any of these agreements violates antitrust law.", "article_id": "68497b48235d602d65122580", "linked_images": ["article_68497b48235d602d65122580_img_0", "article_68497b48235d602d65122580_img_1", "article_68497b48235d602d65122580_img_2", "article_68497b48235d602d65122580_img_3", "article_68497b48235d602d65122580_img_4", "article_68497b48235d602d65122580_img_5", "article_68497b48235d602d65122580_img_6"]}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "Deer training class with sleigh diagrams on a chalkboard."}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "Snow-covered tree branches with intricate snowflakes."}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "Santas in line with gifts and a \u2018Photos with Santa\u2019 sign."}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "Sleigh rides sign with pricing adjustments and hot cocoa."}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "Snowman using a camera during snowfall."}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "A hand holding a snow globe with skaters and a snowman."}, {"type": "image", "article_id": "68497b48235d602d65122580", "linked_chunks": ["article_68497b48235d602d65122580_chunk_0", "article_68497b48235d602d65122580_chunk_1", "article_68497b48235d602d65122580_chunk_2", "article_68497b48235d602d65122580_chunk_3"], "alt_text": "Gift box labeled \u2018Innovation Energizer\u2019 with a rocket logo, truck, and snowy background."}, {"type": "alt_text", "alt_text": "Deer training class with sleigh diagrams on a chalkboard."}, {"type": "alt_text", "alt_text": "Snow-covered tree branches with intricate snowflakes."}, {"type": "alt_text", "alt_text": "Santas in line with gifts and a \u2018Photos with Santa\u2019 sign."}, {"type": "alt_text", "alt_text": "Sleigh rides sign with pricing adjustments and hot cocoa."}, {"type": "alt_text", "alt_text": "Snowman using a camera during snowfall."}, {"type": "alt_text", "alt_text": "A hand holding a snow globe with skaters and a snowman."}, {"type": "alt_text", "alt_text": "Gift box labeled \u2018Innovation Energizer\u2019 with a rocket logo, truck, and snowy background."}, {"type": "text", "text": "Dear friends, I\u2019m thrilled that former students and postdocs of mine won both of this year\u2019s NeurIPS Test of Time Paper Awards. This award recognizes papers published 10 years ago that have significantly shaped the research field. The recipients included Ian Goodfellow (who, as an undergraduate, built my first GPU server for deep learning in his dorm room) and his collaborators for their work on generative adversarial networks, and my former postdoc Ilya Sutskever and PhD student Quoc Le (with Oriol Vinyals) for their work on sequence-to-sequence learning. Congratulations to all these winners! By nature, I tend to focus on the future rather than the past. Steve Jobs famously declined to build a corporate museum, instead donating Apple's archives to Stanford University, because he wanted to keep the company forward-looking. Jeff Bezos encourages teams to approach every day as if it were \u201cDay 1,\u201d a mindset that emphasizes staying in the early, innovative stage of a company or industry. These philosophies resonate with me. But taking a brief look at the past can help us reflect on lessons for the future. One takeaway from looking at what worked 10 to 15 years ago is that many of the teams I led bet heavily on scaling to drive AI progress \u2014 a bet that laid a foundation to build larger and larger AI systems. At the time, the idea of scaling up neural networks was controversial, and I was on the fringe. I recall distinctly that, around 2008, Yoshua Bengio advised me not to bet on scaling and to focus on inventing algorithms instead! A lesson I carry from that time is to not worry about what others think, but follow your convictions, especially if you have data to support your beliefs. Small-scale experiments performed by my Stanford group convinced me that scaling up neural networks would drive significant progress, and that\u2019s why I was willing to ignore the skeptics. The diagram below, generated by Adam Coates and Honglak Lee, is the one that most firmed up my beliefs at that time. It shows that, for a range of models, the larger we scaled them, the better they perform. I remember presenting it at CIFAR 2010, and if I had to pick a single reason why I pushed through to start Google Brain and set as the team\u2019s #1 goal to scale up deep learning algorithms, it is this diagram! I also remember presenting at NeurIPS in 2008 our work on using GPUs to scale up training neural networks. (By the way, one measure of success in academia is when your work becomes sufficiently widely accepted that no one cites it anymore. I\u2019m quite pleased the idea that GPUs should be used for AI \u2014 which was controversial back then \u2014 is now such a widely accepted \u201cfact\u201d that no one bothers to cite early papers that pushed for it.\ud83d\ude03) When I started Google Brain, the thesis was simple: I wanted to use the company\u2019s huge computing capability to scale up deep learning. Shortly afterward, I built Stanford\u2019s first supercomputer for deep learning", "article_id": "68497b48235d602d65122588", "linked_images": ["article_68497b48235d602d65122588_img_0", "article_68497b48235d602d65122588_img_1", "article_68497b48235d602d65122588_img_2", "article_68497b48235d602d65122588_img_3", "article_68497b48235d602d65122588_img_4", "article_68497b48235d602d65122588_img_5"]}, {"type": "text", "text": "such a widely accepted \u201cfact\u201d that no one bothers to cite early papers that pushed for it.\ud83d\ude03) When I started Google Brain, the thesis was simple: I wanted to use the company\u2019s huge computing capability to scale up deep learning. Shortly afterward, I built Stanford\u2019s first supercomputer for deep learning using GPUs, since I could move faster at Stanford than within a large company. A few years later, my team at Baidu showed that as you scale up a model, its performance improves linearly on a log-log scale, which was a precursor to OpenAI\u2019s scaling laws. As I look to the future, I\u2019m sure there are ideas that many people are skeptical about today, but will prove to be accurate. Scaling up AI models turned out to be useful for many teams, and it continues to be exciting, but now I\u2019m even more excited by upcoming ideas that will prove to be even more valuable in the future. This past year, I spent a lot of time encouraging teams to build applications with agentic AI and worked to share best practices. I have a few hypotheses for additional technologies that will be important next year. I plan to spend the winter holiday playing with a few of them, and I will have more to share next year. But if you have an idea that you have conviction on, so long as you can do so responsibly, I encourage you to pursue it! Keep learning, Andrew In our latest short course, you\u2019ll learn how to use OpenAI o1 for advanced reasoning in tasks like coding, planning, and image analysis. Explore tradeoffs between intelligence gains and cost as well as techniques, such as meta prompting, to optimize performance. Enroll now! Microsoft updated its smallest model family with a single, surprisingly high-performance model. What\u2019s new: Marah Abdin and a team at Microsoft released Phi-4, a large language model of 14 billion parameters that outperforms Llama 3.3 70B and Qwen 2.5 (72 billion parameters) on math and reasoning benchmarks. The model is available at Azure AI Foundry under a license that permits non-commercial uses, and the weights will be released via Hugging Face next week. How it works: Phi-4 is a transformer that processes up to 16,000 tokens of input context. The ways the authors constructed the pretraining and fine-tuning datasets accounts for most of its performance advantage over other models. Results: Of 13 benchmarks, Phi-4 outperforms Llama 3.3 70B (its most recent open weights competitor) on six and Qwen 2.5 on five. Why it matters: Phi-4 shows that there\u2019s still room to improve the performance of small models by curating training data, following the age-old adage that better data makes a better model. We\u2019re thinking: Some researchers found that earlier versions of Phi showed signs of overfitting to certain benchmarks. In their paper, the Microsoft team stressed that they had improved the data decontamination process for Phi-4 and added an appendix on their method. We trust that independent tests will show that Phi-4 is as impressive as its benchmark scores suggest. The gap is narrowing", "article_id": "68497b48235d602d65122588", "linked_images": ["article_68497b48235d602d65122588_img_0", "article_68497b48235d602d65122588_img_1", "article_68497b48235d602d65122588_img_2", "article_68497b48235d602d65122588_img_3", "article_68497b48235d602d65122588_img_4", "article_68497b48235d602d65122588_img_5"]}, {"type": "text", "text": "of overfitting to certain benchmarks. In their paper, the Microsoft team stressed that they had improved the data decontamination process for Phi-4 and added an appendix on their method. We trust that independent tests will show that Phi-4 is as impressive as its benchmark scores suggest. The gap is narrowing between closed and open models for video generation. What\u2019s new: Tencent released HunyuanVideo, a video generator that delivers performance competitive with commercial models. The model is available as open code and open weights for developers who have less than a 100 million monthly users and live outside the EU, UK, and South Korea. How it works: HunyuanVideo comprises a convolutional video encoder-decoder, two text encoders, a time-step encoder, and a transformer. The team trained the model in stages (first the encoder-decoder, then the system as a whole) using undisclosed datasets before fine-tuning the system. Results: 60 people judged responses to 1,533 text prompts by HunyuanVideo, Gen-3 and Luma 1.6. The judges preferred HunyuanVideo\u2019s output overall. Examining the systems\u2019 output in more detail, they preferred HunyuanVideo\u2019s quality of motion but Gen-3\u2019s visual quality. Behind the news: In February, OpenAI\u2019s announcement of Sora (which was released as this article was in production) marked a new wave of video generators that quickly came to include Google Veo, Meta Movie Gen, Runway Gen-3 Alpha, and Stability AI Stable Video Diffusion. Open source alternatives like Mochi continue to fall short of publicly available commercial video generators. Why it matters: Research in image generation has advanced at a rapid pace, while progress in video generation has been slower. One reason may be the cost of processing, which is especially intensive when it comes to video. The growing availability of pretrained, open source video generators could accelerate the pace by relieving researchers of the need to pretrain models and enabling them to experiment with fine-tuning and other post-training for specific tasks and applications. We\u2019re thinking: Tencent\u2019s open source models are great contributions to research and development in video generation. It\u2019s exciting to see labs in China contributing high-performance models to the open source community! Google\u2019s Gemini 2.0 Flash, the first member of its updated Gemini family of large multimodal models, combines speed with performance that exceeds that of its earlier flagship model, Gemini 1.5 Pro, on several measures. What\u2019s new: Gemini 2.0 Flash processes an immense 2 million tokens of input context including text, images, video, and speech, and generates text, images, and speech. Text input/output is available in English, Spanish, Japanese, Chinese, and Hindi, while speech input/output is available in English only for now. It can use tools, generate function calls, and respond to a real-time API \u2014 capabilities that underpin a set of pre-built agents that perform tasks like research and coding. Gemini 2.0 Flash is available for free in an experimental preview version via Google AI Studio, Google Developer API, and Gemini Chat. How it works: Gemini 2.0 Flash (parameter count undisclosed) matches or outperforms several competing models on key benchmarks, according to Google\u2019s report. Agents at your service: Google also introduced four agents that take", "article_id": "68497b48235d602d65122588", "linked_images": ["article_68497b48235d602d65122588_img_0", "article_68497b48235d602d65122588_img_1", "article_68497b48235d602d65122588_img_2", "article_68497b48235d602d65122588_img_3", "article_68497b48235d602d65122588_img_4", "article_68497b48235d602d65122588_img_5"]}, {"type": "text", "text": "for free in an experimental preview version via Google AI Studio, Google Developer API, and Gemini Chat. How it works: Gemini 2.0 Flash (parameter count undisclosed) matches or outperforms several competing models on key benchmarks, according to Google\u2019s report. Agents at your service: Google also introduced four agents that take advantage of Gemini 2.0 Flash\u2019s ability to use tools, call functions, and respond to the API in real time. Most are available via a waitlist. Behind the news: OpenAI showed off GPT-4o\u2019s capability for real-time video understanding in May, but Gemini 2.0 Flash beat it to the punch: Google launched the new model and its multimodal API one day ahead of ChatGPT\u2019s Advanced Voice with Vision. Why it matters: Speed and multimodal input/output are valuable characteristics for any AI model, and they\u2019re especially useful in agentic applications. Google CEO Sundar Pichai said he wants Gemini to be a \u201cuniversal assistant.\u201d The new Gemini-based applications for coding, research, and video analysis are steps in that direction. We\u2019re thinking: While other large language models can take advantage of search, Gemini 2.0 Flash generates calls to Google Search and uses that capability in agentic tools \u2014 a demonstration of how Google\u2019s dominance in search strengthens its efforts in AI. How do agents based on large language models compare to human experts when it comes to proposing machine learning research? Pretty well, according to one study. What\u2019s new: Chenglei Si, Diyi Yang, and Tatsunori Hashimoto at Stanford produced ideas for research in machine learning using Anthropic\u2019s Claude 3.5 Sonnet and human researchers, and also evaluated them using both manual and automated methods. Claude 3.5 Sonnet generated competitive proposals, but its evaluations of proposals were less compelling. How it works: Each proposal included a problem statement, motivation, step-by-step plan, backup plan, and examples of baseline outcomes versus expected experimental outcomes. Results: Human judges deemed proposals generated by Claude 3.5 Sonnet as good as or better than those produced by humans. However, large language models proved less effective at judging the proposals\u2019 quality. Why it matters: AI models play a growing role in scientific discovery. This work shows they can set directions for research \u2014 in machine learning, at least \u2014 that rival those set by humans. However, human evaluation remains the gold standard for comparing performance on complex problems like generating text. We\u2019re thinking: Coming up with good research ideas is hard! That a large language model can do it with some competency has exciting implications for the future of both AI and science.", "article_id": "68497b48235d602d65122588", "linked_images": ["article_68497b48235d602d65122588_img_0", "article_68497b48235d602d65122588_img_1", "article_68497b48235d602d65122588_img_2", "article_68497b48235d602d65122588_img_3", "article_68497b48235d602d65122588_img_4", "article_68497b48235d602d65122588_img_5"]}, {"type": "image", "article_id": "68497b48235d602d65122588", "linked_chunks": ["article_68497b48235d602d65122588_chunk_0", "article_68497b48235d602d65122588_chunk_1", "article_68497b48235d602d65122588_chunk_2", "article_68497b48235d602d65122588_chunk_3"], "alt_text": "Graph showing cross-validation accuracy vs. number of features for raw and whitened inputs."}, {"type": "image", "article_id": "68497b48235d602d65122588", "linked_chunks": ["article_68497b48235d602d65122588_chunk_0", "article_68497b48235d602d65122588_chunk_1", "article_68497b48235d602d65122588_chunk_2", "article_68497b48235d602d65122588_chunk_3"], "alt_text": "Promo banner for \"Reasoning with o1\""}, {"type": "image", "article_id": "68497b48235d602d65122588", "linked_chunks": ["article_68497b48235d602d65122588_chunk_0", "article_68497b48235d602d65122588_chunk_1", "article_68497b48235d602d65122588_chunk_2", "article_68497b48235d602d65122588_chunk_3"], "alt_text": "Benchmark results for Phi-4, GPT, LLaMA-3.3, and Qwen 2.5 models."}, {"type": "image", "article_id": "68497b48235d602d65122588", "linked_chunks": ["article_68497b48235d602d65122588_chunk_0", "article_68497b48235d602d65122588_chunk_1", "article_68497b48235d602d65122588_chunk_2", "article_68497b48235d602d65122588_chunk_3"], "alt_text": "A GIF with scenes of a man at a caf\u00e9, a working robot, a ghost in a mirror, and a speeding truck."}, {"type": "image", "article_id": "68497b48235d602d65122588", "linked_chunks": ["article_68497b48235d602d65122588_chunk_0", "article_68497b48235d602d65122588_chunk_1", "article_68497b48235d602d65122588_chunk_2", "article_68497b48235d602d65122588_chunk_3"], "alt_text": "Performance comparison for Gemini models across benchmarks."}, {"type": "image", "article_id": "68497b48235d602d65122588", "linked_chunks": ["article_68497b48235d602d65122588_chunk_0", "article_68497b48235d602d65122588_chunk_1", "article_68497b48235d602d65122588_chunk_2", "article_68497b48235d602d65122588_chunk_3"], "alt_text": "Animation showcasing 7 key NLP topics visually expanding on the screen."}, {"type": "alt_text", "alt_text": "Graph showing cross-validation accuracy vs. number of features for raw and whitened inputs."}, {"type": "alt_text", "alt_text": "Promo banner for \"Reasoning with o1\""}, {"type": "alt_text", "alt_text": "Benchmark results for Phi-4, GPT, LLaMA-3.3, and Qwen 2.5 models."}, {"type": "alt_text", "alt_text": "A GIF with scenes of a man at a caf\u00e9, a working robot, a ghost in a mirror, and a speeding truck."}, {"type": "alt_text", "alt_text": "Performance comparison for Gemini models across benchmarks."}, {"type": "alt_text", "alt_text": "Animation showcasing 7 key NLP topics visually expanding on the screen."}, {"type": "text", "text": "Dear friends, AI Product Management is evolving rapidly. The growth of generative AI and AI-based developer tools has created numerous opportunities to build AI applications. This is making it possible to build new kinds of things, which in turn is driving shifts in best practices in product management \u2014 the discipline of defining what to build to serve users \u2014 because what is possible to build has shifted. In this letter, I\u2019ll share some best practices I have noticed. Use concrete examples to specify AI products. Starting with a concrete idea helps teams gain speed. If a product manager (PM) proposes to build \u201ca chatbot to answer banking inquiries that relate to user accounts,\u201d this is a vague specification that leaves much to the imagination. For instance, should the chatbot answer questions only about account balances or also about interest rates, processes for initiating a wire transfer, and so on? But if the PM writes out a number (say, between 10 and 50) of concrete examples of conversations they\u2019d like a chatbot to execute, the scope of their proposal becomes much clearer. Just as a machine learning algorithm needs training examples to learn from, an AI product development team needs concrete examples of what we want an AI system to do. In other words, the data is your PRD (product requirements document)! In a similar vein, if someone requests \u201ca vision system to detect pedestrians outside our store,\u201d it\u2019s hard for a developer to understand the boundary conditions. Is the system expected to work at night? What is the range of permissible camera angles? Is it expected to detect pedestrians who appear in the image even though they\u2019re 100m away? But if the PM collects a handful of pictures and annotates them with the desired output, the meaning of \u201cdetect pedestrians\u201d becomes concrete. An engineer can assess if the specification is technically feasible and if so, build toward it. Initially, the data might be obtained via a one-off, scrappy process, such as the PM walking around taking pictures and annotating them. Eventually, the data mix will shift to real-word data collected by a system running in production. Using examples (such as inputs and desired outputs) to specify a product has been helpful for many years, but the explosion of possible AI applications is creating a need for more product managers to learn this practice. Assess technical feasibility of LLM-based applications by prompting. When a PM scopes out a potential AI application, whether the application can actually be built \u2014 that is, its technical feasibility \u2014 is a key criterion in deciding what to do next. For many ideas for LLM-based applications, it\u2019s increasingly possible for a PM, who might not be a software engineer, to try prompting \u2014 or write just small amounts of code \u2014 to get an initial sense of feasibility. For example, a PM may envision a new internal tool for routing emails from customers to the right department (such as customer service, sales, etc.). They can prompt an LLM to see if they can get it to select", "article_id": "68497b49235d602d6512258f", "linked_images": ["article_68497b49235d602d6512258f_img_0", "article_68497b49235d602d6512258f_img_1", "article_68497b49235d602d6512258f_img_2", "article_68497b49235d602d6512258f_img_3", "article_68497b49235d602d6512258f_img_4", "article_68497b49235d602d6512258f_img_5"]}, {"type": "text", "text": "amounts of code \u2014 to get an initial sense of feasibility. For example, a PM may envision a new internal tool for routing emails from customers to the right department (such as customer service, sales, etc.). They can prompt an LLM to see if they can get it to select the right department based on an input email, and see if they can achieve high accuracy. If so, this gives engineering a great starting point from which to implement the tool. If not, the PM can falsify the idea themselves and perhaps improve the product idea much faster than if they had to rely on an engineer to build a prototype. Often, testing feasibility requires a little more than prompting. For example, perhaps the LLM-based email system needs basic RAG capability to help it make decisions. Fortunately, the barrier to writing small amounts of code is now quite low, since AI can help by acting as a coding companion, as I describe in the course, \u201cAI Python for Beginners.\u201d This means that PMs can do much more technical feasibility testing, at least at a basic level, than was possible before. Prototype and test without engineers. User feedback to initial prototypes is also instrumental to shaping products. Fortunately, barriers to building prototypes rapidly are falling, and PMs themselves can move prototypes forward without needing software developers. In addition to using LLMs to help write code for prototyping, tools like Replit, Vercel\u2019s V0, Bolt, and Anthropic\u2019s Artifacts (I\u2019m a fan of all of these!) are making it easier for people without a coding background to build and experiment with simple prototypes. These tools are increasingly accessible to non-technical users, though I find that those who understand basic coding are able to use them much more effectively, so it\u2019s still important to learn basic coding. (Interestingly, highly technical, experienced developers use them too!) Many members of my teams routinely use such tools to prototype, get user feedback, and iterate quickly. AI is enabling a lot of new applications to be built, creating massive growth in demand for AI product managers who know how to scope out and help drive progress in building these products. AI product management existed before the rise of generative AI, but the increasing ease of building applications is creating greater demand for AI applications, and thus a lot of PMs are learning AI and these emerging best practices for building AI products. I find this discipline fascinating, and will keep on sharing best practices as they grow and evolve. Keep learning! Andrew Write and code more effectively with OpenAI Canvas, a user-friendly workspace for collaborating with AI. In this free course, explore use cases like building game apps and designing SQL databases from screenshots, and gain insights into how GPT-4o powers Canvas\u2019 features. Join for free Amazon introduced a range of models that confront competitors head-on. What\u2019s new: The Nova line from Amazon includes three vision-language models (Nova Premier, Nova Pro, and Nova Lite), one language model (Nova Micro), an image generator (Nova Canvas), and a video generator (Nova Reel). All", "article_id": "68497b49235d602d6512258f", "linked_images": ["article_68497b49235d602d6512258f_img_0", "article_68497b49235d602d6512258f_img_1", "article_68497b49235d602d6512258f_img_2", "article_68497b49235d602d6512258f_img_3", "article_68497b49235d602d6512258f_img_4", "article_68497b49235d602d6512258f_img_5"]}, {"type": "text", "text": "Canvas\u2019 features. Join for free Amazon introduced a range of models that confront competitors head-on. What\u2019s new: The Nova line from Amazon includes three vision-language models (Nova Premier, Nova Pro, and Nova Lite), one language model (Nova Micro), an image generator (Nova Canvas), and a video generator (Nova Reel). All but Nova Premier are available on Amazon\u2019s Bedrock platform, and Nova Premier, which is the most capable, is expected in early 2025. In addition, Amazon plans to release a speech-to-speech model in early 2025 and a multimodal model that processes text, images, video, and audio by mid-year. (Disclosure: Andrew Ng serves on Amazon\u2019s board of directors.) How it works: Nova models deliver competitive performance at relatively low prices. Amazon hasn\u2019t disclosed parameter counts or details about how the models were built except to say that Nova Pro, Lite, and Micro were trained on a combination of proprietary, licensed, public, and open-source text, images, and video in over 200 languages. Behind the news: The company launched Bedrock in April 2023 with Stability AI\u2019s Stable Diffusion for image generation, Anthropic\u2019s Claude and AI21\u2019s Jurassic-2 for text generation, and its own Titan models for text generation and embeddings. Not long afterward, it added language models from Cohere as well as services for agentic applications and medical applications. It plans to continue to provide models from other companies (including Anthropic), offering a range of choices. Why it matters: While other AI giants raced to outdo one another in models for text and multimodal processing, Amazon was relatively quiet. With Nova, it has staked out a strong position in those areas, as well as the startup-dominated domains of image and video generation. Moreover, it\u2019s strengthening its cloud AI offerings with competitive performance, pricing, and speed. Nova\u2019s pricing continues the rapid drop in AI prices over the last year. Falling per-token prices help make AI agents or applications that process large inputs more practical. For example, Simon Willison, developer of the Django Python framework for web applications, found that Nova Lite generated descriptions for his photo library (tens of thousands of images) for less than $10. We\u2019re thinking: The Nova suite is available via APIs as well as two web playgrounds (one in the Bedrock console, the other a new interface for building AI apps called PartyRock). This accords with Amazon Web Services\u2019 focus on developers. For consumers, Amazon offers the earlier Rufus shopping bot; for enterprises, the Q assistant. OpenAI launched not only its highly anticipated o1 model but also an operating mode that enables the model to deliver higher performance \u2014 at a hefty price. What\u2019s new: Kicking off a 12-day holiday blitz, OpenAI launched o1 (previously available in preview and mini versions) and introduced o1 pro mode, which processes more tokens at inference to produce more accurate output. Both options accept text and image inputs to generate text outputs. They\u2019re available exclusively through a new ChatGPT Pro subscription for $200 monthly. API access is not yet available. How it works: According to an updated system card, o1 models were trained on a mix of public,", "article_id": "68497b49235d602d6512258f", "linked_images": ["article_68497b49235d602d6512258f_img_0", "article_68497b49235d602d6512258f_img_1", "article_68497b49235d602d6512258f_img_2", "article_68497b49235d602d6512258f_img_3", "article_68497b49235d602d6512258f_img_4", "article_68497b49235d602d6512258f_img_5"]}, {"type": "text", "text": "more accurate output. Both options accept text and image inputs to generate text outputs. They\u2019re available exclusively through a new ChatGPT Pro subscription for $200 monthly. API access is not yet available. How it works: According to an updated system card, o1 models were trained on a mix of public, licensed, and proprietary text, code, and images, with a focus on technical, academic, and structured datasets. They respond to prompts by breaking them down into intermediate steps, each of which consumes a number of hidden \u201creasoning tokens.\u201d The models don\u2019t reveal these steps, but ChatGPT presents a natural-language summary of the reasoning process. The new o1 and o1 pro mode perform better than o1-preview and o1-mini, but their additional reasoning requires more processing, which translates into higher costs and slower responses. Behind the news: Since September, when OpenAI introduced o1-preview and o1-mini, other model providers have implemented similar reasoning capabilities. DeepSeek\u2019s R1 displays reasoning steps that o1 models keep hidden. Alibaba\u2019s QwQ 32B excels at visual reasoning but is slower and has a smaller context window. Amazon\u2019s Nova Premier, which is billed as a model for \u201ccomplex reasoning tasks,\u201d is expected in early 2025, but Amazon has not yet described its performance, architecture, or other details. Why it matters: o1 and o1 pro mode highlight a dramatic shift in model development and pricing. Giving models more processing power at inference enables them to provide more accurate output, and it\u2019s a key part of agentic workflows. It also continues to boost performance even as scaling laws that predict better performance with more training data and compute may be reaching their limits. However, it also raises OpenAI\u2019s costs, and at $200 a month, the price of access to o1 and o1 pro is steep. It\u2019s a premium choice for developers who require exceptional accuracy or extensive reasoning. We\u2019re thinking: Discovering scaling laws for using more processing at inference, or test-time compute, is an unsolved problem. Although OpenAI hasn\u2019t disclosed the algorithm behind o1 pro mode, recent work at Google allocated tokens dynamically at inference based on a prompt\u2019s difficulty. This approach boosted the compute efficiency by four times and enabled a model that had shown \u201cnontrivial success rates\u201d to outperform one that was 14 times larger. A new model improves on recent progress in generating interactive virtual worlds from still images. What\u2019s new: Jack Parker-Holder and colleagues from Google introduced Genie 2, which generates three-dimensional video game worlds that respond to keyboard inputs in real time. The model\u2019s output remains consistent (that is, elements don\u2019t morph or disappear) for up to a minute, and it includes first-person shooters, walking simulators, and driving games from viewpoints that include first person, third person, and isometric. Genie 2 follows up on Genie, which generates two-dimensional games. How it works: Genie 2 is a latent diffusion model that generates video frames made up of an encoder, transformer, and decoder. The developers didn\u2019t reveal how they built it or how they improved on earlier efforts. Behind the news: Genie 2 arrives on the heels of Oasis, which generates a", "article_id": "68497b49235d602d6512258f", "linked_images": ["article_68497b49235d602d6512258f_img_0", "article_68497b49235d602d6512258f_img_1", "article_68497b49235d602d6512258f_img_2", "article_68497b49235d602d6512258f_img_3", "article_68497b49235d602d6512258f_img_4", "article_68497b49235d602d6512258f_img_5"]}, {"type": "text", "text": "it works: Genie 2 is a latent diffusion model that generates video frames made up of an encoder, transformer, and decoder. The developers didn\u2019t reveal how they built it or how they improved on earlier efforts. Behind the news: Genie 2 arrives on the heels of Oasis, which generates a Minecraft-like game in real time. Unlike Oasis, Genie 2 worlds are more consistent and not limited to one type of game. It also comes at the same time as another videogame generator, World Labs. However, where Genie 2 generates the next frame given previous frames and keyboard input (acting, in terms of game development, as both graphics and physics engines), World Labs generates a 3D mesh of a game world from a single 2D image. This leaves the implementation of physics, graphics rendering, the player\u2019s character, and other game mechanics to external software. Why it matters: Genie 2 extends models that visualize 3D scenes based on 2D images to encompass interactive worlds, a capability that could prove valuable in design, gaming, virtual reality, and other 3D applications. It generates imagery that, the authors suggest, could serve as training data for agents to learn how to navigate and respond to commands in 3D environments. We\u2019re thinking: Generating gameplay directly in the manner of Genie 2 is a quick approach to developing a game, but the current technology comes with caveats. Developers can\u2019t yet control a game\u2019s physics or mechanics and they must manage any flaws in the model (such as a tendency to generate inconsistent worlds). In contrast, generating a 3D mesh, as World Labs does, is a more cumbersome approach, but it gives developers more control. Large language models that remember more hallucinate less. What\u2019s new: Johnny Li and colleagues at Lamini introduced Mixture of Memory Experts (MoME), a method that enables large language models (LLMs) to memorize many facts with relatively modest computational requirements. (Disclosure: Andrew Ng invested in Lamini.) Key insight: The key to getting factual answers from LLMs is to keep training it until it chooses the correct answer every time. In technical terms, train past the point where tokens relevant to the answer have a similar probability distribution, and continue until a single token has 100 percent probability. But this amount of training takes a lot of computation and, since the model may overfit the training set, it also may degrade performance on the test set. Fine-tuning is one solution, and fine-tuning a LoRA adapter to memorize facts reduces the computational burden. But a single LoRA adapter isn\u2019t enough to store all of the knowledge in a large dataset. Training multiple adapters that are selected by cross-attention enables the LLM to memorize a variety of facts. How it works: The authors extended a pretrained Llama-3-8B with a large number (on the order of 1 million) of LoRA adapters and a cross-attention layer. They froze Llama-3-8B and trained the LoRA adapters to predict the next token in a custom dataset of over 1 million questions and answers. Results: The authors tested their LoRA-enhanced model\u2019s ability to answer questions about", "article_id": "68497b49235d602d6512258f", "linked_images": ["article_68497b49235d602d6512258f_img_0", "article_68497b49235d602d6512258f_img_1", "article_68497b49235d602d6512258f_img_2", "article_68497b49235d602d6512258f_img_3", "article_68497b49235d602d6512258f_img_4", "article_68497b49235d602d6512258f_img_5"]}, {"type": "text", "text": "number (on the order of 1 million) of LoRA adapters and a cross-attention layer. They froze Llama-3-8B and trained the LoRA adapters to predict the next token in a custom dataset of over 1 million questions and answers. Results: The authors tested their LoRA-enhanced model\u2019s ability to answer questions about a database via SQL queries. The model, which was outfitted for retrieval-augmented generation (RAG), achieved 94.7 percent accuracy. An unnamed model with RAG achieved 50 percent accuracy. Yes, but: It stands to reason that the authors\u2019 approach saves processing, but it\u2019s unclear how much. The authors didn\u2019t mention the cost of fine-tuning Llama-3-8B in the usual way on their training dataset for the same number of epochs. Why it matters: The authors argue that eliminating hallucinations is possible in typical training, it\u2019s just computationally very expensive (not to mention the risk of overfitting). An architecture designed to store and retrieve facts, via LoRA adapters in this case, makes the process more feasible. We\u2019re thinking: While some researchers want large language models to memorize facts, others want them to avoid memorizing their training data. These aims address very different problems. Preventing LLMs from memorizing training data would make them less likely to regurgitate it verbatim and thus violate copyrights. On the other hand, this work memorizes facts so the model can deliver consistent, truthful responses that might be stated in a variety of ways.", "article_id": "68497b49235d602d6512258f", "linked_images": ["article_68497b49235d602d6512258f_img_0", "article_68497b49235d602d6512258f_img_1", "article_68497b49235d602d6512258f_img_2", "article_68497b49235d602d6512258f_img_3", "article_68497b49235d602d6512258f_img_4", "article_68497b49235d602d6512258f_img_5"]}, {"type": "image", "article_id": "68497b49235d602d6512258f", "linked_chunks": ["article_68497b49235d602d6512258f_chunk_0", "article_68497b49235d602d6512258f_chunk_1", "article_68497b49235d602d6512258f_chunk_2", "article_68497b49235d602d6512258f_chunk_3", "article_68497b49235d602d6512258f_chunk_4", "article_68497b49235d602d6512258f_chunk_5"], "alt_text": "Cartoon showing people stuck in wet concrete, with a person saying \u2018You asked for a concrete idea!\u2019"}, {"type": "image", "article_id": "68497b49235d602d6512258f", "linked_chunks": ["article_68497b49235d602d6512258f_chunk_0", "article_68497b49235d602d6512258f_chunk_1", "article_68497b49235d602d6512258f_chunk_2", "article_68497b49235d602d6512258f_chunk_3", "article_68497b49235d602d6512258f_chunk_4", "article_68497b49235d602d6512258f_chunk_5"], "alt_text": "Promo banner for \"Collaborative Writing and Coding with OpenAI Canvas\""}, {"type": "image", "article_id": "68497b49235d602d6512258f", "linked_chunks": ["article_68497b49235d602d6512258f_chunk_0", "article_68497b49235d602d6512258f_chunk_1", "article_68497b49235d602d6512258f_chunk_2", "article_68497b49235d602d6512258f_chunk_3", "article_68497b49235d602d6512258f_chunk_4", "article_68497b49235d602d6512258f_chunk_5"], "alt_text": "Berkeley Function Calling Leaderboard with metrics like accuracy, latency, and relevance."}, {"type": "image", "article_id": "68497b49235d602d6512258f", "linked_chunks": ["article_68497b49235d602d6512258f_chunk_0", "article_68497b49235d602d6512258f_chunk_1", "article_68497b49235d602d6512258f_chunk_2", "article_68497b49235d602d6512258f_chunk_3", "article_68497b49235d602d6512258f_chunk_4", "article_68497b49235d602d6512258f_chunk_5"], "alt_text": "o1 Family Benchmarks comparing pass rates across AIME, Codeforces, and GPQA."}, {"type": "image", "article_id": "68497b49235d602d6512258f", "linked_chunks": ["article_68497b49235d602d6512258f_chunk_0", "article_68497b49235d602d6512258f_chunk_1", "article_68497b49235d602d6512258f_chunk_2", "article_68497b49235d602d6512258f_chunk_3", "article_68497b49235d602d6512258f_chunk_4", "article_68497b49235d602d6512258f_chunk_5"], "alt_text": "Game character climbing a ladder with visible controls (QWASD) and health bars."}, {"type": "image", "article_id": "68497b49235d602d6512258f", "linked_chunks": ["article_68497b49235d602d6512258f_chunk_0", "article_68497b49235d602d6512258f_chunk_1", "article_68497b49235d602d6512258f_chunk_2", "article_68497b49235d602d6512258f_chunk_3", "article_68497b49235d602d6512258f_chunk_4", "article_68497b49235d602d6512258f_chunk_5"], "alt_text": "Graph showing how training loss affects token prediction accuracy and hallucination elimination."}, {"type": "alt_text", "alt_text": "Cartoon showing people stuck in wet concrete, with a person saying \u2018You asked for a concrete idea!\u2019"}, {"type": "alt_text", "alt_text": "Promo banner for \"Collaborative Writing and Coding with OpenAI Canvas\""}, {"type": "alt_text", "alt_text": "Berkeley Function Calling Leaderboard with metrics like accuracy, latency, and relevance."}, {"type": "alt_text", "alt_text": "o1 Family Benchmarks comparing pass rates across AIME, Codeforces, and GPQA."}, {"type": "alt_text", "alt_text": "Game character climbing a ladder with visible controls (QWASD) and health bars."}, {"type": "alt_text", "alt_text": "Graph showing how training loss affects token prediction accuracy and hallucination elimination."}, {"type": "text", "text": "Dear friends, There\u2019s a lingering misconception that building with generative AI is expensive. It is indeed expensive to train cutting-edge foundation models, and a number of companies have spent billions of dollars doing this (and even released some of their models as open weights). But as a result, it\u2019s now very inexpensive to build a wide range of AI applications. The AI stack has several layers, shown in the diagram below. Here are the lower layers, from the bottom up: The foundation model layer frequently appears in headlines because foundation models cost so much to build. Some companies have made massive investments in training these models, and a few of those have added to the hype by pointing out that paying lots for compute and data would lead (probably) to predictably better performance following scaling laws. This layer is also currently hyper-competitive, and switching costs for application developers to move from one model to another are fairly low (for example, requiring changes to just a few lines of code). Sequoia Capital\u2019s thoughtful article on \u201cAI's $600B Question\u201d points out that, to justify massive capital investments in AI infrastructure (particularly GPU purchases and data center buildouts), generative AI needs to get around $600B of revenue. This has made investing at the foundation model layer challenging. It\u2019s expensive, and this sector still needs to figure out how to deliver returns. (I\u2019m cautiously optimistic it will work out!) On top of this layer is an emerging orchestration layer, which provides software that helps coordinate multiple calls to LLMs and perhaps to other APIs. This layer is becoming increasingly agentic. For example, Langchain has helped many developers build LLM applications, and its evolution into LangGraph for building agents has been a great development. Other platforms such as Autogen, MemGPT, and CrewAI (disclosure: I made a personal investment in CrewAI) are also making it easier to build agentic workflows. Switching costs for this layer are much higher than for the foundation model layer, since, if you\u2019ve built an agent on one of these frameworks, it\u2019s a lot of work to switch to a different one. Still, competition in the orchestration layer, as in the foundation model layer, seems intense. Finally, there\u2019s the application layer. Almost by definition, this layer has to do better financially than all the layers below. In fact, for investments at the lower layers to make financial sense, the applications had better generate even more revenue, so the application vendors can afford to pay providers of infrastructure, cloud computing, foundation models, and orchestration. (This is why my team AI Fund focuses primarily on AI application companies, as I discussed in a talk.) Fortunately, because of the massive investments in foundation models, it\u2019s now incredibly inexpensive to experiment and build prototypes in the applications layer! Over Thanksgiving holiday, I spent about one and a half days prototyping different generative AI applications, and my bill for OpenAI API calls came out to about $3. On my personal AWS account, which I use for prototyping and experimentation, my most recent monthly bill was $35.30. I find it", "article_id": "68497b49235d602d65122596", "linked_images": ["article_68497b49235d602d65122596_img_0", "article_68497b49235d602d65122596_img_1", "article_68497b49235d602d65122596_img_2", "article_68497b49235d602d65122596_img_3", "article_68497b49235d602d65122596_img_4", "article_68497b49235d602d65122596_img_5"]}, {"type": "text", "text": "Over Thanksgiving holiday, I spent about one and a half days prototyping different generative AI applications, and my bill for OpenAI API calls came out to about $3. On my personal AWS account, which I use for prototyping and experimentation, my most recent monthly bill was $35.30. I find it amazing how much fun you can have on these platforms for a small number of dollars! By building on widely available AI tools, AI Fund now budgets $55,000 to get to a working prototype. And while that is quite a lot of money, it\u2019s far less than the billions companies are raising to develop foundation models. Individuals and businesses can experiment and test important ideas at reasonable cost. Keep learning! Andrew Starting your career in AI has never been easier with Machine Learning Specialization, a foundational program for beginners in machine learning. Get started! One of the world\u2019s biggest payment processors is enabling large language models to spend real money. What\u2019s new: Stripe announced Stripe Agent Toolkit, a library for Python and Typescript that supports agentic workflows that use API calls to execute monetary transactions. You can download it here. How it works: An agentic purchasing workflow may look like this: A user asks the agent to find a flight to a certain destination, on a certain schedule, with a certain price limit; and an LLM queries a flight database, chooses a flight, obtains authorization from the user, and purchases the flight. Stripe Agent Toolkit supports agentic workflow frameworks from CrewAI, LangChain, and Vercel. It doesn\u2019t yet implement all of Stripe\u2019s API, but Stripe expects to extend it in the future. Why it matters: Agents that can spend money securely open a wide variety of applications. Stripe\u2019s API previously made it possible to enable an LLM-based application to make purchases online, but doing so required trusting the LLM to generate the right API calls and not to make inappropriate ones. The new library makes it easier to enforce spending limits and API constraints, and thus to build agents that engage in ecommerce safely. We\u2019re thinking: Stripe\u2019s offering helps developers build agents that are cents-ible! Mistral AI unveiled Pixtral Large, which rivals top models at processing combinations of text and images. What\u2019s new: Pixtral Large outperforms a number of leading vision-language models on some tasks. The weights are free for academic and non-commercial use and can be licensed for business use. Access is available via Mistral AI\u2019s website or API for $2/$6 per million tokens for input/output. In addition, Pixtal Large now underpins le Chat, Mistral AI\u2019s chatbot, which also gained several new features. How it works: Pixtral Large generates text in response to text and images in dozens of languages. It processes 131,072 tokens of context, which is sufficient to track relationships among 30 high-resolution images at a time. Based on Mistral Large 2 (a 123 billion-parameter large language model) and a 1 billion-parameter vision encoder, it demonstrates strong performance across several benchmarks (as reported by Mistral). Behind the news: Pixtral Large arrives as competition intensifies among vision-language models. Meta recently entered", "article_id": "68497b49235d602d65122596", "linked_images": ["article_68497b49235d602d65122596_img_0", "article_68497b49235d602d65122596_img_1", "article_68497b49235d602d65122596_img_2", "article_68497b49235d602d65122596_img_3", "article_68497b49235d602d65122596_img_4", "article_68497b49235d602d65122596_img_5"]}, {"type": "text", "text": "among 30 high-resolution images at a time. Based on Mistral Large 2 (a 123 billion-parameter large language model) and a 1 billion-parameter vision encoder, it demonstrates strong performance across several benchmarks (as reported by Mistral). Behind the news: Pixtral Large arrives as competition intensifies among vision-language models. Meta recently entered the field with Llama 3.2 vision models in 11B and 90B variants. Both Pixtral Large and Llama 3.2 90B offer open weights, making them smaller and more widely available than Anthropic\u2019s, Google\u2019s, or OpenAI\u2019s leading vision-language models. However, like those models, Pixtral Large falls short of the reported benchmark scores of the smaller, more permissively licensed Qwen2-VL 72B. Why it matters: Pixtral Large and updates to le Chat signal that vision-language capabilities \u2014 combining text generation, image recognition, and visual reasoning \u2014 are essential to compete with the AI leaders. In addition, context windows of 129,000 tokens and above have become more widely available, making it possible to analyze lengthy (or multiple) documents that include text, images, and graphs as well as video clips. We\u2019re thinking: Mistral is helping to internationalize development of foundation models. We\u2019re glad to see major developers emerging in Europe! Rapid progress in generative AI comes with a hidden environmental cost: mountains of obsolete hardware. What\u2019s new: A study projects that servers used to process generative AI could produce millions of metric tons of electronic waste by 2030. Extending server lifespans could reduce the burden substantially, according to author Peng Weng and colleagues at the Chinese Academy of Sciences and Reichman University. How it works: The study extrapolated from publicly available data to model accumulation of electronic waste, or e-waste, between 2023 and 2030. The authors examined four scenarios: One scenario assumed linear growth in which hardware manufacturing expands at the current rate of 41 percent annually. The other three assumed exponential growth of demand for computing: conservative (85 percent annually), moderate (115 percent annually), and aggressive (136 percent annually). The study evaluated each scenario with and without measures taken to reduce waste. Why it matters: E-waste is a problem not only due to its sheer quantity. Server hardware contains materials that are both hazardous and valuable. Discarded servers contain toxic substances like lead and chromium that can find their way into food water supplies. They also contain valuable metals, such as gold, silver, and platinum, that could save the environmental and financial costs of producing more of them. Proper recycling of these components could yield $14 billion to $28 billion, highlighting both the economic potential and the urgent need to develop and deploy advanced recycling technologies. We\u2019re thinking: Humanity dumps over 2 billion metric tons of waste annually, so even comprehensive recycling and repurposing of AI hardware and other electronic devices would make only a small dent in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy. Jailbreak prompts can prod a large language model (LLM) to overstep built-in boundaries, leading it to do", "article_id": "68497b49235d602d65122596", "linked_images": ["article_68497b49235d602d65122596_img_0", "article_68497b49235d602d65122596_img_1", "article_68497b49235d602d65122596_img_2", "article_68497b49235d602d65122596_img_3", "article_68497b49235d602d65122596_img_4", "article_68497b49235d602d65122596_img_5"]}, {"type": "text", "text": "in the overall volume. However, the high density of valuable materials in e-waste could make mining such waste profitable and help recycle waste into valuable products, making for a more sustainable tech economy. Jailbreak prompts can prod a large language model (LLM) to overstep built-in boundaries, leading it to do things like respond to queries it was trained to refuse to answer. Researchers devised a way to further boost the probability that LLMs will respond in ways that respect such limits. What\u2019s new: Jingtong Su, Julia Kempe, and Karen Ullrich at New York University and MetaAI improved model behavior via E-DPO. Their method modifies Direct Preference Optimization (DPO), a popular way to align models with human preferences. Key insight: DPO fine-tunes a model to encourage a developer\u2019s notion of good behavior and suppress bad behavior, but it must also ensure that the model doesn\u2019t forget knowledge it learned during pretraining. To this end, DPO\u2019s loss function includes a regularization constraint that encourages the model to produce token probabilities similar to those it produced prior to fine-tuning. However, this causes the model to retain not only desired knowledge but also undesired knowledge that may lead it to produce an unwanted response. We can reduce the probability that it will draw on such undesired knowledge by changing the regularization constraint. The idea is to ensure similar token probabilities between (a) a model prior to fine-tuning, asked to behave harmlessly prior to receiving the harmful prompt and (b) the fine-tuned model, given a harmful prompt. This adjustment helps the fine-tuned model deliver outputs based on benign knowledge, along with the usual benefits of DPO. How it works: The authors used E-DPO to further fine-tune Mistral-7b-sft-constitutional-ai (which is aligned using the technique known as constitutional AI) on two datasets in which each example consists of a prompt, a preferred response, and an objectionable response. Results: E-DPO reduced Mistral-7b-SFT-constitutional-ai\u2019s average attack success rate (ASR, the percentage of times a jailbreak prompt successfully elicited an objectionable responses) across 11 jailbreak datasets and methods (two sets of human-proposed jailbreak prompts and a variety of automatic jailbreak prompt-finding methods) from the HarmBench benchmark. The fine-tuned model achieved 36.95 ASR, while prior to fine-tuning it achieved 44.47 ASR. Typical DPO reduced the average ASR to 42.00. Why it matters: We can\u2019t train a model to respond in a desirable way to all jailbreaks, no matter how big the training dataset. The space of potential jailbreaks is practically unlimited. Instead, it\u2019s necessary to alter training methods, as this work does. We\u2019re thinking: Humans, like learning algorithms, can circumvent social norms when they encounter a harmful request (attack your neighbors) cloaked in a manipulative scenario (to uphold religious or nationalistic values). While we work on aligning models with human preferences, let\u2019s make sure we ourselves are aligned, too.", "article_id": "68497b49235d602d65122596", "linked_images": ["article_68497b49235d602d65122596_img_0", "article_68497b49235d602d65122596_img_1", "article_68497b49235d602d65122596_img_2", "article_68497b49235d602d65122596_img_3", "article_68497b49235d602d65122596_img_4", "article_68497b49235d602d65122596_img_5"]}, {"type": "text", "text": "we ourselves are aligned, too.", "article_id": "68497b49235d602d65122596", "linked_images": ["article_68497b49235d602d65122596_img_0", "article_68497b49235d602d65122596_img_1", "article_68497b49235d602d65122596_img_2", "article_68497b49235d602d65122596_img_3", "article_68497b49235d602d65122596_img_4", "article_68497b49235d602d65122596_img_5"]}, {"type": "image", "article_id": "68497b49235d602d65122596", "linked_chunks": ["article_68497b49235d602d65122596_chunk_0", "article_68497b49235d602d65122596_chunk_1", "article_68497b49235d602d65122596_chunk_2", "article_68497b49235d602d65122596_chunk_3", "article_68497b49235d602d65122596_chunk_4"], "alt_text": "AI ecosystem layers: applications, orchestration, foundational models, cloud, and semiconductors."}, {"type": "image", "article_id": "68497b49235d602d65122596", "linked_chunks": ["article_68497b49235d602d65122596_chunk_0", "article_68497b49235d602d65122596_chunk_1", "article_68497b49235d602d65122596_chunk_2", "article_68497b49235d602d65122596_chunk_3", "article_68497b49235d602d65122596_chunk_4"], "alt_text": "Promo banner for \"Machine Learning Specialization\""}, {"type": "image", "article_id": "68497b49235d602d65122596", "linked_chunks": ["article_68497b49235d602d65122596_chunk_0", "article_68497b49235d602d65122596_chunk_1", "article_68497b49235d602d65122596_chunk_2", "article_68497b49235d602d65122596_chunk_3", "article_68497b49235d602d65122596_chunk_4"], "alt_text": "Flow diagram of an application using LLMs to process prompts and tools for responses."}, {"type": "image", "article_id": "68497b49235d602d65122596", "linked_chunks": ["article_68497b49235d602d65122596_chunk_0", "article_68497b49235d602d65122596_chunk_1", "article_68497b49235d602d65122596_chunk_2", "article_68497b49235d602d65122596_chunk_3", "article_68497b49235d602d65122596_chunk_4"], "alt_text": "Table comparing model performance on Mathvista, MMMU, ChartQA, DocVQA, and other tasks."}, {"type": "image", "article_id": "68497b49235d602d65122596", "linked_chunks": ["article_68497b49235d602d65122596_chunk_0", "article_68497b49235d602d65122596_chunk_1", "article_68497b49235d602d65122596_chunk_2", "article_68497b49235d602d65122596_chunk_3", "article_68497b49235d602d65122596_chunk_4"], "alt_text": "Pile of discarded green circuit boards from electronic devices."}, {"type": "image", "article_id": "68497b49235d602d65122596", "linked_chunks": ["article_68497b49235d602d65122596_chunk_0", "article_68497b49235d602d65122596_chunk_1", "article_68497b49235d602d65122596_chunk_2", "article_68497b49235d602d65122596_chunk_3", "article_68497b49235d602d65122596_chunk_4"], "alt_text": "Table comparing HarmBench and AdvBench ASR performance across models and benchmarks."}, {"type": "alt_text", "alt_text": "AI ecosystem layers: applications, orchestration, foundational models, cloud, and semiconductors."}, {"type": "alt_text", "alt_text": "Promo banner for \"Machine Learning Specialization\""}, {"type": "alt_text", "alt_text": "Flow diagram of an application using LLMs to process prompts and tools for responses."}, {"type": "alt_text", "alt_text": "Table comparing model performance on Mathvista, MMMU, ChartQA, DocVQA, and other tasks."}, {"type": "alt_text", "alt_text": "Pile of discarded green circuit boards from electronic devices."}, {"type": "alt_text", "alt_text": "Table comparing HarmBench and AdvBench ASR performance across models and benchmarks."}, {"type": "text", "text": "Dear friends, Happy Thanksgiving! In the United States, this is a week when many reflect on their blessings and give thanks. Even as I reflect on how lucky I am to have food, shelter, family, and friends, I think about those who have much less and what we can do to help them. Last week, I spoke with a woman who had been severely physically abused by her husband. She showed me pictures of her face from a few years ago, which had a bloodied sequence of tears down the middle. She also showed me scars left by cigarette burns inflicted by her husband, who told her these burns made her ugly so no other man would ever want her. She is no longer with her husband but continues to struggle. Her phone is badly cracked and barely holds a charge. Without a high-school degree, she has struggled to find a job and is surviving by staying on the couch of a friend. As winter approaches, they keep their place chilly to save the cost of electricity. Working in AI, I am fortunate to interact with many of the smartest and most capable technology and business leaders in the world. But both at home and when I travel, I try to meet with people of a broad range of backgrounds, because ultimately I want to do work that helps people broadly, and this requires that I understand people broadly. When you go to a grocery store and see someone put down a $5 carton of eggs because it is too expensive, and hear them think through how to explain to their kids why they\u2019re skipping eggs that week, it gives you a deeper appreciation for why a $1.50/hour raise can be life-changing for many people. While I can try to help out individuals here and there, technology is advancing rapidly, and this gives me a lot of optimism for the future. Technology remains the best way I know of to help people at scale through providing better education, career guidance, healthcare, personal safety, healthier food, or other things needed to support thriving. I am optimistic about the future because I see so many ways life can be so much better for so many people. I feel blessed that, when my kids or I are cold, we have warm clothing, and when we are hungry, we have a working car to drive to the grocery and buy fresh food. I feel blessed that, rather than using a badly cracked cellphone, I have a modern laptop and a fast internet connection to do my work on. As a child, my father taught me the aphorism \u201cthere but for the grace of God go I\u201d to recognize that, in even slightly different circumstances, I might have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life,", "article_id": "68497b49235d602d6512259d", "linked_images": ["article_68497b49235d602d6512259d_img_0", "article_68497b49235d602d6512259d_img_1", "article_68497b49235d602d6512259d_img_2", "article_68497b49235d602d6512259d_img_3", "article_68497b49235d602d6512259d_img_4", "article_68497b49235d602d6512259d_img_5"]}, {"type": "text", "text": "have ended up with much less. Having worked on many software products, I know that, to make good decisions, I have to understand the people I hope to serve. This is why I continue to routinely seek out, speak with, and try to understand people from all walks of life, and I hope many others in AI will do so, too. I see so many people in the AI community building things to make the world better. I am thankful for what the AI community has already done, and I look forward to continuing to build and serve others together. Keep building! Andrew Get started coding in Python with AI Python for Beginners, a four-part course led by Andrew Ng. Build projects from the very first lesson with real-time support from an AI assistant. Complete the course and bring your ideas to life! Start today An up-and-coming Hangzhou AI lab unveiled a model that implements run-time reasoning similar to OpenAI o1 and delivers competitive performance. Unlike o1, it displays its reasoning steps. What\u2019s new: DeepSeek announced DeepSeek-R1, a model family that processes prompts by breaking them down into steps. A free preview version is available on the web, limited to 50 messages daily; API pricing is not yet announced. R1-lite-preview performs comparably to o1-preview on several math and problem-solving benchmarks. DeepSeek said it would release R1 as open source but didn't announce licensing terms or a release date. How it works: DeepSeek-R1-lite-preview uses a smaller base model than DeepSeek 2.5, which comprises 236 billion parameters. Like o1-preview, most of its performance gains come from an approach known as test-time compute, which trains an LLM to think at length in response to prompts, using more compute to generate deeper answers. Unlike o1-preview, which hides its reasoning, at inference, DeepSeek-R1-lite-preview\u2019s reasoning steps are visible. This makes the model more transparent, but it may also make it more vulnerable to jailbreaks and other manipulation. Behind the news: DeepSeek-R1 follows OpenAI in implementing this approach at a time when scaling laws that predict higher performance from bigger models and/or more training data are being questioned. Why it matters: DeepSeek is challenging OpenAI with a competitive large language model. It\u2019s part of an important movement, after years of scaling models by raising parameter counts and amassing larger datasets, toward achieving high performance by spending more energy on generating output. We\u2019re thinking: Models that do and don\u2019t take advantage of additional test-time compute are complementary. Those that do increase test-time compute perform well on math and science problems, but they\u2019re slow and costly. Those that don\u2019t use additional test-time compute do well on language tasks at higher speed and lower cost. Applications that require facility in both math and language may benefit by switching between the two. A new generation of robots can handle some household chores with unusual skill. What\u2019s new: Physical Intelligence, a startup based in San Francisco, unveiled \u03c00 (pronounced \u201cpi-zero\u201d), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The", "article_id": "68497b49235d602d6512259d", "linked_images": ["article_68497b49235d602d6512259d_img_0", "article_68497b49235d602d6512259d_img_1", "article_68497b49235d602d6512259d_img_2", "article_68497b49235d602d6512259d_img_3", "article_68497b49235d602d6512259d_img_4", "article_68497b49235d602d6512259d_img_5"]}, {"type": "text", "text": "new generation of robots can handle some household chores with unusual skill. What\u2019s new: Physical Intelligence, a startup based in San Francisco, unveiled \u03c00 (pronounced \u201cpi-zero\u201d), a machine learning system that enables robots to perform housekeeping tasks that require high coordination and dexterity, like folding clothes and cleaning tables. The company also announced $400 million in investments from OpenAI, Jeff Bezos, and several Silicon Valley venture capital firms. How it works: \u03c00 is a version of the pretrained PaliGemma vision-language model that has been modified for flow matching. (Flow matching is similar to diffusion, in which a model learns to remove noise from inputs to which noise has been added, and ultimately generates output by removing noise from an input of pure noise). A user supplies a text command, and the robot uses its sensor inputs to remove noise from a pure-noise action embedding to generate an appropriate action. Results: \u03c00 outperformed the open robotics models OpenVLA, Octo, ACT, and Diffusion Policy, all of which were fine-tuned on the same data, on all tasks tested, as measured by a robot\u2019s success rate in completing each task. For example, using a single robotic arm to stack a set of bowls of four sizes, \u03c00 completed about 100 percent on average. Diffusion Policy completed about 55 percent, ACT about 45 percent, and OpenVLA and Octo below 10 percent. Across all tasks, \u03c00 completed about 80 percent on average, while Diffusion Policy completed about 35 percent on average. Yes, but: The robot occasionally makes mistakes. In one video, it puts too many eggs into a carton and tries to force it shut. In another, it throws a container off a table instead of filling it with items. Behind the news: Commercial robotics appears to be undergoing a renaissance. Skild raised $300 million to develop a \u201cgeneral-purpose brain for robots.\u201d Figure AI secured $675 million to build humanoid robots powered by multimodal models. Covariant, which specializes in industrial robotics, licensed its technology to Amazon. (Disclosure: Andrew Ng is a member of Amazon's board of directors). OpenAI renewed its robotics effort after dismantling its robotics department in 2020. Why it matters: Robots have been slow to benefit from machine learning, but the generative AI revolution is driving rapid innovations that make them much more useful. Large language models have made it possible to command robots using plain English. Meanwhile, the team at Physical Intelligence collected a dataset of sufficient size and variety to train the model to generate highly articulated and practical actions. Household robots may not be right around the corner, but \u03c00 shows that they can perform tasks that people need done. We\u2019re thinking: One of the team members compared \u03c00 to GPT-1 for robotics \u2014 an inkling of things to come. Although there are significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning. Amazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services\u2019 AI infrastructure and", "article_id": "68497b49235d602d6512259d", "linked_images": ["article_68497b49235d602d6512259d_img_0", "article_68497b49235d602d6512259d_img_1", "article_68497b49235d602d6512259d_img_2", "article_68497b49235d602d6512259d_img_3", "article_68497b49235d602d6512259d_img_4", "article_68497b49235d602d6512259d_img_5"]}, {"type": "text", "text": "significant differences between text data (which is available in large quantities) and robot data (which is hard to get and varies per robot), it looks like a new era of large robotics foundation models is dawning. Amazon and Anthropic expanded their partnership, potentially strengthening Amazon Web Services\u2019 AI infrastructure and lengthening the high-flying startup\u2019s runway. What\u2019s new: Amazon, already a significant investor in Anthropic, put another $4 billion into the AI company. In exchange, Anthropic will train and run its AI models on Amazon\u2019s custom-designed chips. (Disclosure: Andrew Ng serves on Amazon\u2019s board of directors.) How it works: The new round brings Amazon\u2019s investment in Anthropic to $8 billion (though it remains a minority stake without a seat on the startup\u2019s board). The deal extended the partnership in several ways: Behind the news: In November, Anthropic agreed to use Google\u2019s cloud-computing infrastructure in return for a $2 billion investment. The previous month, Amazon had committed to invest as much as $4 billion in Anthropic, and Anthropic had made Amazon Web Services the primary provider of its models. Yes, but: The UK\u2019s Competition and Markets Authority recently cleared both Amazon\u2019s and Google\u2019s investments in Anthropic, but regulators continue to monitor such arrangements for violations of antitrust laws. Microsoft and OpenAI face a similar investigation by the European Commission and U.S. Federal Trade Commission. Why it matters: The speed and skill required to build state-of-the-art AI models is driving tech giants to collaborate with startups, while the high cost is driving startups to partner with tech giants. If the partnership between Amazon and Anthropic lives up to its promise, Claude users and developers could see gains in performance and efficiency. This could validate Amazon's hardware as a competitor with Nvidia and strengthen Amazon Web Services\u2019 position in the cloud market. On the other hand, if Claude faces any challenges in scaling while using Trainium and Inferentia, that could affect both companies' ambitions. We\u2019re thinking: Does the agreement between Amazon and Anthropic give the tech giant special access to the startup\u2019s models for distillation, research, or integration, as the partnership between Microsoft and OpenAI does? The companies\u2019 announcements don\u2019t say. An open source model is designed to perform sophisticated object detection on edge devices like phones, cars, medical equipment, and smart doorbells. What\u2019s new: Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, and colleagues at the International Digital Economy Academy introduced Grounding DINO 1.5, a system that enables devices with limited processing power to detect arbitrary objects in images based on a text list of objects (also known as open-vocabulary object detection). You can download the code and weights here. Key insight: The original Grounding DINO follows many of its predecessors by using image embeddings of different levels (from lower-level embeddings produced by an image encoder\u2019s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to better detect objects at different scales. However, it takes a lot of computation. To enable the", "article_id": "68497b49235d602d6512259d", "linked_images": ["article_68497b49235d602d6512259d_img_0", "article_68497b49235d602d6512259d_img_1", "article_68497b49235d602d6512259d_img_2", "article_68497b49235d602d6512259d_img_3", "article_68497b49235d602d6512259d_img_4", "article_68497b49235d602d6512259d_img_5"]}, {"type": "text", "text": "encoder\u2019s earlier layers, which are larger and represent simple patterns such as edges, to higher-level embeddings produced by later layers, which are smaller and represent complex patterns such as objects). This enables it to better detect objects at different scales. However, it takes a lot of computation. To enable the system to run on devices that have less processing power, Grounding DINO 1.5 uses only the smallest (highest-level) image embeddings for a crucial part of the process. How it works: Grounding DINO 1.5 is made up of components that produce text and image embeddings, fuse them, and classify them. It follows the system architecture and training of Grounding DINO with the following exceptions: (i) It uses a different image encoder, (ii) a different model combines text and image embeddings, and (iii) it was trained on a newer dataset of 20 million publicly available text-image examples. Results: Grounding DINO 1.5 performed significantly faster than the original Grounding DINO: 10.7 frames per second versus 1.1 frames per second running on an Nvidia Jetson Orin NX computer. Tested on a dataset of images of common objects annotated with labels and bounding boxes, Grounding DINO 1.5 achieved better average precision (a measure of how many objects it identified correctly in their correct location, higher is better) than both Grounding DINO and YOLO-Worldv2-L (a CNN-based object detector). Grounding DINO 1.5 scored 33.5 percent, Grounding DINO 27.4 percent, and YOLO-Worldv2-L 33 percent. Why it matters: The authors achieved 10 times the speed with just a couple of small changes (a more efficient image encoder and a smaller image embedding when performing cross-attention between embeddings of images and texts). Small changes can yield big results. We\u2019re thinking: Lately model builders have been building better, smaller, faster large language models for edge devices. We\u2019re glad to see object detection get similar treatment.", "article_id": "68497b49235d602d6512259d", "linked_images": ["article_68497b49235d602d6512259d_img_0", "article_68497b49235d602d6512259d_img_1", "article_68497b49235d602d6512259d_img_2", "article_68497b49235d602d6512259d_img_3", "article_68497b49235d602d6512259d_img_4", "article_68497b49235d602d6512259d_img_5"]}, {"type": "image", "article_id": "68497b49235d602d6512259d", "linked_chunks": ["article_68497b49235d602d6512259d_chunk_0", "article_68497b49235d602d6512259d_chunk_1", "article_68497b49235d602d6512259d_chunk_2", "article_68497b49235d602d6512259d_chunk_3", "article_68497b49235d602d6512259d_chunk_4"], "alt_text": "Cornucopia overflowing with fruits and vegetables."}, {"type": "image", "article_id": "68497b49235d602d6512259d", "linked_chunks": ["article_68497b49235d602d6512259d_chunk_0", "article_68497b49235d602d6512259d_chunk_1", "article_68497b49235d602d6512259d_chunk_2", "article_68497b49235d602d6512259d_chunk_3", "article_68497b49235d602d6512259d_chunk_4"], "alt_text": "Promo banner for \"AI Python for Beginners\""}, {"type": "image", "article_id": "68497b49235d602d6512259d", "linked_chunks": ["article_68497b49235d602d6512259d_chunk_0", "article_68497b49235d602d6512259d_chunk_1", "article_68497b49235d602d6512259d_chunk_2", "article_68497b49235d602d6512259d_chunk_3", "article_68497b49235d602d6512259d_chunk_4"], "alt_text": "Bar charts comparing performance of AI models across six tasks."}, {"type": "image", "article_id": "68497b49235d602d6512259d", "linked_chunks": ["article_68497b49235d602d6512259d_chunk_0", "article_68497b49235d602d6512259d_chunk_1", "article_68497b49235d602d6512259d_chunk_2", "article_68497b49235d602d6512259d_chunk_3", "article_68497b49235d602d6512259d_chunk_4"], "alt_text": "Robotic arms collaborating to fold a red garment on a table."}, {"type": "image", "article_id": "68497b49235d602d6512259d", "linked_chunks": ["article_68497b49235d602d6512259d_chunk_0", "article_68497b49235d602d6512259d_chunk_1", "article_68497b49235d602d6512259d_chunk_2", "article_68497b49235d602d6512259d_chunk_3", "article_68497b49235d602d6512259d_chunk_4"], "alt_text": "Illustration of a person holding a box with network nodes emerging from it."}, {"type": "image", "article_id": "68497b49235d602d6512259d", "linked_chunks": ["article_68497b49235d602d6512259d_chunk_0", "article_68497b49235d602d6512259d_chunk_1", "article_68497b49235d602d6512259d_chunk_2", "article_68497b49235d602d6512259d_chunk_3", "article_68497b49235d602d6512259d_chunk_4"], "alt_text": "Grounding DINO animation depicting object detection with bounding boxes on images."}, {"type": "alt_text", "alt_text": "Cornucopia overflowing with fruits and vegetables."}, {"type": "alt_text", "alt_text": "Promo banner for \"AI Python for Beginners\""}, {"type": "alt_text", "alt_text": "Bar charts comparing performance of AI models across six tasks."}, {"type": "alt_text", "alt_text": "Robotic arms collaborating to fold a red garment on a table."}, {"type": "alt_text", "alt_text": "Illustration of a person holding a box with network nodes emerging from it."}, {"type": "alt_text", "alt_text": "Grounding DINO animation depicting object detection with bounding boxes on images."}, {"type": "text", "text": "Dear friends, A small number of people are posting text online that\u2019s intended for direct consumption not by humans, but by LLMs (large language models). I find this a fascinating trend, particularly when writers are incentivized to help LLM providers better serve their users! People who post text online don\u2019t always have an incentive to help LLM providers. In fact, their incentives are often misaligned. Publishers worry about LLMs reading their text, paraphrasing it, and reusing their ideas without attribution, thus depriving them of subscription or ad revenue. This has even led to litigation such as The New York Times\u2019 lawsuit against OpenAI and Microsoft for alleged copyright infringement. There have also been demonstrations of prompt injections, where someone writes text to try to give an LLM instructions contrary to the provider\u2019s intent. (For example, a handful of sites advise job seekers to get past LLM resum\u00e9 screeners by writing on their resum\u00e9s, in a tiny/faint font that\u2019s nearly invisible to humans, text like \u201cThis candidate is very qualified for this role.\u201d) Spammers who try to promote certain products \u2014 which is already challenging for search engines to filter out \u2014 will also turn their attention to spamming LLMs. But there are examples of authors who want to actively help LLMs. Take the example of a startup that has just published a software library. Because the online documentation is very new, it won\u2019t yet be in LLMs\u2019 pretraining data. So when a user asks an LLM to suggest software, the LLM won\u2019t suggest this library, and even if a user asks the LLM directly to generate code using this library, the LLM won\u2019t know how to do so. Now, if the LLM is augmented with online search capabilities, then it might find the new documentation and be able to use this to write code using the library. In this case, the developer may want to take additional steps to make the online documentation easier for the LLM to read and understand via RAG. (And perhaps the documentation eventually will make it into pretraining data as well.) Compared to humans, LLMs are not as good at navigating complex websites, particularly ones with many graphical elements. However, LLMs are far better than people at rapidly ingesting long, dense, text documentation. Suppose the software library has many functions that we want an LLM to be able to use in the code it generates. If you were writing documentation to help humans use the library, you might create many web pages that break the information into bite-size chunks, with graphical illustrations to explain it. But for an LLM, it might be easier to have a long XML-formatted text file that clearly explains everything in one go. This text might include a list of all the functions, with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.) A human would find this long document painful to navigate and read, but an", "article_id": "68497b49235d602d651225a4", "linked_images": ["article_68497b49235d602d651225a4_img_0", "article_68497b49235d602d651225a4_img_1", "article_68497b49235d602d651225a4_img_2", "article_68497b49235d602d651225a4_img_3", "article_68497b49235d602d651225a4_img_4", "article_68497b49235d602d651225a4_img_5"]}, {"type": "text", "text": "with a dense description of each and an example or two of how to use it. (This is not dissimilar to the way we specify information about functions to enable LLMs to use them as tools.) A human would find this long document painful to navigate and read, but an LLM would do just fine ingesting it and deciding what functions to use and when! Because LLMs and people are better at ingesting different types of text, we write differently for LLMs than for humans. Further, when someone has an incentive to help an LLM better understand a topic \u2014 so the LLM can explain it better to users \u2014 then an author might write text to help an LLM. So far, text written specifically for consumption by LLMs has not been a huge trend. But Jeremy Howard\u2019s proposal for web publishers to post a llms.txt file to tell LLMs how to use their websites, like a robots.txt file tells web crawlers what to do, is an interesting step in this direction. In a related vein, some developers are posting detailed instructions that tell their IDE how to use tools, such as the plethora of .cursorrules files that tell the Cursor IDE how to use particular software stacks. I see a parallel with SEO (search engine optimization). The discipline of SEO has been around for decades. Some SEO helps search engines find more relevant topics, and some is spam that promotes low-quality information. But many SEO techniques \u2014 those that involve writing text for consumption by a search engine, rather than by a human \u2014 have survived so long in part because search engines process web pages differently than humans, so providing tags or other information that tells them what a web page is about has been helpful. The need to write text separately for LLMs and humans might diminish if LLMs catch up with humans in their ability to understand complex websites. But until then, as people get more information through LLMs, writing text to help LLMs will grow. Keep learning! Andrew P.S. I like LLMs, but I like humans even more. So please keep writing text for humans as well. \ud83d\ude00 Learn how to develop applications with large language models by building AI-powered games! Gain essential skills by designing a shareable text-based game and integrating safety features. If you\u2019ve completed our AI Python for Beginners series or want to improve your coding skills in a fun, interactive way, this is a perfect course for you! Start today Builders of large AI models have relied on the idea that bigger neural networks trained on more data and given more processing power would show steady improvements. Recent developments are challenging that idea. What\u2019s new: Next-generation large language models from OpenAI, Google, and Anthropic are falling short of expectations, employees at those companies told multiple publications. All three companies are responding by shifting their focus from pretraining to enhancing performance through techniques like fine-tuning and multi-step inference. Scaling law basics: A classic 2020 paper shows that, assuming a sufficient quantity of data, a", "article_id": "68497b49235d602d651225a4", "linked_images": ["article_68497b49235d602d651225a4_img_0", "article_68497b49235d602d651225a4_img_1", "article_68497b49235d602d651225a4_img_2", "article_68497b49235d602d651225a4_img_3", "article_68497b49235d602d651225a4_img_4", "article_68497b49235d602d651225a4_img_5"]}, {"type": "text", "text": "Anthropic are falling short of expectations, employees at those companies told multiple publications. All three companies are responding by shifting their focus from pretraining to enhancing performance through techniques like fine-tuning and multi-step inference. Scaling law basics: A classic 2020 paper shows that, assuming a sufficient quantity of data, a transformer network\u2019s performance rises predictably with increases in model size (demonstrated between 768 parameters and 1.5 billion parameters). Likewise, assuming sufficient model size, performance rises predictably with increases in dataset size (demonstrated between 22 million tokens and 23 billion tokens). Furthermore, performance rises predictably with increases in both model and dataset sizes. The 2022 Chinchilla paper shows that, to build an optimal model, every 4x increase in compute requires a 2x increase in the size of the model and dataset (demonstrated for models between 70 million and 16 billion parameters, trained on between 5 billion and 500 billion tokens). Due to limited experimentation and lack of a theoretical basis of their findings, the authors didn\u2019t determine whether these relationships would continue to hold at larger scales. Diminishing returns: Major AI companies have been counting on scaling laws to keep their models growing more capable at a steady pace. However, the next generation of high-profile models has not shown the expected improvements despite larger architectures, more training data, and more processing power. What they\u2019re saying: AI leaders are divided on the future of scaling laws as they are currently understood. Why it matters: AI\u2019s phenomenal advance has drawn hundreds of millions of users and sparked a new era of progress and hope. Slower-than-expected improvements in future foundation models may blunt this progress. At the same time, the cost of training large AI models is rising dramatically. The latest models cost as much as $100 million to train, and this number could reach $100 billion within a few years, according to Anthropic\u2019s Dario Amodei. Rising costs could lead companies to reallocate their gargantuan training budgets and researchers to focus on more cost-effective, application-specific approaches. We\u2019re thinking: AI\u2019s power-law curves may be flattening, but we don\u2019t see overall progress slowing. Many developers already have shifted to building smaller, more processing-efficient models, especially networks that can run on edge devices. Agentic workflows are taking off and bringing huge gains in performance. Training on synthetic data is another frontier that\u2019s only beginning to be explored. AI technology holds many wonders to come! A real-time video generator lets you explore an open-ended, interactive virtual world \u2014 a video game without a game engine. What\u2019s new: Decart, a startup that\u2019s building a platform for AI applications, and Etched, which designs specialized AI chips, introduced Oasis, which generates a Minecraft-like game in real time. The weights are open and available here. You can play with a demo here. How it works: The system generates one frame at a time based on a user\u2019s keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it\u2019s almost certainly based on videos of Minecraft gameplay, given the output\u2019s striking semblance to that game. Results: The Oasis web demo enables users", "article_id": "68497b49235d602d651225a4", "linked_images": ["article_68497b49235d602d651225a4_img_0", "article_68497b49235d602d651225a4_img_1", "article_68497b49235d602d651225a4_img_2", "article_68497b49235d602d651225a4_img_3", "article_68497b49235d602d651225a4_img_4", "article_68497b49235d602d651225a4_img_5"]}, {"type": "text", "text": "works: The system generates one frame at a time based on a user\u2019s keystrokes, mouse movements, and previously generated frames. The training dataset is undisclosed, but it\u2019s almost certainly based on videos of Minecraft gameplay, given the output\u2019s striking semblance to that game. Results: The Oasis web demo enables users to interact with 360-by-360-pixel frames at 20 frames per second. Users can place blocks, place fences, and move through a Minecraft-like world. The demo starts with an image of a location, but users can upload an image (turning, say, a photo of your cat into a blocky Minecraft-style level, as reported by Wired). Yes, but: The game has its fair share of issues. For instance, objects disappear and menus items change unaccountably. The world\u2019s physics are similarly inconsistent. For instance, players don\u2019t fall into holes dug directly beneath them and, after jumping into water, players are likely to find themselves standing on a blue floor. Behind the news: In February, Google announced Genie, a model that generates two-dimensional platformer games from input images. We weren\u2019t able to find a publicly available demo or model. Why it matters: Oasis is more a proof of concept than a product. Nonetheless, as an open-world video game entirely generated by AI \u2014 albeit based on data produced by a traditional implementation \u2014 it sets a bar for future game generators. We\u2019re thinking: Real-time video generation suggests a wealth of potential applications \u2014 say, a virtual workspace for interior decorating that can see and generate your home, or an interactive car repair manual that can create custom clips based on your own vehicle. Oasis is an early step in this direction. The largest manufacturer of AI chips told its Chinese customers it would stop fabricating their most advanced designs, further limiting China\u2019s access to AI hardware. What\u2019s new: Taiwan Semiconductor Manufacturing Corp. (TSMC) notified Alibaba, Baidu, and others it would halt production of their most advanced chips starting November 13, according to multiple reports. The restriction affects chip designs that are based on manufacturing processes at scales of 7 nanometers and below. TSMC must receive explicit permission from the U.S. government to manufacture advanced chips for a given customer, which likely would require that the government assess each chip to prevent potential military applications. How it works: The United States Department of Commerce ordered TSMC to halt shipments of advanced AI chips to China after a chip fabricated by TSMC was discovered in an AI system sold by the Chinese telecoms giant Huawei, apparently in violation of earlier U.S. controls, Reuters reported. Taiwan\u2019s economic ministry said it would follow all domestic and international regulations. Behind the news: The U.S.-China chip standoff began in 2020 and has escalated since. Initial restrictions barred U.S.-based companies like AMD, Intel, and Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded by promoting domestic chip fabrication. In 2022, the U.S. passed the CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan. Why it matters: TSMC finds itself", "article_id": "68497b49235d602d651225a4", "linked_images": ["article_68497b49235d602d651225a4_img_0", "article_68497b49235d602d651225a4_img_1", "article_68497b49235d602d651225a4_img_2", "article_68497b49235d602d651225a4_img_3", "article_68497b49235d602d651225a4_img_4", "article_68497b49235d602d651225a4_img_5"]}, {"type": "text", "text": "Nvidia from selling advanced chips to Huawei and affiliated Chinese firms. China responded by promoting domestic chip fabrication. In 2022, the U.S. passed the CHIPS and Science Act to boost its own chip industry, seeking to counter China and decrease U.S. reliance on Taiwan. Why it matters: TSMC finds itself in the middle of an AI arms race in which cutting-edge chips could tip the balance. The company itself, which has been operating at full capacity, is unlikely to suffer business losses. We\u2019re thinking: AI developers in China have been resourceful in navigating previous restrictions. Chip manufacturing is extraordinarily difficult to master, but China has made strides in this direction. A proliferation of factories that can fabricate advanced chips would reshape AI research and business worldwide. Researchers cut the processing required to train transformers by around 20 percent with only a slight degradation in performance. What\u2019s new: Xiuying Wei and colleagues at Swiss Federal Institute of Technology Lausanne replaced a transformer\u2019s linear layers with approximations based on computationally efficient low-rank linear layers. Key insight: A low-rank approximation replaces a matrix with a product of two smaller matrices. This technique is widely used to streamline fine-tuning via LoRA, which modifies the weights in each of a transformer\u2019s linear layers by adding a learned low-rank approximation. As a direct replacement for the weights in linear layers, low-rank approximation saves processing during training, but it also causes unstable fluctuations in the training loss and slower convergence. The authors mitigated these undesirable effects by training each full-size layer in parallel with a low-rank approximation of the layer while gradually phasing out the full-size layer. This approach costs more memory and computation initially, but it saves those resources in the long run. How it works: The authors modified a transformer (1.3 billion parameters) to use low-rank approximation (which trimmed the parameter count to 985 million). They trained both models on 25.5B tokens of text scraped from the web, filtered, and deduplicated. Results: The authors tested both the modified and full-size transformers on 500 million tokens from the validation set according to perplexity (a measure of the likelihood that a model will predict the next word, lower is better). The modified version achieved 12.86 perplexity, slightly worse than the full-size version\u2019s 12.46 perplexity. However, training the modified version required more than 20 percent less processing and 14 percent less time. The modified transformer used 1.66*10^20 FLOPS and took 302 hours, while the full-size version used 2.10*10^20 FLOPS and took 352 hours. Why it matters: Training large transformers requires a lot of computation. Low-rank approximation lightens the processing load. This work approximates a transformer's linear layers to save memory, while the earlier GaLore approximates the gradient to save optimizer memory. We\u2019re thinking: The authors note that this approach also works for fine-tuning pretrained models \u2014 a potential alternative to LoRA. Simply replace each pretrained linear layer (with weights W) with two linear layers (with weights U and V), and initialize U and V such that W = UV.", "article_id": "68497b49235d602d651225a4", "linked_images": ["article_68497b49235d602d651225a4_img_0", "article_68497b49235d602d651225a4_img_1", "article_68497b49235d602d651225a4_img_2", "article_68497b49235d602d651225a4_img_3", "article_68497b49235d602d651225a4_img_4", "article_68497b49235d602d651225a4_img_5"]}, {"type": "text", "text": "works for fine-tuning pretrained models \u2014 a potential alternative to LoRA. Simply replace each pretrained linear layer (with weights W) with two linear layers (with weights U and V), and initialize U and V such that W = UV.", "article_id": "68497b49235d602d651225a4", "linked_images": ["article_68497b49235d602d651225a4_img_0", "article_68497b49235d602d651225a4_img_1", "article_68497b49235d602d651225a4_img_2", "article_68497b49235d602d651225a4_img_3", "article_68497b49235d602d651225a4_img_4", "article_68497b49235d602d651225a4_img_5"]}, {"type": "image", "article_id": "68497b49235d602d651225a4", "linked_chunks": ["article_68497b49235d602d651225a4_chunk_0", "article_68497b49235d602d651225a4_chunk_1", "article_68497b49235d602d651225a4_chunk_2", "article_68497b49235d602d651225a4_chunk_3", "article_68497b49235d602d651225a4_chunk_4", "article_68497b49235d602d651225a4_chunk_5"], "alt_text": "Two people reading in bed, one with a book on library functions and a head labeled with AI layers."}, {"type": "image", "article_id": "68497b49235d602d651225a4", "linked_chunks": ["article_68497b49235d602d651225a4_chunk_0", "article_68497b49235d602d651225a4_chunk_1", "article_68497b49235d602d651225a4_chunk_2", "article_68497b49235d602d651225a4_chunk_3", "article_68497b49235d602d651225a4_chunk_4", "article_68497b49235d602d651225a4_chunk_5"], "alt_text": "Promo banner for \"Building an AI-Powered Game\""}, {"type": "image", "article_id": "68497b49235d602d651225a4", "linked_chunks": ["article_68497b49235d602d651225a4_chunk_0", "article_68497b49235d602d651225a4_chunk_1", "article_68497b49235d602d651225a4_chunk_2", "article_68497b49235d602d651225a4_chunk_3", "article_68497b49235d602d651225a4_chunk_4", "article_68497b49235d602d651225a4_chunk_5"], "alt_text": "Graph showing test loss decreases with more tokens and larger model sizes (103-109 parameters)."}, {"type": "image", "article_id": "68497b49235d602d651225a4", "linked_chunks": ["article_68497b49235d602d651225a4_chunk_0", "article_68497b49235d602d651225a4_chunk_1", "article_68497b49235d602d651225a4_chunk_2", "article_68497b49235d602d651225a4_chunk_3", "article_68497b49235d602d651225a4_chunk_4", "article_68497b49235d602d651225a4_chunk_5"], "alt_text": "Comparison of Minecraft terrain with and without player modifications."}, {"type": "image", "article_id": "68497b49235d602d651225a4", "linked_chunks": ["article_68497b49235d602d651225a4_chunk_0", "article_68497b49235d602d651225a4_chunk_1", "article_68497b49235d602d651225a4_chunk_2", "article_68497b49235d602d651225a4_chunk_3", "article_68497b49235d602d651225a4_chunk_4", "article_68497b49235d602d651225a4_chunk_5"], "alt_text": "Close-up of a Chinese-made server chip labeled with the logo and text \u2018710\u2019 mounted on a motherboard."}, {"type": "image", "article_id": "68497b49235d602d651225a4", "linked_chunks": ["article_68497b49235d602d651225a4_chunk_0", "article_68497b49235d602d651225a4_chunk_1", "article_68497b49235d602d651225a4_chunk_2", "article_68497b49235d602d651225a4_chunk_3", "article_68497b49235d602d651225a4_chunk_4", "article_68497b49235d602d651225a4_chunk_5"], "alt_text": "Efficient Foundations animation showing layered AI model components."}, {"type": "alt_text", "alt_text": "Two people reading in bed, one with a book on library functions and a head labeled with AI layers."}, {"type": "alt_text", "alt_text": "Promo banner for \"Building an AI-Powered Game\""}, {"type": "alt_text", "alt_text": "Graph showing test loss decreases with more tokens and larger model sizes (103-109 parameters)."}, {"type": "alt_text", "alt_text": "Comparison of Minecraft terrain with and without player modifications."}, {"type": "alt_text", "alt_text": "Close-up of a Chinese-made server chip labeled with the logo and text \u2018710\u2019 mounted on a motherboard."}, {"type": "alt_text", "alt_text": "Efficient Foundations animation showing layered AI model components."}, {"type": "text", "text": "Dear friends, Large language models (LLMs) are typically optimized to answer peoples\u2019 questions. But there is a trend toward models also being optimized to fit into agentic workflows. This will give a huge boost to agentic performance! Following ChatGPT\u2019s breakaway success at answering questions, a lot of LLM development focused on providing a good consumer experience. So LLMs were tuned to answer questions (\u201cWhy did Shakespeare write Macbeth?\u201d) or follow human-provided instructions (\u201cExplain why Shakespeare wrote Macbeth\u201d). A large fraction of the datasets for instruction tuning guide models to provide more helpful responses to human-written questions and instructions of the sort one might ask a consumer-facing LLM like those offered by the web interfaces of ChatGPT, Claude, or Gemini. But agentic workloads call on different behaviors. Rather than directly generating responses for consumers, AI software may use a model in part of an iterative workflow to reflect on its own output, use tools, write plans, and collaborate in a multi-agent setting. Major model makers are increasingly optimizing models to be used in AI agents as well. Take tool use (or function calling). If an LLM is asked about the current weather, it won\u2019t be able to derive the information needed from its training data. Instead, it might generate a request for an API call to get that information. Even before GPT-4 natively supported function calls, application developers were already using LLMs to generate function calls, but by writing more complex prompts (such as variations of ReAct prompts) that tell the LLM what functions are available and then have the LLM generate a string that a separate software routine parses (perhaps with regular expressions) to figure out if it wants to call a function. Generating such calls became much more reliable after GPT-4 and then many other models natively supported function calling. Today, LLMs can decide to call functions to search for information for retrieval-augmented generation (RAG), execute code, send emails, place orders online, and much more. Recently, Anthropic released a version of its model that is capable of computer use, using mouse-clicks and keystrokes to operate a computer (usually a virtual machine). I\u2019ve enjoyed playing with the demo. While other teams have been prompting LLMs to use computers to build a new generation of RPA (robotic process automation) applications, native support for computer use by a major LLM provider is a great step forward. This will help many developers! As agentic workflows mature, here is what I am seeing: Most LLMs have been optimized for answering questions primarily to deliver a good consumer experience, and we\u2019ve been able to \u201cgraft\u201d them into complex agentic workflows to build valuable applications. The trend of LLMs built to support particular operations in agents natively will create a lot of lift for agentic performance. I\u2019m confident that large agentic performance gains in this direction will be realized in the next few years. Keep learning! Andrew Prevent common issues in applications based on large language models such as hallucinations, data leaks, and off-topic responses. Build guardrails that protect against incorrect or sensitive responses in our new short", "article_id": "68497b49235d602d651225ab", "linked_images": ["article_68497b49235d602d651225ab_img_0", "article_68497b49235d602d651225ab_img_1", "article_68497b49235d602d651225ab_img_2", "article_68497b49235d602d651225ab_img_3", "article_68497b49235d602d651225ab_img_4", "article_68497b49235d602d651225ab_img_5", "article_68497b49235d602d651225ab_img_6"]}, {"type": "text", "text": "that large agentic performance gains in this direction will be realized in the next few years. Keep learning! Andrew Prevent common issues in applications based on large language models such as hallucinations, data leaks, and off-topic responses. Build guardrails that protect against incorrect or sensitive responses in our new short course, made in collaboration with GuardrailsAI. Sign up now! A new open source large language model outperforms competitors, including the open-weights Llama 3.1 405B, on a variety of benchmarks. What\u2019s new: Tencent released Hunyuan-Large, a mixture-of-experts model with open code and open weights. It comes in base and instruction-tuned versions, both of which can process a relatively large input context window of 256,000 tokens. It\u2019s free for developers outside the European Union who have fewer than 100 million monthly users. You can experiment with it here. Mixture of experts (MoE) basics: The MoE architecture uses different subsets of its parameters to process different inputs. Each MoE layer contains a group of neural networks, or experts, preceded by a gating module that learns to choose which one(s) to use based on the input. In this way, different experts learn to specialize in different types of examples. Because not all parameters are used to produce any given output, the network uses less energy and runs faster than models of similar size that use all parameters to process every input. How it works: Hunyuan-Large comprises 389 billion parameters but uses 52 billion parameters to process any given input. The team pretrained the model on 7 trillion tokens primarily of English and Chinese text, of which 5.5 trillion tokens came from unspecified sources and 1.5 trillion synthetic tokens were generated by unspecified large language models. The models used to generate training data were \u201cspecialized\u201d to provide expert-level responses in various domains. The team fine-tuned Hunyuan-Large on unspecified datasets of instructions and human feedback. Results: The team compared the Hunyuan-Large models to four open source models and their instruction-tuned versions: Llama 3.1 70B, Llama 3.1 405B, and the MoE models Mixtral-8x22B and DeepSeek-V2. Why it matters: Hunyuan-Large generally outperforms Llama 405B, achieving the performance of a 405 billion parameter model while computing only 52 billion parameters. That\u2019s a significantly lower processing requirement, and the model is free for many purposes. We\u2019re thinking: Setting aside Switch Transformer \u2014 a 1.6 trillion parameter behemoth that was built to test the limits of size rather than performance \u2014 Hunyuan-Large is among the largest MoE models we\u2019ve come across. It\u2019s an impressive demonstration of what larger MoE models can accomplish. Two top AI companies changed their stances on military and intelligence applications. What\u2019s new: Meta made its Llama family of large language models available to the U.S. government for national security purposes \u2014 a major change in its policy on military applications. Similarly, Anthropic will offer its Claude models to U.S. intelligence and defense agencies. How it works: Meta and Anthropic are relying on partnerships with government contractors to navigate the security and procurement requirements for military and intelligence work. Behind the news: In 2018, Google faced backlash when it won", "article_id": "68497b49235d602d651225ab", "linked_images": ["article_68497b49235d602d651225ab_img_0", "article_68497b49235d602d651225ab_img_1", "article_68497b49235d602d651225ab_img_2", "article_68497b49235d602d651225ab_img_3", "article_68497b49235d602d651225ab_img_4", "article_68497b49235d602d651225ab_img_5", "article_68497b49235d602d651225ab_img_6"]}, {"type": "text", "text": "applications. Similarly, Anthropic will offer its Claude models to U.S. intelligence and defense agencies. How it works: Meta and Anthropic are relying on partnerships with government contractors to navigate the security and procurement requirements for military and intelligence work. Behind the news: In 2018, Google faced backlash when it won a contract with the U.S. government to build Project Maven, an AI-assisted intelligence platform. Employees protested, resigned, and called on the company to eschew military AI work. Google withdrew from the project and Palantir took it over. Subsequently, many AI developers, including Meta and Anthropic, have forbidden use of their models for military applications. Llama\u2019s new availability to U.S. military and intelligence agencies is a notable exception. In July, Anthropic, too, began to accommodate use of its models for intelligence work. Anthropic still prohibits using Claude to develop weapons or mount cyberattacks. Why it matters: The shift in Meta\u2019s and Anthropic\u2019s policies toward military uses of AI is momentous. Lately AI has become a battlefield staple in the form of weaponized drones, and AI companies must take care that their new policies are consistent with upholding human rights. Military uses for AI include not only weapons development and targeting but also potentially life-saving search and rescue, logistics, intelligence, and communications. Moreover, defense contracts represent major opportunities for AI companies that can fund widely beneficial research and applications. We\u2019re thinking: Peace-loving nations face difficult security challenges, and AI can be helpful in meeting them. At the same time, the militarization of AI brings challenges to maintaining peace and stability, upholding human rights, and retaining human control over autonomous systems. We call on developers of military AI to observe the guidelines, proposed by Responsible Artificial Intelligence in the Military, which are endorsed by more than 60 countries and call for robust governance, oversight, accountability, and respect for human rights. Some voters navigated last week\u2019s United States elections with help from a large language model that generated output based on verified, nonpartisan information. What\u2019s new: Perplexity, an AI-powered search engine founded in 2022 by former OpenAI and Meta researchers, launched its Election Information Hub, an AI-enhanced website that combines AI-generated analysis with real-time data. The model provided live updates, summaries, and explanations of key issues in the recent national, state, and local elections in the U.S. (The hub remains live, but it no longer displays information about local contests or delivers detailed results for election-related searches.) How it works: Perplexity partnered with Associated Press for election news and Democracy Works, a nonprofit that develops technology and data related to democracy. Democracy Works provided an API for information about elections, issues, and polling locations. Behind the news: While Perplexity courted demand for AI-generated information about the U.S. elections, other search-engine providers took more cautious approaches. You.com offered an election chatbot that focused on vote tallies provided by Decision Desk HQ, an election information broker, rather than information about issues or polling locations. Google and Microsoft Bing emphasized information from vetted sources. Microsoft Copilot and OpenAI (which had launched its SearchGPT service the week before the election)", "article_id": "68497b49235d602d651225ab", "linked_images": ["article_68497b49235d602d651225ab_img_0", "article_68497b49235d602d651225ab_img_1", "article_68497b49235d602d651225ab_img_2", "article_68497b49235d602d651225ab_img_3", "article_68497b49235d602d651225ab_img_4", "article_68497b49235d602d651225ab_img_5", "article_68497b49235d602d651225ab_img_6"]}, {"type": "text", "text": "offered an election chatbot that focused on vote tallies provided by Decision Desk HQ, an election information broker, rather than information about issues or polling locations. Google and Microsoft Bing emphasized information from vetted sources. Microsoft Copilot and OpenAI (which had launched its SearchGPT service the week before the election) simply declined to answer election-related questions, referring users to other sources of information. Why it matters: Chatbots are maturing to the point where they can provide fairly trustworthy information in high-stakes decisions like elections. The combination of web search and retrieval-augmented generation contributes to decision support systems that are both personalized and accurate. We\u2019re thinking: Perfect information is hard to come by in any election. Traditional media, social media, and your uncle\u2019s strongly held opinions all have limitations. Chatbots aren\u2019t perfect either, but when they\u2019re properly designed to avoid biased output and outfitted with high-quality information sources, they can help strengthen users\u2019 choices and voices. An open source package inspired by the commercial agentic code generator Devin aims to automate computer programming and more. What\u2019s new: OpenHands, previously known as OpenDevin, implements a variety of agents for coding and other tasks. It was built by Xingyao Wang and a team at University of Illinois Urbana-Champaign, Carnegie Mellon, Yale, University of California Berkeley, Contextual AI, King Abdullah University of Science and Technology, Australian National University, Ho Chi Minh City University of Technology, Alibaba, and All Hands AI. The code is free to download, use, and modify. How it works: OpenHands provides a set of agents, or workflows for the user\u2019s choice of large language models. Users can command various agents to generate, edit, and run code; interact with the web; and perform auxiliary tasks related to coding and other work. The agents run in a secure Docker container with access to a server to execute code, a web browser, and tools that, say, copy text from pdfs or transcribe audio files. Results: Overall, OpenHands agents achieve similar performance to previous agents on software engineering problems, web browsing, and miscellaneous tasks like answering questions. For example, fixing issues in Github in SWE-Bench, the CodeAct agent using Claude 3.5 Sonnet solved 26 percent while Moatless Tools using the same model solved 26.7 percent. On GPQA Diamond, a set of graduate-level questions about physics, chemistry, and biology, the CodeAct agent using GPT-4-turbo with search wrote code to perform the necessary calculations and found relevant information to answer the questions, achieving 51.8 percent accuracy. GPT-4 with search achieved 38.8 percent accuracy. Why it matters: Agentic workflows are rapidly expanding the scope and capabilities of large language models. As open source software, this system gives developers an extensible toolkit for designing agentic systems. Although it\u2019s oriented toward coding, it accommodates a variety of information-gathering, -processing, and -publishing tasks. We\u2019re thinking: This system lets users tailor custom agents simply by rewriting prompts. We look forward to seeing what non-programmers do with it! Build AI applications that have long-term agentic memory! Our short course \u201cLLMs as Operating Systems: Agent Memory\u201d is based on insights from the MemGPT paper and taught", "article_id": "68497b49235d602d651225ab", "linked_images": ["article_68497b49235d602d651225ab_img_0", "article_68497b49235d602d651225ab_img_1", "article_68497b49235d602d651225ab_img_2", "article_68497b49235d602d651225ab_img_3", "article_68497b49235d602d651225ab_img_4", "article_68497b49235d602d651225ab_img_5", "article_68497b49235d602d651225ab_img_6"]}, {"type": "text", "text": "We\u2019re thinking: This system lets users tailor custom agents simply by rewriting prompts. We look forward to seeing what non-programmers do with it! Build AI applications that have long-term agentic memory! Our short course \u201cLLMs as Operating Systems: Agent Memory\u201d is based on insights from the MemGPT paper and taught by two of its coauthors. Learn how to implement persistent, efficient memory management for applications based on large language models. Enroll for free", "article_id": "68497b49235d602d651225ab", "linked_images": ["article_68497b49235d602d651225ab_img_0", "article_68497b49235d602d651225ab_img_1", "article_68497b49235d602d651225ab_img_2", "article_68497b49235d602d651225ab_img_3", "article_68497b49235d602d651225ab_img_4", "article_68497b49235d602d651225ab_img_5", "article_68497b49235d602d651225ab_img_6"]}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "Man with tools says, \u201cI optimized for tool use!\u201d Woman at computer replies, \u201cShould\u2019ve optimized for computer use!\u201d"}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "Promo banner for \"Safe and Reliable AI via Guardrails\""}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "Performance comparison of models across tasks in English, Chinese, Math, and Code, with Hunyuan-Large leading in most metrics."}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "Llama wearing a camouflage helmet, looking determined with a light blue background."}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "User entering ZIP code \u201894103\u2019 in U.S. General Election ballot lookup to view contests and candidates."}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "OpenDevin animation illustrating open-source AI model collaboration."}, {"type": "image", "article_id": "68497b49235d602d651225ab", "linked_chunks": ["article_68497b49235d602d651225ab_chunk_0", "article_68497b49235d602d651225ab_chunk_1", "article_68497b49235d602d651225ab_chunk_2", "article_68497b49235d602d651225ab_chunk_3", "article_68497b49235d602d651225ab_chunk_4"], "alt_text": "Promo banner for \"LLMs as Operating Systems: Agent Memory\""}, {"type": "alt_text", "alt_text": "Man with tools says, \u201cI optimized for tool use!\u201d Woman at computer replies, \u201cShould\u2019ve optimized for computer use!\u201d"}, {"type": "alt_text", "alt_text": "Promo banner for \"Safe and Reliable AI via Guardrails\""}, {"type": "alt_text", "alt_text": "Performance comparison of models across tasks in English, Chinese, Math, and Code, with Hunyuan-Large leading in most metrics."}, {"type": "alt_text", "alt_text": "Llama wearing a camouflage helmet, looking determined with a light blue background."}, {"type": "alt_text", "alt_text": "User entering ZIP code \u201894103\u2019 in U.S. General Election ballot lookup to view contests and candidates."}, {"type": "alt_text", "alt_text": "OpenDevin animation illustrating open-source AI model collaboration."}, {"type": "alt_text", "alt_text": "Promo banner for \"LLMs as Operating Systems: Agent Memory\""}]